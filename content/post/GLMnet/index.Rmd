---
title: "Manhattan Real Estate"
author: "Jim Gruman"
date: "2020-07-08"
diagram: true
output: 
 blogdown::html_page:
  toc: false
categories: [Data Science, R, machine learning]
description: "Machine Learning in R with GLMnet" 
# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image: 
  caption: 'by Sam Trotman on Unsplash'
  focal_point: "Smart"
  preview_only: false
featured: false
draft: true
---

Statistics today has become almost synonymous with machine learning, a collection of techniques that benefit from today's incredible computing power. 

This post focuses on a relatively new method for implementing machine learning  in R. We will start with the foundation of it all, the linear model and its generalization, the `glm`. We briefly look how to assess model quality with traditional measures and cross-validation and visualize models with coefficient plots. Finally we turn to penalized regression with the Elastic Net. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      cache=TRUE, 
                      dev = 'jpeg',
                      fig.width = 12,
                      fig.height = 8,
                      out.width = '70%')

library(glmnet) #Generalized Linear Model via Penalized Maximum Likelihood
library(useful) #Utility functions for preparing GLM
library(coefplot)  #Visualizations for illustrating GLM performance
library(dygraphs)
library(Cairo)
library(recipes)

theme_set(hrbrthemes::theme_ipsum())
```

> Jared Lander teaches statistics at Columbia University in New York. You can find his [2018 ODSC Video](https://youtu.be/WWuW4kkI_SA), his [Github repo](https://github.com/jaredlander/odsclondon2018), and web site [jaredlander.com](https://www.jaredlander.com/).

## Project Structure

Making use of RStudio projects functionality greatly improves the analysts organization and workflow. To follow along with this post, install the `usethis` package and then run the following code which will recreate Jared's project materials locally on your computer. Be sure to select the positive prompts such as `yes`, `yeah`, etc.

```{r ProjectSetup, eval=FALSE, include=TRUE}
newProject <- usethis::use_course('https://github.com/jaredlander/odsclondon2018/archive/master.zip')
```

## Data

Data for the class is kept at `data.world`. Run the following code to download all the data locally to your project folder labeled `data`. This requires that you are using an RStudio Project setup exactly like this repo, which is done if you followed the preceding code.

```{r DataDownload, eval=FALSE, include=TRUE}
source('prep/DownloadData.r')
```

## Linear Models and GLMnet

Recall that many business problems require a model of future behavior built on past observations. In a machine learning context, these are known as supervised models. In a mathematical context, the question is simply "what function of X inputs yields an optimal estimate of Y?"

The predicted variable could be continuous, like a property value. Or the prediction could be a classification, like Spam or Not Spam.

$$
Y = f(X)
$$

One technique for modeling continuous outputs is to add the contributions of each individual data element multiplied by a coefficient, or weight, for each shown as $b$ here as a best fit line. The long form looks like

$$
y = f(x) = a + b_1x_1 + b_2x_2 + \ldots + b_px_p 
$$

In linear algebra, an equivalent but more compact way of saying the same thing with matrices is:

$$
 Y = \beta X
$$

Our goal, then is to build a model that provides a list of $\beta$ estimates in a function that most closely transforms the independent variables $X$ into the dependent variable $Y$ for new, unseen data.

$$
\hat{\beta} = (X^TX)^{-1}X^TY
$$

Let's read in the prepared training dataset. The file includes 32,250 real estate transactions in Manhattan with a total sale value and 45 other variables.

```{r ReadTrainingData}
land_train <- readRDS(here::here("content","post","GLMnet","data",'manhattan_Train.rds'))

head(land_train) %>%
  knitr::kable(caption = "Manhattan Property Sales")
```

Let's take a quick look at the range of Sale Prices

```{r, warning=FALSE}
land_train %>%
  ggplot()+
  geom_histogram(aes(TotalValue), fill = "darkblue")+
  scale_x_log10(labels = scales::dollar_format())+
  theme(plot.title.position = "plot")+
  labs(title = "Manhattan Real Estate Values",
       subtitle = "Including Low Value Vacant Classes",
       y = "Count of Transactions", x = "")

```

For brevity here, note that this dataset has no missing observations, but some of the factor levels are labeled as `"Missing"`.  Expect that some of the variables are highly correlated with one another. Others have no influence on the `TotalValue` of the property. For a deeper discussion on pre-assessing features, see [Practial Machine Learning in R](https://jimgruman.netlify.app/post/practical-machine-learning-in-r/)

Let's explore the the continuous variables and their counts by histogram.

```{r histogram, fig.align="center", warning = FALSE}
land_train %>%
  dplyr::select(-ID)%>%
  purrr::keep(is.numeric) %>%
  tidyr::pivot_longer(cols = Council:FacilFAR,
                      values_to = "value") %>%
  ggplot() +
  geom_histogram(mapping = aes(x=value,fill=name), color="black") +
  scale_x_continuous(n.breaks = 2)+
  facet_wrap(~ name, scales = "free") +
  theme(legend.position = "")+
  labs(title = "Manhattan Numeric Features Histogram", x = "", y = "Count of Observations")
```

Let's explore the the continuous variables and how they relate to  `TotalValue`.

```{r Exploratory, fig.align="center", warning = FALSE}
land_train %>%
  dplyr::select(-ID)%>%
  purrr::keep(is.numeric) %>%
  tidyr::pivot_longer(cols = Council:FacilFAR,
                      values_to = "value") %>%
  ggplot() +
  geom_point(mapping = aes(x=value,
                         y=TotalValue,
                         color=name), alpha = 0.4) +
  scale_x_continuous(n.breaks = 2, labels = scales::comma)+
  scale_y_continuous(n.breaks = 2, labels = scales::comma)+
  facet_wrap(~ name, scales = "free") +
  theme(legend.position = "")+
  labs(title = "Manhattan Numeric Features Distributions", x = "", y = "Total Value")
```

For this exercise, we will manually choose variables through the R formula interface. Note the use of the tilde `"~"` to separate the dependent variable from the other variables. The algorithm will automatically handle categorical factors. At the end, the `"-1"` excludes the y-intercept constant. 

Other features can be engineered from variables encoded as follows:

| Operator | Example | Meaning |
|----------|---------|---------|
|~|y~x|Model y as a function of x|
|+|y~a+b|Include columns a and b|
|-|y~a-b|Include a but exclude b|
|:|y~a:b|Estimate the interaction of a and b|
|\*|y~a*b|Include columns as well as their interaction (that is, y ~ a + b + a:b)
|\||y~a\|b|Estimate y as a function of a conditional on b|

```{r}
valueFormula <- TotalValue ~ FireService + 
    ZoneDist1 + ZoneDist2 + Class + LandUse + 
    OwnerType + LotArea + BldgArea + ComArea + 
    ResArea + OfficeArea + RetailArea + 
    GarageArea + FactryArea + NumBldgs + 
    NumFloors + UnitsRes + UnitsTotal + 
    LotFront + LotDepth + BldgFront + 
    BldgDepth + LotType + Landmark + BuiltFAR +
    Built + HistoricDistrict
```

```{r}
land_rec<- recipes::recipe(valueFormula, data = land_train) %>%
    step_zv(all_predictors()) %>%
    step_YeoJohnson(all_predictors(), -all_nominal()) %>% 
    step_normalize(all_predictors(), -all_nominal()) 

land_prep <- prep(land_rec)

df_train <- juice(land_prep)

```

A simple linear model is built with the `lm` function. In this case we assign it to `model1` to retrieve summary stats and a plot of the estimated coefficients.

The summary output is ugly. It includes $\beta$ coefficients for every variable, along with significance estimates as p-values. 

```{r Model1, warning=FALSE}
model1 <- lm(valueFormula, data=df_train)
# model1           # text output of the model 
# summary(model1)  # a compact model summary
coefplot(model1, sort='magnitude')

```

Looking closely at the Coefficient Plot, each $\beta$ weight is shown along with a bar corresponding to a confidence interval. Where the confidence interval overlaps with zero the p-values do not satisfy the threshold for significance. Rather than risk p-hacking through stepwise regression, this post offers another solution to feature selection. For other techniques for checking for problematic features, see [Practial Machine Learning in R](https://jimgruman.netlify.app/post/practical-machine-learning-in-r/)

## GLMnet

> See the full [Elastic Net](https://www.jaredlander.com/content/2015/11/LassoForEveryone.html#14) equation.

The conventional linear regression can be improved by adding penalty terms with two components, the $l_2$ component, also known as ridge, and $l_1$, also known as lasso.  The proportion of each component is $\alpha$, where an $\alpha = 1$ is entirely lasso and $\alpha = 0$ is entirely ridge.

With the GLMnet package, the input and output design matrices have to be built. For now, we will use the `useful` package functions to build them. Note that in GLMnet, even the collinear features can be included. The `build.x` will even encode categorical variables on all feature levels (rather than all - 1) with `contrasts = FALSE`, in spite of the collinearity. The design matrix is entirely numeric.

```{r}
landX_train <- useful::build.x(valueFormula, data=df_train,
                       contrasts=FALSE, sparse=TRUE)
landY_train <- useful::build.y(valueFormula, data=df_train)

# head(as.matrix(landX_train)) # preview of the design matrix
```

The `glmnet` function builds hundreds of models with varying lambda penalty. The underlying code is actually written in FORTRAN and is very fast. `coefpath` is an interactive visualization of each feature $\beta$ coefficient for the Log of lambda. You can see that as Log Lambda increases, feature coefficients drop to zero and individual features fall out of the model.

We will start by building a lasso-only model, where $\alpha = 1$, the default. This provides fast model fitting, the same order of computation as single ordinary least squares, variable selection and shrinkage, and more stable predictions.

```{r, out.width="100%"}
model2 <- glmnet(x=landX_train, y=landY_train, family='gaussian')
# plot(model2, xvar='lambda', label=TRUE)
# model2$beta
coefpath(model2)
```

So the question remains - "What is the optimal lambda?"

Cross validation is a technique that breaks the training set into folds and holds out a different set on each run to arrive at an optimal lambda averaged over all runs. For a Lasso glmnet:

```{r}
# lasso regression, to eliminate features
model3 <- cv.glmnet(x=landX_train, y=landY_train,
                    family='gaussian',
                    nfolds=5)
plot(model3)
```

The two vertical lines indicate the lambda values that correspond to A) the minimum Mean Squared Error and B) a simpler model with features no worse than 1 standard deviation of error worse.

As of 2020 it is common practice, to avoid overfitting, to use the cross validated 1se model.

```{r, out.width="100%"}
coefpath(model3)
```

Individual coefficient content for the lowest error and the 1 standard error lambda values are as follows:

```{r, warning=FALSE}
# best model, by min error
coefplot(model3, sort='magnitude', lambda='lambda.min') +
  labs(subtitle = "For Minimum Standard Error")
# smaller model, one standard deviation from min error
coefplot(model3, sort='magnitude', lambda='lambda.1se')+
  labs(subtitle = "For 1 Standard Error Step in Lambda from Minimum")
```

The GLMnet model with ridge penalty only is built by setting $\alpha = 0$. This technique addresses the challenge of highly correlated variables.

```{r, out.width="100%"}
# ridge regression
model4 <- cv.glmnet(x=landX_train, y=landY_train,
                    family='gaussian',
                    alpha=0,
                    nfolds=5)
coefpath(model4)
```
```{r, warning = FALSE}
coefplot(model4, sort='magnitude', lambda='lambda.1se')
```

So, how do we decide if we want lasso or ridge? One approach is to simply blend the two by setting $\alpha$, in this case for 90% lasso, 10% ridge.

```{r}
model5 <- cv.glmnet(x=landX_train, y=landY_train,
                    family='gaussian',
                    alpha=0.9,
                    nfolds=5)
coefpath(model5)

model5
```

Ultimately, the performance of the model is assessed on test data.

```{r}
land_test <- readRDS(here::here("content","post","GLMnet","data",'manhattan_Test.rds'))

landX_test <- build.x(valueFormula, data=bake(land_prep, new_data = land_test),
                      contrasts=FALSE, sparse=TRUE)

valuePredictions5 <- predict(model5, newx=landX_test, 
                             s='lambda.1se')
head(valuePredictions5)

```

```{r, warning=FALSE}
dplyr::tibble(
  .predict = valuePredictions5,
  actual = land_test$TotalValue
) %>%
  ggplot()+
  geom_point(aes(actual, .predict))+
  geom_abline(slope = 1, intercept = 0, color = "darkblue")+
  theme(plot.title.position = "plot")+
  labs(title = "Manhattan Real Estate Model GLMnet Predictive Performance")
```

----

### Did you find this page helpful? Consider sharing it ðŸ™Œ





