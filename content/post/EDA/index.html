---
date: "2021-01-26"
diagram: true
image: 
  caption: 'art by Allison Horst'
  focal_point: "TopRight"
  preview_only: false
title: 'Exploratory Data Analysis'
author: "Jim Gruman"
output: 
 blogdown::html_page:
  toc: false
categories: [R]
description: "Crafting a Data Narrative"
featured: false
draft: false
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>
<script src="{{< blogdown/postref >}}index_files/twitter-widget/widgets.js"></script>
<script src="{{< blogdown/postref >}}index_files/htmlwidgets/htmlwidgets.js"></script>
<script src="{{< blogdown/postref >}}index_files/plotly-binding/plotly.js"></script>
<script src="{{< blogdown/postref >}}index_files/typedarray/typedarray.min.js"></script>
<script src="{{< blogdown/postref >}}index_files/jquery/jquery.min.js"></script>
<link href="{{< blogdown/postref >}}index_files/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="{{< blogdown/postref >}}index_files/crosstalk/js/crosstalk.min.js"></script>
<link href="{{< blogdown/postref >}}index_files/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="{{< blogdown/postref >}}index_files/plotly-main/plotly-latest.min.js"></script>


<p>More often than not, the most beneficial analytics projects do not involve any artificial intelligence. The tangible financial return on analytics comes from insights delivered to the organization via Exploratory Data Analysis, or EDA.</p>
<p>Analytics professionals who focus on decision science, that is, people that use data to provide ground truth to the business, must be able to articulate what the EDA process is and what the standards are for the process. Analysts should be the best people available to find meaning in the data.</p>
<p>Crafting an analysis that takes a vast amount of privileged company data and converts it into a concise result could be considered an art. The act of figuring out what is meaningful mathematically, what the business cares about, and how to bridge the gap between the two is not something that most people will know how to do naturally.</p>
<p>This post will be a short outline of the major elements of Exploratory Data Analysis. I welcome your feedback, and especially from those that have extraordinary analysts on their teams.</p>
<div id="what-makes-a-good-analysis" class="section level1">
<h1>What makes a good analysis?</h1>
<p>According to the 2020 book <a href="https://www.manning.com/books/build-a-career-in-data-science">Build a Career in Data Science</a>:</p>
<ul>
<li><p><em>It answers the question</em>. An analysis starts with someone asking a question, and it must provide an answer to that question.</p></li>
<li><p><em>It is made quickly</em>. Businesses have deadlines. If the analysis takes too long to create, the decision will be made without the analysis.</p></li>
<li><p><em>It can be shared</em>. The work product needs to be shared, not only with the person that asked for the analysis, but also with whomever that person wants to share it with in turn.</p></li>
<li><p><em>It is self-contained</em>. Because you can’t predict who will see the analysis, it needs to be understandable on its own. Plots and tables have to have clear annotations, axes must be labeled, and explanations should be written down.</p></li>
<li><p><em>It can be revisited</em>. Most questions will be asked again in the future. Sometimes, answering subsequent versions of the same question is a matter of running the same script on fresh data.</p></li>
</ul>
<p>Ultimately, a good analysis is simply the work product that helps non-analysts do their own jobs.</p>
<blockquote class="twitter-tweet" data-width="550" data-lang="en" data-dnt="true" data-theme="light"><p lang="en" dir="ltr">There is so much more to <a href="https://twitter.com/hashtag/datascience?src=hash&amp;ref_src=twsrc%5Etfw">#datascience</a> than <a href="https://twitter.com/hashtag/coding?src=hash&amp;ref_src=twsrc%5Etfw">#coding</a> skills. Land your dream job; successfully develop and manage <a href="https://twitter.com/hashtag/datasci?src=hash&amp;ref_src=twsrc%5Etfw">#datasci</a> projects, adopt the soft skills that give your technical know-how a leg-up with Build a Career in Data Science: <a href="https://t.co/cVBIjBCmPI">https://t.co/cVBIjBCmPI</a> <a href="https://twitter.com/robinson_es?ref_src=twsrc%5Etfw">@robinson_es</a> <a href="https://twitter.com/skyetetra?ref_src=twsrc%5Etfw">@skyetetra</a> <a href="https://t.co/guzkmVLHif">pic.twitter.com/guzkmVLHif</a></p>&mdash; Manning Publications (@ManningBooks) <a href="https://twitter.com/ManningBooks/status/1237122435436118017?ref_src=twsrc%5Etfw">March 9, 2020</a></blockquote>

<p>The American mathematician John Tukey formally introduced the concept in 1961. The idea of summarizing a dataset’s key characteristics coincided with the development of early programming languages. This was also a time when scientists and engineers were working on new data-driven approaches related to early computing. For more than 60 years, EDA has been widely adopted as a core tenet of data science.</p>
<div id="the-analysis-plan" class="section level2">
<h2>The analysis plan</h2>
<p>Knowing whether the data that could plausibly answer the question is available is <em>really</em> important. An analysis plan should consist of actionable tasks that drill down into the dimensions of the business question and how they should be addressed. It should be shared with both the team manager and the client to secure an agreed-upon foundation for the work. For example, importing and cleaning data will require that files and security access be made available. Given the size and the nature of the dataset, cloud resources may need to be provisioned.</p>
<p>So, where do analytics plans fail?</p>
<blockquote>
<p>By far, the most common mistake is to solve the problems that nobody has.</p>
</blockquote>
<p>Every EDA plan steps through identifying potential issues that require remedial work before moving forward. This could be characterized as data cleaning or data checking. Generally speaking, the pattern here is:</p>
<ol style="list-style-type: decimal">
<li><p>Generate questions about the data.</p></li>
<li><p>Search for answers by visualizing, transforming, and modeling the data.</p></li>
<li><p>Use what is learned to refine the questions and generate new questions.</p></li>
</ol>
<p>Bear in mind that EDA is never a single linear process with a strict set of rules. During the initial phases of EDA every idea will be investigated. Some of these ideas will pan out, and most will be dead ends. As exploration continues, the analyst will discover a few particularly productive areas to write up and communicate.</p>
<blockquote>
<p>“Far better an approximate answer to the right question, which is often
vague, than an exact answer to the wrong question, which can always be made
precise.” — John Tukey</p>
</blockquote>
<p>EDA is fundamentally a creative process. And like most creative processes, the key to asking <em>quality</em> questions is to generate a large <em>quantity</em> of questions. It is always difficult to ask revealing questions at the start of the analysis because the analyst does not yet know what insights are contained in the dataset. On the other hand, each new question asked will expose a new aspect of the data and increase the chance of making a discovery.</p>
<div id="the-pitfalls-of-the-data-frame-interface" class="section level3">
<h3>The pitfalls of the data frame interface</h3>
<blockquote>
<p>This was the tendency of jobs to be adapted to tools, rather than adapting tools to jobs. <a href="https://en.wikipedia.org/wiki/Silvan_Tomkins">Silvan Tomkins</a>, Computer Simulation of Personality: Frontier of Psychological Theory (1963)</p>
</blockquote>
<p>Data analysis tooling has coalesced around the <strong>data frame</strong> object, a huge productivity boost for the analyst. In some settings we talk about self-service, or even “citizen” data scientists that have access to the prepared data frame structures. There is a risk here that EDA, because of the ease and uniformity of use of the tooling, becomes an exercise in applying templates. This results in a creativity trap where the analysis can be constrained by the templates in a particular set of tools. While reporting tools are powerful when they genuinely support the analyst in developing an understanding of the data narrative, its important to avoid becoming too reliant on any single tool.</p>
<p>In my experience its best to have familiarity with tooling at several levels of abstraction, whether python or R, Tableau or PowerBI. High level interfaces to auto-generate certain types of exploratory analysis are handy when they provide just what is needed. However, the majority of EDA is more creative in nature and becoming expert with data manipulation tools like dplyr and pandas in combination with graphical tools like matplotlib and ggplot2 provides finer control and fewer restrictions on creativity.</p>
<p>The main point here is that exploratory data analysis cannot and should not be automated, because it is a process to support human learning, and to do that well there are few shortcuts. As an analogy, think of this like running a document through a spellchecker versus reading it yourself. While software is useful for spotting typos and grammatical errors, only a critical human eye can detect the nuance. EDA is similar in this respect, in that tools can help, but it requires our own intuition to make sense of it. This personal, in-depth insight will support detailed data analysis further down the line.</p>
<hr />
<p><a href="#Conclusion">TL;DR - skip to the Conclusion</a></p>
<hr />
<p>At this point, all of the technical tools of EDA will be needed: visualization, transformation, and modeling. For the purposes of this blog post, we will organize activities under these headings:</p>
<ul>
<li><p>Feature names and types</p></li>
<li><p>Missingness</p></li>
<li><p>Variance and covariance</p></li>
<li><p>Patterns and modeling feature inferences</p></li>
</ul>
<p>For illustrative purposes, let’s load a few libraries with plotting functions and a sample <a href="https://allisonhorst.github.io/palmerpenguins/">penguin</a> dataset.</p>
<pre class="r"><code>suppressPackageStartupMessages({
library(tidyverse) # tidy data manipulation, including dplyr and ggplot
library(systemfonts) # register Windows Fonts
library(palmerpenguins) # penguin sample dataset
library(GGally) #ggplot2 extensions
})

theme_set(theme_minimal())  # a clean style for plotting </code></pre>
</div>
<div id="feature-names-and-types" class="section level3">
<h3>Feature names and types</h3>
<p>Analysts are often handed sample data in Excel or a .csv text file at the first engagement of a project. In my experience, always, always check for the correctness of the column types. Expect to find integers incorrectly coded as strings, dates coded as strings, timestamps in odd time zones, non-English character sets, and other issues. Sometimes a column that should be numeric has the occasional string entry. Perhaps the query did not specify the correct schema when reading the database, or the data are encoded in an ambiguous way that results in an inappropriate type, or maybe some earlier data manipulation caused a problem.</p>
<p>Going further, let’s define some terms:</p>
<ul>
<li><p>A <strong>variable</strong> is a quantity, quality, or property that is measured. Often shown in columns, these are also known in data science as features.</p></li>
<li><p>A <strong>value</strong> is the state of a variable when measured. The value of a variable may change from measurement to measurement.</p></li>
<li><p>An <strong>observation</strong> is a set of measurements made under similar conditions. Usually all of the measurements in an observation are taken at the same time and on the same object. An observation will contain several values, each associated with a different variable. We sometimes refer to an observation as an instance.</p></li>
<li><p><strong>Tabular data</strong> is a set of values, each associated with a variable and an observation. Tabular data is <em>tidy</em> if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row.</p></li>
<li><p><strong>Dimensionality</strong> represents the number of variables, or features, of a dataset. The higher the dimensionality, the more we know about the data, but also the higher the computational complexity. There are approaches to reducing the dimensionality without losing predictive power.</p></li>
<li><p><strong>Sparsity and Density</strong> describe the degree to which data exists for the features in a dataset. For example, if 20 percent of the values in a dataset are missing, the dataset is said to be 20 percent sparse. Density is the complement of sparsity. Geospatial, survival, natural language processing, and text mining datasets are often sparse.</p></li>
<li><p><strong>Resolution</strong> describes the grain or level of detail in the data. The more detailed the data, the finer (or higher) the resolution. Satellite imagery consists of pixels on a grid spaced at some 3 meters or so on the ground. Aerial imagery has somewhat higher resolution, and images taken at ground level can have very high resolution.</p></li>
</ul>
<p>Descriptive statistics are useful in building understanding. They involve the use of measures to describe characteristics of variables. For example, categorical variables are described in the frequencies, or counts, of their occurrence in the dataset. For continuous variables, summary measures like mean and median are often useful.</p>
<p>For the penguins dataset:</p>
<pre class="r"><code>summary(penguins)</code></pre>
<pre><code>##       species          island    bill_length_mm bill_depth_mm  flipper_length_mm
##  Adelie   :152   Biscoe   :168   Min.   :32.1   Min.   :13.1   Min.   :172      
##  Chinstrap: 68   Dream    :124   1st Qu.:39.2   1st Qu.:15.6   1st Qu.:190      
##  Gentoo   :124   Torgersen: 52   Median :44.5   Median :17.3   Median :197      
##                                  Mean   :43.9   Mean   :17.2   Mean   :201      
##                                  3rd Qu.:48.5   3rd Qu.:18.7   3rd Qu.:213      
##                                  Max.   :59.6   Max.   :21.5   Max.   :231      
##                                  NA&#39;s   :2      NA&#39;s   :2      NA&#39;s   :2        
##   body_mass_g       sex           year     
##  Min.   :2700   female:165   Min.   :2007  
##  1st Qu.:3550   male  :168   1st Qu.:2007  
##  Median :4050   NA&#39;s  : 11   Median :2008  
##  Mean   :4202                Mean   :2008  
##  3rd Qu.:4750                3rd Qu.:2009  
##  Max.   :6300                Max.   :2009  
##  NA&#39;s   :2</code></pre>
</div>
<div id="missingness" class="section level3">
<h3>Missingness</h3>
<p>We must check the prevalence of missing values and their relationship with other features. There is a story about a marine mammal weights dataset where the missing values are those whales that exceeded the capacity of the scale. Remedies here might include dropping or transforming columns, imputing missing values as the median, or choosing an algorithm that handles missingness out of the box. As a worse case, it may be necessary to solicit more data.</p>
<p>We should make every effort to understand what makes observations with missing values different versus observations with recorded values. For example, in the sample dataset <code>nycflights13::flights</code>, missing values in the <code>dep_time</code> variable indicate that the flight was cancelled. It is important to compare the scheduled departure times for cancelled and non-cancelled times.</p>
<pre class="r"><code>nycflights13::flights %&gt;% 
  mutate(
    cancelled = is.na(dep_time),
    sched_hour = sched_dep_time %/% 100,
    sched_min = sched_dep_time %% 100,
    sched_dep_time = sched_hour + sched_min / 60
  ) %&gt;% 
  ggplot(mapping = aes(sched_dep_time)) + 
    geom_freqpoly(mapping = aes(colour = cancelled), binwidth = 1/4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>This is also a good example of an unbalanced dataset, where the class of a variable of interest may be overwhelmed by the other classes. There are analytics techniques for synthesizing upsampling, or resampling, to prepare the data to better understand the underlying patterns.</p>
</div>
<div id="variance-and-covariance" class="section level3">
<h3>Variance and covariance</h3>
<p><strong>Variance</strong> is the tendency of the values of a single variable to change from measurement to measurement. Variation is easily observed in real life; measuring any continuous variable twice, there will often be two different results. This is true even when measuring quantities that are constant, like the speed of light. Each measurement will include a small amount of error that varies from measurement to measurement. Every variable has its own pattern of variation. The best way to understand that pattern is to visualize the distribution of the variable’s values.</p>
<p>The means of visualizing the distribution of a variable will depend on whether the variable is categorical or continuous. A variable is <strong>categorical</strong> if it can only take one of a small set of values. To examine the distribution of counts of a categorical variable, use a bar chart:</p>
<pre class="r"><code>ggplot(data = penguins) +
  geom_bar(mapping = aes(x = island, fill = island), show.legend = FALSE)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The height of the bars displays how many observations occurred with each value. A table that displays the same information:</p>
<pre class="r"><code>penguins %&gt;% 
  count(island)</code></pre>
<pre><code>## # A tibble: 3 x 2
##   island        n
## * &lt;fct&gt;     &lt;int&gt;
## 1 Biscoe      168
## 2 Dream       124
## 3 Torgersen    52</code></pre>
<p>A variable is <strong>continuous</strong> if it can take any of an infinite set of ordered values. Numbers and date-times are two examples of continuous variables. To examine the distribution of a continuous variable, use a histogram:</p>
<pre class="r"><code>ggplot(data = penguins) +
  geom_histogram(mapping = aes(x = bill_length_mm), binwidth = 2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Or, as a summary table, in bins:</p>
<pre class="r"><code>penguins %&gt;% 
  count(cut_width(bill_length_mm, 2))</code></pre>
<pre><code>## # A tibble: 16 x 2
##    `cut_width(bill_length_mm, 2)`     n
##  * &lt;fct&gt;                          &lt;int&gt;
##  1 [31,33]                            1
##  2 (33,35]                           10
##  3 (35,37]                           31
##  4 (37,39]                           40
##  5 (39,41]                           38
##  6 (41,43]                           34
##  7 (43,45]                           23
##  8 (45,47]                           58
##  9 (47,49]                           31
## 10 (49,51]                           47
## 11 (51,53]                           20
## 12 (53,55]                            4
## 13 (55,57]                            3
## 14 (57,59]                            1
## 15 (59,61]                            1
## 16 &lt;NA&gt;                               2</code></pre>
<p>A histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. In the graph above, the tallest bar shows that almost 58 observations have a <code>bill_length_mm</code> value between 45 and 47, which are the left and right edges of the bar.</p>
<p>More than one histogram could be plotted together, but a better approach graphically may be to use lines instead, as it’s often easier to understand overlapping lines than bars.</p>
<pre class="r"><code>ggplot(data = penguins, mapping = aes(x = bill_length_mm, colour = species)) +
  geom_freqpoly(binwidth = 2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Now that we can visualize variation, what should we look for in plots? And what type of follow-up questions should we ask? I’ve put together a list below of the most useful types of information that to find in graphs, along with some follow-up questions for each type of information. The key to asking good follow-up questions will be to rely on curiosity as well as skepticism (How could this be misleading?).</p>
<p>In both bar charts and histograms, tall bars show the common values of a variable, and shorter bars show less-common values. Places that do not have bars reveal values that were not seen in the data. To turn this information into useful questions, look for anything unexpected:</p>
<ul>
<li><p>Which values are the most common? Why?</p></li>
<li><p>Which values are rare? Does that match expectations?</p></li>
<li><p>Can we see any unusual patterns? What might explain them?</p></li>
</ul>
<p>Clusters of similar values suggest that subgroups exist in the data. To understand the subgroups, ask:</p>
<ul>
<li><p>How are the observations within each cluster similar to each other?</p></li>
<li><p>How are the observations in separate clusters different from each other?</p></li>
<li><p>How can you explain or describe the clusters?</p></li>
<li><p>Why might the appearance of clusters be misleading?</p></li>
</ul>
<p>The histogram below shows the length (in minutes) of 272 eruptions of the Old Faithful Geyser in Yellowstone National Park. Eruption times appear to be clustered into two groups: there are short eruptions (of around 2 minutes) and long eruptions (4-5 minutes), but little in between.</p>
<pre class="r"><code>ggplot(data = faithful, mapping = aes(x = eruptions)) + 
  geom_histogram(binwidth = 0.25)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Many of the questions above will prompt us to explore a relationship <em>between</em> variables to see if the values of one variable can explain the behavior of another variable.</p>
<p><strong>Outliers</strong> are observations that are unusual; data points that don’t seem to fit the pattern. Sometimes outliers are data entry errors; other times outliers suggest important new science. When there is a lot of data, outliers are sometimes difficult to see in a histogram. For example, take the distribution of the <code>y</code> variable from the diamonds dataset. The only evidence of outliers is the unusually wide limits on the x-axis.</p>
<pre class="r"><code>ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>It’s good practice to repeat the analysis with and without the outliers. If they have minimal effect on the results, it may be reasonable to replace them with missing values and move on. However, if they have a substantial effect on the results, do not drop them without justification. Figure out what caused them (e.g. a data entry error) and disclose that they were removed in the write-up.</p>
<p>If variation describes the behavior <em>within</em> a variable, covariation describes the behavior <em>between</em> variables. <strong>Covariation</strong> is the tendency for the values of two or more variables to vary together in a related way. The best way to spot covariation is to visualize the relationship between two or more variables.</p>
<p>It’s common to want to explore the distribution of a continuous variable broken down by a categorical variable, as in the previous frequency polygon. To make the comparison easier we need to swap what is displayed on the y-axis. Instead of displaying count, we’ll display <strong>density</strong>, which is the count standardized so that the area under each frequency polygon is one.</p>
<pre class="r"><code>ggplot(data = penguins, mapping = aes(x = body_mass_g, y = ..density..)) + 
  geom_freqpoly(mapping = aes(colour = species), binwidth = 100)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>This may be a little hard for most audiences to interpret. There’s a lot going on in this plot.</p>
<p>An alternative: a <strong>boxplot</strong> is a type of visual shorthand for a distribution of values that is popular among statisticians. Each boxplot consists of:</p>
<ul>
<li><p>A box that stretches from the 25th percentile of the distribution to the 75th percentile, a distance known as the interquartile range (IQR). In the middle of the box is a line that displays the median, i.e. 50th percentile.</p></li>
<li><p>Visual points that display observations that fall more than 1.5 times the IQR from either edge of the box. These outlying points are unusual so are plotted individually.</p></li>
<li><p>A line (or whisker) that extends from each end of the box and goes to the farthest non-outlier point in the distribution.</p></li>
</ul>
<p>Let’s take a look at the distribution of body mass by species:</p>
<pre class="r"><code>ggplot(data = penguins, mapping = aes(x = species, y = body_mass_g       )) +
  geom_boxplot()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>We see less information about the distribution, but the boxplots are much more compact so we can more easily compare them (and fit more on one plot).</p>
<p>With long variable names, the boxplot works better if flipped by 90°. In this case, we will add the original data points, with “jitter” (random spacing off from the center axis), and color for each species.</p>
<pre class="r"><code>ggplot(data = penguins, 
       mapping = aes(y = reorder(species, body_mass_g, FUN = median), 
                     x = body_mass_g,
                     color = species)) +
  geom_boxplot(show.legend = FALSE) +
  geom_point(alpha = 0.5, position = &quot;jitter&quot;, show.legend = FALSE) +
  labs(y = NULL)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>To visualize the covariation between categorical variables, we will need to count the number of observations for each combination. One approach:</p>
<pre class="r"><code>ggplot(data = penguins) +
  geom_count(mapping = aes(x = island, y = species))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>The size of each circle in the plot displays how many observations occurred at each combination of values. Covariation will appear as a strong correlation between specific x values and specific y values.</p>
<p>Another approach is to calculate the counts:</p>
<pre class="r"><code>penguins %&gt;% 
  count(species, island)</code></pre>
<pre><code>## # A tibble: 5 x 3
##   species   island        n
##   &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;
## 1 Adelie    Biscoe       44
## 2 Adelie    Dream        56
## 3 Adelie    Torgersen    52
## 4 Chinstrap Dream        68
## 5 Gentoo    Biscoe      124</code></pre>
<p>Then visualize as a tile map with a fill aesthetic:</p>
<pre class="r"><code>penguins %&gt;% 
  count(species, island) %&gt;%  
  ggplot(mapping = aes(x = species, y = island)) +
    geom_tile(mapping = aes(fill = n))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>We’ve already seen one great way to visualize the covariation between two continuous variables: draw a scatterplot. Note the covariation as a pattern in the points. For example, observe the relationship between the body mass and bill length of the penguins:</p>
<pre class="r"><code>ggplot(data = penguins) +
  geom_point(mapping = aes(x = body_mass_g, y = bill_length_mm    ))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Scatterplots become less useful as the size of the dataset grows, because points begin to overplot and pile up into areas of uniform black. One way to fix the problem is to add transparency (here, called <code>alpha</code>). Again, we will add color as species.</p>
<pre class="r"><code>ggplot(data = penguins) + 
  geom_point(mapping = aes(x = body_mass_g, 
                           y = bill_length_mm, 
                           color = species), alpha = 0.5)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Another option is to bin one continuous variable so it acts like a categorical variable. Then use one of the techniques for visualizing the combination of a categorical and a continuous variable. For example, bin body_mass_g and then for each group, and then display a boxplot:</p>
<pre class="r"><code>ggplot(data = penguins, mapping = aes(x = body_mass_g, y = bill_length_mm)) + 
  geom_boxplot(mapping = aes(group = cut_width(body_mass_g, 100)))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>The cuts as used above divides <code>x</code> into bins of width <code>width</code>. By default, boxplots look roughly the same (apart from number of outliers) regardless of how many observations there are, so it’s difficult to tell that each boxplot summarizes a different number of points.</p>
<p>Another approach is to display approximately the same number of points in each bin:</p>
<pre class="r"><code>ggplot(data = penguins, mapping = aes(x = body_mass_g, y = bill_length_mm)) + 
  geom_boxplot(mapping = aes(group = cut_number(body_mass_g, 20)))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>For small datasets, a combination <code>GGally</code> visualization can be built to illustrate all of the pairwise relationships. Unfortunately, the graphic that looks reasonable with only three variables is not so useful with more, particularly for non-technical audiences.</p>
<pre class="r"><code>GGally::ggpairs(data = penguins, 
                columns = c(&quot;island&quot;, 
                            &quot;bill_length_mm&quot;, 
                            &quot;body_mass_g&quot;),
                mapping = aes(color = species),
                progress = FALSE) </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
<div id="patterns-and-models" class="section level3">
<h3>Patterns and models</h3>
<p>Patterns in data provide clues about relationships. If a systematic relationship exists between two variables it will appear as a pattern in the data. When spotting a pattern, ask:</p>
<ul>
<li><p>Could this pattern be due to coincidence (i.e. random chance)?</p></li>
<li><p>How can you describe the relationship implied by the pattern?</p></li>
<li><p>How strong is the relationship implied by the pattern?</p></li>
<li><p>What other variables might affect the relationship?</p></li>
<li><p>Does the relationship change if you look at individual subgroups of the data?</p></li>
</ul>
<p>A scatterplot of Old Faithful eruption lengths versus the wait time between eruptions shows a pattern: longer wait times are associated with longer eruptions. The scatterplot also displays the two clusters that we noticed above.</p>
<pre class="r"><code>ggplot(data = faithful) + 
  geom_point(mapping = aes(x = eruptions, y = waiting))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Patterns provide one of the most useful tools for data scientists because they reveal covariation. If two variables have covariance, the values of one variable may interact with the values of the second. If the covariation is due to a causal relationship (a special case), then the interaction should be explored.</p>
<p>Machine learning models can provide additional insight via inference. Just briefly here, we simply fit a linear model for penguin body mass against the bill length and the species.</p>
<pre class="r"><code>mod &lt;- lm(body_mass_g ~ bill_length_mm + species, data = penguins)

broom::tidy(mod)</code></pre>
<pre><code>## # A tibble: 4 x 5
##   term             estimate std.error statistic  p.value
##   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)         154.     269.       0.572 5.68e- 1
## 2 bill_length_mm       91.4      6.89    13.3   1.16e-32
## 3 speciesChinstrap   -886.      88.3    -10.0   6.37e-21
## 4 speciesGentoo       579.      75.4      7.68  1.76e-13</code></pre>
<p>We find here that each additional mm of bill length is related to a 91.4 gram increase in body mass, for a penguin of the same species. More sophisticated machine learning techniques can be used to discover complex patterns and interactions.</p>
<p>Another modeling tool, <strong>Survival</strong> analysis is one of the oldest subfields of statistics. Today survival analysis models are important in Engineering, Insurance, Marketing, Medicine, Econometrics, and many other application areas.</p>
<p>A simpler characterization of Survival might be time-to-event analysis. These are use cases where we are concerned with the time it takes for an <em>event</em> to occur after an <em>exposure</em>. As an example, an exposure might be the date of a medical diagnosis. Or it could be the moment of childbirth. Or the date of delivery of a new tractor to a customer. Survival analysis data is aligned for all participants to zero at each participants exposure time. The event of interest could be the failure of a component of the tractor.</p>
<p>Most survival datasets have some censoring. A censored study subject may be censored due to:</p>
<ul>
<li><p>Loss to follow-up (i.e. undocumented, not recorded)</p></li>
<li><p>Withdrawal from study</p></li>
<li><p>No event of interest by the end of the fixed study period</p></li>
</ul>
<p>Censored subjects still provide information so must be appropriately included in the analysis. Ignoring censoring leads to an overestimate of the overall survival probability, because the censored subjects only contribute information for part of the follow-up time, and then fall out of the risk set, thus pulling down the cumulative probability of survival.</p>
<p>In the heavy equipment industry it is necessary to track component warranty claims and cost on machines sold. Warranty datasets are heavily censored because the study period rarely aligns with the warranty period for each machine delivered in a fleet. The study time period and fleet population membership must be carefully considered and explicitly defined.</p>
<p>We have special methods in survival for time to event data, first, because duration times are always positive. Traditional regression models make a number of assumptions for independence and normality that are no longer valid in this domain. There are distributions, like Weibull, that better represent the problem space.</p>
<p><strong>Time Series</strong> modeling, sometimes also called forecasting, seeks to leverage the underlying patterns in data with a timestamp feature to make statements about causal inference or to predict the future. Time is a challenging feature to work with, in that the variables are not observations taken at once. Further, they are almost never independent, and will often be auto-correlated at intervals, or lags. This time series example is annual US Gross Domestic Product, in Billions of $US, pulled from the Federal Reserve web API with the <code>fredr</code> package.</p>
<pre class="r"><code>fredr::fredr(series_id = &#39;GDPC1&#39;) %&gt;% 
  timetk::plot_time_series(.date_var = date,
                           .value = value,
                           .smooth = FALSE,
                           .title = &quot;Federal Reserve Economic Data | US Real Gross Domestic Product&quot;)</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"data":[{"x":["1947-01-01","1947-04-01","1947-07-01","1947-10-01","1948-01-01","1948-04-01","1948-07-01","1948-10-01","1949-01-01","1949-04-01","1949-07-01","1949-10-01","1950-01-01","1950-04-01","1950-07-01","1950-10-01","1951-01-01","1951-04-01","1951-07-01","1951-10-01","1952-01-01","1952-04-01","1952-07-01","1952-10-01","1953-01-01","1953-04-01","1953-07-01","1953-10-01","1954-01-01","1954-04-01","1954-07-01","1954-10-01","1955-01-01","1955-04-01","1955-07-01","1955-10-01","1956-01-01","1956-04-01","1956-07-01","1956-10-01","1957-01-01","1957-04-01","1957-07-01","1957-10-01","1958-01-01","1958-04-01","1958-07-01","1958-10-01","1959-01-01","1959-04-01","1959-07-01","1959-10-01","1960-01-01","1960-04-01","1960-07-01","1960-10-01","1961-01-01","1961-04-01","1961-07-01","1961-10-01","1962-01-01","1962-04-01","1962-07-01","1962-10-01","1963-01-01","1963-04-01","1963-07-01","1963-10-01","1964-01-01","1964-04-01","1964-07-01","1964-10-01","1965-01-01","1965-04-01","1965-07-01","1965-10-01","1966-01-01","1966-04-01","1966-07-01","1966-10-01","1967-01-01","1967-04-01","1967-07-01","1967-10-01","1968-01-01","1968-04-01","1968-07-01","1968-10-01","1969-01-01","1969-04-01","1969-07-01","1969-10-01","1970-01-01","1970-04-01","1970-07-01","1970-10-01","1971-01-01","1971-04-01","1971-07-01","1971-10-01","1972-01-01","1972-04-01","1972-07-01","1972-10-01","1973-01-01","1973-04-01","1973-07-01","1973-10-01","1974-01-01","1974-04-01","1974-07-01","1974-10-01","1975-01-01","1975-04-01","1975-07-01","1975-10-01","1976-01-01","1976-04-01","1976-07-01","1976-10-01","1977-01-01","1977-04-01","1977-07-01","1977-10-01","1978-01-01","1978-04-01","1978-07-01","1978-10-01","1979-01-01","1979-04-01","1979-07-01","1979-10-01","1980-01-01","1980-04-01","1980-07-01","1980-10-01","1981-01-01","1981-04-01","1981-07-01","1981-10-01","1982-01-01","1982-04-01","1982-07-01","1982-10-01","1983-01-01","1983-04-01","1983-07-01","1983-10-01","1984-01-01","1984-04-01","1984-07-01","1984-10-01","1985-01-01","1985-04-01","1985-07-01","1985-10-01","1986-01-01","1986-04-01","1986-07-01","1986-10-01","1987-01-01","1987-04-01","1987-07-01","1987-10-01","1988-01-01","1988-04-01","1988-07-01","1988-10-01","1989-01-01","1989-04-01","1989-07-01","1989-10-01","1990-01-01","1990-04-01","1990-07-01","1990-10-01","1991-01-01","1991-04-01","1991-07-01","1991-10-01","1992-01-01","1992-04-01","1992-07-01","1992-10-01","1993-01-01","1993-04-01","1993-07-01","1993-10-01","1994-01-01","1994-04-01","1994-07-01","1994-10-01","1995-01-01","1995-04-01","1995-07-01","1995-10-01","1996-01-01","1996-04-01","1996-07-01","1996-10-01","1997-01-01","1997-04-01","1997-07-01","1997-10-01","1998-01-01","1998-04-01","1998-07-01","1998-10-01","1999-01-01","1999-04-01","1999-07-01","1999-10-01","2000-01-01","2000-04-01","2000-07-01","2000-10-01","2001-01-01","2001-04-01","2001-07-01","2001-10-01","2002-01-01","2002-04-01","2002-07-01","2002-10-01","2003-01-01","2003-04-01","2003-07-01","2003-10-01","2004-01-01","2004-04-01","2004-07-01","2004-10-01","2005-01-01","2005-04-01","2005-07-01","2005-10-01","2006-01-01","2006-04-01","2006-07-01","2006-10-01","2007-01-01","2007-04-01","2007-07-01","2007-10-01","2008-01-01","2008-04-01","2008-07-01","2008-10-01","2009-01-01","2009-04-01","2009-07-01","2009-10-01","2010-01-01","2010-04-01","2010-07-01","2010-10-01","2011-01-01","2011-04-01","2011-07-01","2011-10-01","2012-01-01","2012-04-01","2012-07-01","2012-10-01","2013-01-01","2013-04-01","2013-07-01","2013-10-01","2014-01-01","2014-04-01","2014-07-01","2014-10-01","2015-01-01","2015-04-01","2015-07-01","2015-10-01","2016-01-01","2016-04-01","2016-07-01","2016-10-01","2017-01-01","2017-04-01","2017-07-01","2017-10-01","2018-01-01","2018-04-01","2018-07-01","2018-10-01","2019-01-01","2019-04-01","2019-07-01","2019-10-01","2020-01-01","2020-04-01","2020-07-01"],"y":[2033.061,2027.639,2023.452,2055.103,2086.017,2120.45,2132.598,2134.981,2105.562,2098.38,2120.044,2102.251,2184.872,2251.507,2338.514,2383.291,2415.66,2457.517,2508.166,2513.69,2540.55,2546.022,2564.401,2648.621,2697.855,2718.709,2703.411,2662.482,2649.755,2652.643,2682.601,2735.091,2813.212,2858.988,2897.598,2914.993,2903.671,2927.665,2925.035,2973.179,2992.219,2985.663,3014.919,2983.727,2906.274,2925.379,2993.068,3063.085,3121.936,3192.38,3194.653,3203.759,3275.757,3258.088,3274.029,3232.009,3253.826,3309.059,3372.581,3438.721,3500.054,3531.683,3575.07,3586.827,3625.981,3666.669,3747.278,3771.845,3851.366,3893.296,3954.121,3966.335,4062.311,4113.629,4205.086,4301.973,4406.693,4421.747,4459.195,4495.777,4535.591,4538.37,4581.309,4615.853,4709.993,4788.688,4825.799,4844.779,4920.605,4935.564,4968.164,4943.935,4936.594,4943.6,4989.159,4935.693,5069.746,5097.179,5139.128,5151.245,5245.974,5365.045,5415.712,5506.396,5642.669,5704.098,5674.1,5727.96,5678.713,5692.21,5638.411,5616.526,5548.156,5587.8,5683.444,5759.972,5889.5,5932.711,5965.265,6008.504,6079.494,6197.686,6309.514,6309.652,6329.791,6574.39,6640.497,6729.755,6741.854,6749.063,6799.2,6816.203,6837.641,6696.753,6688.794,6813.535,6947.042,6895.559,6978.135,6902.105,6794.878,6825.876,6799.781,6802.497,6892.144,7048.982,7189.896,7339.893,7483.371,7612.668,7686.059,7749.151,7824.247,7893.136,8013.674,8073.239,8148.603,8185.303,8263.639,8308.021,8369.93,8460.233,8533.635,8680.162,8725.006,8839.641,8891.435,9009.913,9101.508,9170.977,9238.923,9257.128,9358.289,9392.251,9398.499,9312.937,9269.367,9341.642,9388.845,9421.565,9534.346,9637.732,9732.979,9834.51,9850.973,9908.347,9955.641,10091.049,10188.954,10327.019,10387.382,10506.372,10543.644,10575.1,10665.06,10737.478,10817.896,10998.322,11096.976,11212.205,11284.587,11472.137,11615.636,11715.393,11832.486,11942.032,12091.614,12287,12403.293,12498.694,12662.385,12877.593,12924.179,13160.842,13178.419,13260.506,13222.69,13299.984,13244.784,13280.859,13397.002,13478.152,13538.072,13559.032,13634.253,13751.543,13985.073,14145.645,14221.147,14329.523,14464.984,14609.876,14771.602,14839.782,14972.054,15066.597,15267.026,15302.705,15326.368,15456.928,15493.328,15582.085,15666.738,15761.967,15671.383,15752.308,15667.032,15328.027,15155.94,15134.117,15189.222,15356.058,15415.145,15557.277,15671.967,15750.625,15712.754,15825.096,15820.7,16004.107,16129.418,16198.807,16220.667,16239.138,16382.964,16403.18,16531.685,16663.649,16616.54,16841.475,17047.098,17143.038,17305.752,17422.845,17486.021,17514.062,17613.264,17668.203,17764.388,17876.179,17977.299,18054.052,18185.636,18359.432,18530.483,18654.383,18752.355,18813.923,18950.347,19020.599,19141.744,19253.959,19010.848,17302.511,18596.521],"text":["date: 1947-01-01<br />.value:  2033","date: 1947-04-01<br />.value:  2028","date: 1947-07-01<br />.value:  2023","date: 1947-10-01<br />.value:  2055","date: 1948-01-01<br />.value:  2086","date: 1948-04-01<br />.value:  2120","date: 1948-07-01<br />.value:  2133","date: 1948-10-01<br />.value:  2135","date: 1949-01-01<br />.value:  2106","date: 1949-04-01<br />.value:  2098","date: 1949-07-01<br />.value:  2120","date: 1949-10-01<br />.value:  2102","date: 1950-01-01<br />.value:  2185","date: 1950-04-01<br />.value:  2252","date: 1950-07-01<br />.value:  2339","date: 1950-10-01<br />.value:  2383","date: 1951-01-01<br />.value:  2416","date: 1951-04-01<br />.value:  2458","date: 1951-07-01<br />.value:  2508","date: 1951-10-01<br />.value:  2514","date: 1952-01-01<br />.value:  2541","date: 1952-04-01<br />.value:  2546","date: 1952-07-01<br />.value:  2564","date: 1952-10-01<br />.value:  2649","date: 1953-01-01<br />.value:  2698","date: 1953-04-01<br />.value:  2719","date: 1953-07-01<br />.value:  2703","date: 1953-10-01<br />.value:  2662","date: 1954-01-01<br />.value:  2650","date: 1954-04-01<br />.value:  2653","date: 1954-07-01<br />.value:  2683","date: 1954-10-01<br />.value:  2735","date: 1955-01-01<br />.value:  2813","date: 1955-04-01<br />.value:  2859","date: 1955-07-01<br />.value:  2898","date: 1955-10-01<br />.value:  2915","date: 1956-01-01<br />.value:  2904","date: 1956-04-01<br />.value:  2928","date: 1956-07-01<br />.value:  2925","date: 1956-10-01<br />.value:  2973","date: 1957-01-01<br />.value:  2992","date: 1957-04-01<br />.value:  2986","date: 1957-07-01<br />.value:  3015","date: 1957-10-01<br />.value:  2984","date: 1958-01-01<br />.value:  2906","date: 1958-04-01<br />.value:  2925","date: 1958-07-01<br />.value:  2993","date: 1958-10-01<br />.value:  3063","date: 1959-01-01<br />.value:  3122","date: 1959-04-01<br />.value:  3192","date: 1959-07-01<br />.value:  3195","date: 1959-10-01<br />.value:  3204","date: 1960-01-01<br />.value:  3276","date: 1960-04-01<br />.value:  3258","date: 1960-07-01<br />.value:  3274","date: 1960-10-01<br />.value:  3232","date: 1961-01-01<br />.value:  3254","date: 1961-04-01<br />.value:  3309","date: 1961-07-01<br />.value:  3373","date: 1961-10-01<br />.value:  3439","date: 1962-01-01<br />.value:  3500","date: 1962-04-01<br />.value:  3532","date: 1962-07-01<br />.value:  3575","date: 1962-10-01<br />.value:  3587","date: 1963-01-01<br />.value:  3626","date: 1963-04-01<br />.value:  3667","date: 1963-07-01<br />.value:  3747","date: 1963-10-01<br />.value:  3772","date: 1964-01-01<br />.value:  3851","date: 1964-04-01<br />.value:  3893","date: 1964-07-01<br />.value:  3954","date: 1964-10-01<br />.value:  3966","date: 1965-01-01<br />.value:  4062","date: 1965-04-01<br />.value:  4114","date: 1965-07-01<br />.value:  4205","date: 1965-10-01<br />.value:  4302","date: 1966-01-01<br />.value:  4407","date: 1966-04-01<br />.value:  4422","date: 1966-07-01<br />.value:  4459","date: 1966-10-01<br />.value:  4496","date: 1967-01-01<br />.value:  4536","date: 1967-04-01<br />.value:  4538","date: 1967-07-01<br />.value:  4581","date: 1967-10-01<br />.value:  4616","date: 1968-01-01<br />.value:  4710","date: 1968-04-01<br />.value:  4789","date: 1968-07-01<br />.value:  4826","date: 1968-10-01<br />.value:  4845","date: 1969-01-01<br />.value:  4921","date: 1969-04-01<br />.value:  4936","date: 1969-07-01<br />.value:  4968","date: 1969-10-01<br />.value:  4944","date: 1970-01-01<br />.value:  4937","date: 1970-04-01<br />.value:  4944","date: 1970-07-01<br />.value:  4989","date: 1970-10-01<br />.value:  4936","date: 1971-01-01<br />.value:  5070","date: 1971-04-01<br />.value:  5097","date: 1971-07-01<br />.value:  5139","date: 1971-10-01<br />.value:  5151","date: 1972-01-01<br />.value:  5246","date: 1972-04-01<br />.value:  5365","date: 1972-07-01<br />.value:  5416","date: 1972-10-01<br />.value:  5506","date: 1973-01-01<br />.value:  5643","date: 1973-04-01<br />.value:  5704","date: 1973-07-01<br />.value:  5674","date: 1973-10-01<br />.value:  5728","date: 1974-01-01<br />.value:  5679","date: 1974-04-01<br />.value:  5692","date: 1974-07-01<br />.value:  5638","date: 1974-10-01<br />.value:  5617","date: 1975-01-01<br />.value:  5548","date: 1975-04-01<br />.value:  5588","date: 1975-07-01<br />.value:  5683","date: 1975-10-01<br />.value:  5760","date: 1976-01-01<br />.value:  5890","date: 1976-04-01<br />.value:  5933","date: 1976-07-01<br />.value:  5965","date: 1976-10-01<br />.value:  6009","date: 1977-01-01<br />.value:  6079","date: 1977-04-01<br />.value:  6198","date: 1977-07-01<br />.value:  6310","date: 1977-10-01<br />.value:  6310","date: 1978-01-01<br />.value:  6330","date: 1978-04-01<br />.value:  6574","date: 1978-07-01<br />.value:  6640","date: 1978-10-01<br />.value:  6730","date: 1979-01-01<br />.value:  6742","date: 1979-04-01<br />.value:  6749","date: 1979-07-01<br />.value:  6799","date: 1979-10-01<br />.value:  6816","date: 1980-01-01<br />.value:  6838","date: 1980-04-01<br />.value:  6697","date: 1980-07-01<br />.value:  6689","date: 1980-10-01<br />.value:  6814","date: 1981-01-01<br />.value:  6947","date: 1981-04-01<br />.value:  6896","date: 1981-07-01<br />.value:  6978","date: 1981-10-01<br />.value:  6902","date: 1982-01-01<br />.value:  6795","date: 1982-04-01<br />.value:  6826","date: 1982-07-01<br />.value:  6800","date: 1982-10-01<br />.value:  6802","date: 1983-01-01<br />.value:  6892","date: 1983-04-01<br />.value:  7049","date: 1983-07-01<br />.value:  7190","date: 1983-10-01<br />.value:  7340","date: 1984-01-01<br />.value:  7483","date: 1984-04-01<br />.value:  7613","date: 1984-07-01<br />.value:  7686","date: 1984-10-01<br />.value:  7749","date: 1985-01-01<br />.value:  7824","date: 1985-04-01<br />.value:  7893","date: 1985-07-01<br />.value:  8014","date: 1985-10-01<br />.value:  8073","date: 1986-01-01<br />.value:  8149","date: 1986-04-01<br />.value:  8185","date: 1986-07-01<br />.value:  8264","date: 1986-10-01<br />.value:  8308","date: 1987-01-01<br />.value:  8370","date: 1987-04-01<br />.value:  8460","date: 1987-07-01<br />.value:  8534","date: 1987-10-01<br />.value:  8680","date: 1988-01-01<br />.value:  8725","date: 1988-04-01<br />.value:  8840","date: 1988-07-01<br />.value:  8891","date: 1988-10-01<br />.value:  9010","date: 1989-01-01<br />.value:  9102","date: 1989-04-01<br />.value:  9171","date: 1989-07-01<br />.value:  9239","date: 1989-10-01<br />.value:  9257","date: 1990-01-01<br />.value:  9358","date: 1990-04-01<br />.value:  9392","date: 1990-07-01<br />.value:  9398","date: 1990-10-01<br />.value:  9313","date: 1991-01-01<br />.value:  9269","date: 1991-04-01<br />.value:  9342","date: 1991-07-01<br />.value:  9389","date: 1991-10-01<br />.value:  9422","date: 1992-01-01<br />.value:  9534","date: 1992-04-01<br />.value:  9638","date: 1992-07-01<br />.value:  9733","date: 1992-10-01<br />.value:  9835","date: 1993-01-01<br />.value:  9851","date: 1993-04-01<br />.value:  9908","date: 1993-07-01<br />.value:  9956","date: 1993-10-01<br />.value: 10091","date: 1994-01-01<br />.value: 10189","date: 1994-04-01<br />.value: 10327","date: 1994-07-01<br />.value: 10387","date: 1994-10-01<br />.value: 10506","date: 1995-01-01<br />.value: 10544","date: 1995-04-01<br />.value: 10575","date: 1995-07-01<br />.value: 10665","date: 1995-10-01<br />.value: 10737","date: 1996-01-01<br />.value: 10818","date: 1996-04-01<br />.value: 10998","date: 1996-07-01<br />.value: 11097","date: 1996-10-01<br />.value: 11212","date: 1997-01-01<br />.value: 11285","date: 1997-04-01<br />.value: 11472","date: 1997-07-01<br />.value: 11616","date: 1997-10-01<br />.value: 11715","date: 1998-01-01<br />.value: 11832","date: 1998-04-01<br />.value: 11942","date: 1998-07-01<br />.value: 12092","date: 1998-10-01<br />.value: 12287","date: 1999-01-01<br />.value: 12403","date: 1999-04-01<br />.value: 12499","date: 1999-07-01<br />.value: 12662","date: 1999-10-01<br />.value: 12878","date: 2000-01-01<br />.value: 12924","date: 2000-04-01<br />.value: 13161","date: 2000-07-01<br />.value: 13178","date: 2000-10-01<br />.value: 13261","date: 2001-01-01<br />.value: 13223","date: 2001-04-01<br />.value: 13300","date: 2001-07-01<br />.value: 13245","date: 2001-10-01<br />.value: 13281","date: 2002-01-01<br />.value: 13397","date: 2002-04-01<br />.value: 13478","date: 2002-07-01<br />.value: 13538","date: 2002-10-01<br />.value: 13559","date: 2003-01-01<br />.value: 13634","date: 2003-04-01<br />.value: 13752","date: 2003-07-01<br />.value: 13985","date: 2003-10-01<br />.value: 14146","date: 2004-01-01<br />.value: 14221","date: 2004-04-01<br />.value: 14330","date: 2004-07-01<br />.value: 14465","date: 2004-10-01<br />.value: 14610","date: 2005-01-01<br />.value: 14772","date: 2005-04-01<br />.value: 14840","date: 2005-07-01<br />.value: 14972","date: 2005-10-01<br />.value: 15067","date: 2006-01-01<br />.value: 15267","date: 2006-04-01<br />.value: 15303","date: 2006-07-01<br />.value: 15326","date: 2006-10-01<br />.value: 15457","date: 2007-01-01<br />.value: 15493","date: 2007-04-01<br />.value: 15582","date: 2007-07-01<br />.value: 15667","date: 2007-10-01<br />.value: 15762","date: 2008-01-01<br />.value: 15671","date: 2008-04-01<br />.value: 15752","date: 2008-07-01<br />.value: 15667","date: 2008-10-01<br />.value: 15328","date: 2009-01-01<br />.value: 15156","date: 2009-04-01<br />.value: 15134","date: 2009-07-01<br />.value: 15189","date: 2009-10-01<br />.value: 15356","date: 2010-01-01<br />.value: 15415","date: 2010-04-01<br />.value: 15557","date: 2010-07-01<br />.value: 15672","date: 2010-10-01<br />.value: 15751","date: 2011-01-01<br />.value: 15713","date: 2011-04-01<br />.value: 15825","date: 2011-07-01<br />.value: 15821","date: 2011-10-01<br />.value: 16004","date: 2012-01-01<br />.value: 16129","date: 2012-04-01<br />.value: 16199","date: 2012-07-01<br />.value: 16221","date: 2012-10-01<br />.value: 16239","date: 2013-01-01<br />.value: 16383","date: 2013-04-01<br />.value: 16403","date: 2013-07-01<br />.value: 16532","date: 2013-10-01<br />.value: 16664","date: 2014-01-01<br />.value: 16617","date: 2014-04-01<br />.value: 16841","date: 2014-07-01<br />.value: 17047","date: 2014-10-01<br />.value: 17143","date: 2015-01-01<br />.value: 17306","date: 2015-04-01<br />.value: 17423","date: 2015-07-01<br />.value: 17486","date: 2015-10-01<br />.value: 17514","date: 2016-01-01<br />.value: 17613","date: 2016-04-01<br />.value: 17668","date: 2016-07-01<br />.value: 17764","date: 2016-10-01<br />.value: 17876","date: 2017-01-01<br />.value: 17977","date: 2017-04-01<br />.value: 18054","date: 2017-07-01<br />.value: 18186","date: 2017-10-01<br />.value: 18359","date: 2018-01-01<br />.value: 18530","date: 2018-04-01<br />.value: 18654","date: 2018-07-01<br />.value: 18752","date: 2018-10-01<br />.value: 18814","date: 2019-01-01<br />.value: 18950","date: 2019-04-01<br />.value: 19021","date: 2019-07-01<br />.value: 19142","date: 2019-10-01<br />.value: 19254","date: 2020-01-01<br />.value: 19011","date: 2020-04-01<br />.value: 17303","date: 2020-07-01<br />.value: 18597"],"type":"scatter","mode":"lines","line":{"width":1.88976377952756,"color":"rgba(44,62,80,1)","dash":"solid"},"hoveron":"points","showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null}],"layout":{"margin":{"t":43.7625570776256,"r":7.30593607305936,"b":25.5707762557078,"l":40.1826484018265},"plot_bgcolor":"rgba(255,255,255,1)","paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(44,62,80,1)","family":"","size":14.6118721461187},"title":{"text":"Federal Reserve Economic Data | US Real Gross Domestic Product","font":{"color":"rgba(44,62,80,1)","family":"","size":17.5342465753425},"x":0,"xref":"paper"},"xaxis":{"domain":[0,1],"automargin":true,"type":"date","autorange":true,"range":["1943-04-29","2024-03-04"],"tickmode":"auto","ticktext":["1960","1980","2000","2020"],"tickvals":[-3653,3652,10957,18262],"categoryorder":"array","categoryarray":["1960","1980","2000","2020"],"nticks":null,"ticks":"outside","tickcolor":"rgba(204,204,204,1)","ticklen":3.65296803652968,"tickwidth":0.22139200221392,"showticklabels":true,"tickfont":{"color":"rgba(44,62,80,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(204,204,204,1)","gridwidth":0.22139200221392,"zeroline":false,"anchor":"y","title":{"text":"","font":{"color":"rgba(44,62,80,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"automargin":true,"type":"linear","autorange":true,"range":[1161.92665,20115.48435],"tickmode":"auto","ticktext":["5000","10000","15000","20000"],"tickvals":[5000,10000,15000,20000],"categoryorder":"array","categoryarray":["5000","10000","15000","20000"],"nticks":null,"ticks":"outside","tickcolor":"rgba(204,204,204,1)","ticklen":3.65296803652968,"tickwidth":0.22139200221392,"showticklabels":true,"tickfont":{"color":"rgba(44,62,80,1)","family":"","size":11.689497716895},"tickangle":-0,"showline":false,"linecolor":null,"linewidth":0,"showgrid":true,"gridcolor":"rgba(204,204,204,1)","gridwidth":0.22139200221392,"zeroline":false,"anchor":"x","title":{"text":"","font":{"color":"rgba(44,62,80,1)","family":"","size":14.6118721461187}},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":"transparent","line":{"color":"rgba(44,62,80,1)","width":0.33208800332088,"linetype":"solid"},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(44,62,80,1)","family":"","size":11.689497716895}},"hovermode":"closest","barmode":"relative"},"config":{"doubleClick":"reset","showSendToCloud":false},"source":"A","attrs":{"2f5c1b77e59":{"x":{},"y":{},"type":"scatter"}},"cur_data":"2f5c1b77e59","visdat":{"2f5c1b77e59":["function (y) ","x"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p><strong>Geospatial</strong> and geospatial time-series modeling combine all of the challenges above and warrant their own detailed blog posts.</p>
<p><a id="Conclusion"></a></p>
</div>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<blockquote>
<p>When you claim to “grok” some knowledge or technique, you are asserting that you have not merely learned it in a detached instrumental way but that it has become part of you, part of your identity. <a href="http://catb.org/~esr/jargon/html/G/grok.html">‘grok’</a>, The Jargon File</p>
</blockquote>
<p>There is a persistent, unfounded myth that the data analyst is an introverted, robotic data processor. EDA has a crucial role in converting data from a contextless collection of bytes into meaningful insights. EDA is a creative journey that is supported by a continuing dialogue that probes the broader context.</p>
<p>Data itself is an incomplete and encoded representation of real world experiences. Part of the analyst’s role is to represent the story of how the data were generated. <a href="https://thedatasciencekernel.com/2021/01/24/exploratory-data-analysis-whats-the-point/">Alastair Rushworth</a> calls this story the data narrative. Part of the narrative might be the sequencing of events that lead to each data record coming into existence, or the data’s lineage in terms of the wrangling required. The data narrative completely frames the work, how we interpret every insight, and most importantly, the credibility which ultimately influences the audience.</p>
<p>It is common that analysts are neither domain experts nor are they provided with nice detailed documentation. In this case, the narrative is something that will be revealed through detective work, drawing on a combination of data analysis and the experience of collaborating with domain experts.</p>
<blockquote>
<p>Your goal during EDA is to develop an understanding of your data. The easiest way to do this is to use questions as tools to guide your investigation. When you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make. <a href="https://r4ds.had.co.nz/">R for Data Science</a> by Hadley Wickham</p>
</blockquote>
<p>EDA cannot happen in a computational vacuum. To do this well, we need to be alternating between interrogating the data and asking ourselves if what we find is consistent with our internal understanding of the data’s narrative. <em>Does what I see make sense to me? Would I feel comfortable to explain it to someone else?</em></p>
<p>Cassie Kozyrkov at Google, in <a href="https://www.linkedin.com/pulse/ultimate-guide-starting-ai-cassie-kozyrkov/">The ultimate guide to starting AI</a>, argues that:</p>
<blockquote>
<p>You can’t expect to get anything useful by asking wizards to sprinkle machine learning magic on your business without some effort from you first.</p>
</blockquote>
<p>In large organizations, the analyst must engage many domain experts, though they need not be internal if the data come from outside the organization. In my workplace, domain expert most often has the title “engineer,” though it could as easily be “scientist” or “product manager.” The best analysts always demand that they have access to the domain expert, as this person will supercharge the eventual analysis and will often be the difference between success or failure of the entire project. The key here is not about checking for correctness, but to grow understanding of the data: its important to remember that its one thing to be told something about the data narrative, but its much more meaningful to discover it expressed in the data.</p>
<hr />
<p>Excellent book references on the topics covered by this post:</p>
<blockquote>
<p><a href="https://r4ds.had.co.nz/">R for Data Science</a> Chapter 7</p>
</blockquote>
<blockquote>
<p><a href="https://www.wiley.com/en-us/Practical+Machine+Learning+in+R-p-9781119591535">Practical Machine Learning in R</a> Chapter 3</p>
</blockquote>
<blockquote>
<p><a href="https://www.manning.com/books/build-a-career-in-data-science">Build a Career in Data Science</a> Chapter 10</p>
</blockquote>
<p>Other posts:</p>
<blockquote>
<p><a href="https://rstudio.com/resources/rstudioconf-2017/opinionated-analysis-development/">Opinionated Analysis Development</a> Hilary Parker video</p>
</blockquote>
<blockquote>
<p><a href="https://careerfoundry.com/en/blog/data-analytics/exploratory-data-analysis/">What Is Exploratory Data Analysis?</a></p>
</blockquote>
<blockquote>
<p><a href="https://thedatasciencekernel.com/2021/01/24/exploratory-data-analysis-whats-the-point/">Exploratory Data Analysis: what’s the point?</a></p>
</blockquote>
<blockquote class="twitter-tweet" data-width="550" data-lang="en" data-dnt="true" data-theme="light"><p lang="en" dir="ltr">Loving the new colour cover for R4DS! On track for mid Dec printing. Pre-order: <a href="https://t.co/BMFYacy7x5">https://t.co/BMFYacy7x5</a>. Read for 🆓: <a href="https://t.co/48IKOWCtSc">https://t.co/48IKOWCtSc</a> <a href="https://t.co/2R1hFrR6v6">pic.twitter.com/2R1hFrR6v6</a></p>&mdash; Hadley Wickham (@hadleywickham) <a href="https://twitter.com/hadleywickham/status/804838201047339009?ref_src=twsrc%5Etfw">December 3, 2016</a></blockquote>

<hr />
<div id="did-you-find-this-page-helpful-consider-sharing-it" class="section level3">
<h3>Did you find this page helpful? Consider sharing it 🙌</h3>
</div>
</div>
