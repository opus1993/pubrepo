---
title: "Linear Regression"
author: "Jim Gruman"
date: '2020-01-01'
menu:
  example:
    parent: R Language Welcome
    weight: 3
linktitle: Linear Regression
draft: no
type: docs
weight: 1
---



<p>In statistics, modeling is where we get down to business. Models quantify the relationships between our variables, and let us make predictions.</p>
<p>A simple linear regression is the most basic model. It’s just two variables and is modeled as a linear relationship with an error term:</p>
<p><span class="math display">\[
y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon
\]</span></p>
<p>We are given the data for <em>x</em> and <em>y</em>. Our mission is to fit the model, which will give us the best estimates for <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span></p>
<p>This generalizes naturally to multiple linear regression, where we have multiple variables on the right hand side of the relationship:</p>
<p><span class="math display">\[
y_{i} = \beta_{0} + \beta_{1}u_{i} + \beta_{2}v_{i} + \beta_{3}w_{i} + \epsilon
\]</span></p>
<p>Statisticians call <em>u</em>, <em>v</em>, and <em>w</em> the <em>predictors</em> and <em>y</em> the <em>response</em>. Obviously, the model is only useful if there is a fairly linear relationship between the predictors and the response.</p>
<p>The beauty of R is that anyone can build these linear models. The models are all built the the function <em>lm</em>, which returns a model object. From the model object, we get coefficients(<span class="math inline">\(\beta_{i}\)</span>) and regression statistics. Easy!</p>
<p>The horror of R is that anyone can build these linear models. Nothing requires you to check that the model is reasonable, much less statistically significant. Before you blindly believe a model, check it! Most of the information you need is in the regression summary.</p>
<hr />
<pre class="r"><code>library(tidyverse)
library(lmtest)
library(patchwork)</code></pre>
<div id="simple-linear-regression" class="section level2">
<h2>Simple Linear Regression</h2>
<p>The problem: you have two vectors, <em>x</em> and <em>y</em>, that hold paired observations. You believe that there is a linear relationship between <em>x</em> and <em>y</em>.</p>
<p>The solution:</p>
<pre class="r"><code>## create dummy data
set.seed(42)
x &lt;- rnorm(100)
e &lt;- rnorm(100, mean = 0, sd = 5)
y &lt;- 5 + 15 * x + e  ##

lm(y ~ x)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Coefficients:
## (Intercept)            x  
##       4.558       15.136</code></pre>
<p>In this case, the regression equation is</p>
<p><span class="math display">\[
y_{i} = 4.558 + 15.136x_{i} + \epsilon
\]</span></p>
<p>More commonly, the data to be captured is inside of a data frame, in which case the regression is performed between two columns.</p>
<pre class="r"><code>df &lt;- data.frame(x, y)
head(df)</code></pre>
<pre><code>##            x         y
## 1  1.3709584 31.569204
## 2 -0.5646982  1.753283
## 3  0.3631284  5.430883
## 4  0.6328626 23.735349
## 5  0.4042683  7.730158
## 6 -0.1061245  3.935701</code></pre>
<p>The <em>lm</em> function lets you specify a data frame by using a <em>data</em> parameter. If you do , the function will take the variables from the data frame and not from the workspace.</p>
<pre class="r"><code>lm(y ~ x, data = df)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = df)
## 
## Coefficients:
## (Intercept)            x  
##       4.558       15.136</code></pre>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2>Multiple Linear Regression</h2>
<p>R uses the same <em>lm</em> function for both simple and multiple linear regression. You simply add more variables to the righthand side of the model formula.</p>
<pre class="r"><code>## create dummy data
set.seed(42)
u &lt;- rnorm(100)
v &lt;- rnorm(100, mean = 3, sd = 2)
w &lt;- rnorm(100, mean = -3, sd =1)
e &lt;- rnorm(100, mean = 0, sd = 5)
y &lt;- 5 + 4 * u + 3 * v + 2 * w  + e  

m&lt;-lm(y ~ u + v + w)

m</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ u + v + w)
## 
## Coefficients:
## (Intercept)            u            v            w  
##       4.617        4.289        3.022        1.842</code></pre>
<p>The results are pretty close to the equation originally created. The <em>data</em> parameter is especially valuable when the number of variables increases, since it’s much easier to keep data in one data frame than in many separate variables.</p>
</div>
<div id="getting-regression-statistics" class="section level2">
<h2>Getting Regression Statistics</h2>
<p>Assuming a regression model <em>m</em> from above</p>
<p>The following functions provide information:</p>
<p><code>anova(m)</code></p>
<p>ANOVA table</p>
<p><code>coefficients(m)</code></p>
<p>Model coefficients</p>
<p><code>coef(m)</code></p>
<p>Same as coefficients()</p>
<p><code>confint(m)</code></p>
<p>Confidence intervals for the coefficients</p>
<p><code>deviance(m)</code></p>
<p>Residual sum of squares</p>
<p><code>effects(m)</code></p>
<p>Vector of orthogonal effects</p>
<p><code>fitted(m)</code></p>
<p>Vector of fitted <em>y</em> values</p>
<p><code>residuals(m)</code></p>
<p>Model residuals</p>
<p><code>resid(m)</code></p>
<p>Same as residuals</p>
<p><code>vcov(m)</code></p>
<p>Variance-covariance matric of main parameters</p>
<pre class="r"><code>summary(m)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ u + v + w)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.9720 -2.9335 -0.5192  3.0941 11.6400 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.6171     1.6152   2.859  0.00522 ** 
## u             4.2888     0.4328   9.908 2.34e-16 ***
## v             3.0221     0.2473  12.222  &lt; 2e-16 ***
## w             1.8419     0.4441   4.147 7.26e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.434 on 96 degrees of freedom
## Multiple R-squared:  0.7387, Adjusted R-squared:  0.7305 
## F-statistic: 90.46 on 3 and 96 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Key statistics, such as the <em>F</em> statistic and the residual standard error. Read this from bottom to top, The <em>F</em> statistic tells you whether the model overall is significant or not. Conventionally, a p-value of less than 0.05 indicates that the model is likely significant, whereas values exceeding 0.05 indicate that the model is not. The wise statistician starts here.</p>
<p>A handy feature is that R flags the significant variables for quick identification. Did you notice the extremen righthand column containing triple asterisks (*)? Other values you might see in this column are double asterisks, a single asterisk, or a period. This column highlights the significant features. The line Signif. codes at the bottom gives a cryptic guide to the flags’ meanings.</p>
<hr />
</div>
<div id="performing-a-linear-regression-without-an-intercept" class="section level2">
<h2>Performing a Linear Regression without an Intercept</h2>
<p>lm(y ~ x + 0)</p>
<p>Simply add a constant 0 to the formula</p>
</div>
<div id="regressing-only-variables-that-highly-correlate-with-the-dependant-variable" class="section level2">
<h2>Regressing Only Variables that Highly Correlate with the Dependant Variable</h2>
<p>If you have a data frame with many variables, there are a number of approaches to build a multiple linear regression using only the variables that are highly correlated with the response variable.</p>
<p>By combining mapping functions, we can create a recipe to remove low-correlation variables from a set of predictors and use the high-correlation predictors in a regression.</p>
<p>We have an example data frame that contains six predictor variables named <em>pred1</em> through <em>pred6</em>. The response variable is named <em>resp</em>. Loading the data and dropping the resp variable is pretty straightforward. Look at the result of mapping the cor function:</p>
<pre class="r"><code>load(&quot;./data/pred.rdata&quot;)

pred %&gt;%
  select(-resp) %&gt;%
  map_dbl(cor, y = pred$resp) </code></pre>
<pre><code>##     pred1     pred2     pred3     pred4     pred5     pred6 
## 0.5734779 0.2790847 0.7530723 0.7985430 0.3222117 0.6074011</code></pre>
<p>The output is a named vector of values where the names are the variable names and the values are the pairwise correlations between each predictor variable and resp, the response variable. If we sort this vector, we get the correlations in decreasing order:</p>
<pre class="r"><code>pred %&gt;%
  select(-resp) %&gt;%
  map_dbl(cor, y = pred$resp) %&gt;%
  sort(decreasing = TRUE)</code></pre>
<pre><code>##     pred4     pred3     pred6     pred1     pred5     pred2 
## 0.7985430 0.7530723 0.6074011 0.5734779 0.3222117 0.2790847</code></pre>
<p>Using subsetting allows us to select the top four records. The . operator is a special operator that tells the pipe where to put the result of the prior step.</p>
<pre class="r"><code>pred %&gt;%
  select(-resp) %&gt;%
  map_dbl(cor, y = pred$resp) %&gt;%
  sort(decreasing = TRUE) %&gt;%
  .[1:4]</code></pre>
<pre><code>##     pred4     pred3     pred6     pred1 
## 0.7985430 0.7530723 0.6074011 0.5734779</code></pre>
<p>We then use the names function to extract the names from our vector. The names are the names of the columns we ultimately want to use as our independent variables:</p>
<pre class="r"><code>pred %&gt;%
  select(-resp) %&gt;%
  map_dbl(cor, y = pred$resp) %&gt;%
  sort(decreasing = TRUE) %&gt;%
  .[1:4] %&gt;%
  names</code></pre>
<pre><code>## [1] &quot;pred4&quot; &quot;pred3&quot; &quot;pred6&quot; &quot;pred1&quot;</code></pre>
<p>When we pass the vector of names into pred[.], the names are used to select columns from the pred data frame. We then use head to select only the top six rows for easier illustration:</p>
<pre class="r"><code>pred %&gt;%
  select(-resp) %&gt;%
  map_dbl(cor, y = pred$resp) %&gt;%
  sort(decreasing = TRUE) %&gt;%
  .[1:4] %&gt;%
  names %&gt;%
  pred[.] %&gt;%
  head</code></pre>
<pre><code>##        pred4       pred3      pred6      pred1
## 1  7.2517934  1.51270701  0.5599457  0.2059986
## 2  2.0764368  0.25792144 -0.1243682 -0.3610573
## 3 -0.6494477  0.08844023  0.6567396  0.7581632
## 4  1.3648958 -0.12089654  0.1219251 -0.7267048
## 5 -5.4440890 -1.19432890 -0.3910749 -1.3682810
## 6  2.5535014  0.61199690  1.2731020  0.4328180</code></pre>
<p>Now let’s bring it all together and pass the resulting data into the regression:</p>
<pre class="r"><code>best_pred &lt;- pred %&gt;%
  select(-resp) %&gt;%
  map_dbl(cor, y = pred$resp) %&gt;%
  sort(decreasing = TRUE) %&gt;%
  .[1:4] %&gt;%
  names %&gt;%
  pred[.]

mod &lt;- lm(pred$resp ~ as.matrix(best_pred))

summary(mod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = pred$resp ~ as.matrix(best_pred))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.4848 -0.6191  0.1893  0.5623  1.3979 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)                 1.1167     0.3404   3.281  0.00505 **
## as.matrix(best_pred)pred4   0.5230     0.2066   2.531  0.02305 * 
## as.matrix(best_pred)pred3  -0.6928     0.8698  -0.796  0.43819   
## as.matrix(best_pred)pred6   1.1598     0.6817   1.701  0.10953   
## as.matrix(best_pred)pred1   0.3426     0.3589   0.955  0.35494   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9269 on 15 degrees of freedom
## Multiple R-squared:  0.838,  Adjusted R-squared:  0.7948 
## F-statistic:  19.4 on 4 and 15 DF,  p-value: 8.586e-06</code></pre>
<hr />
</div>
<div id="performing-linear-regression-with-interaction" class="section level2">
<h2>Performing Linear Regression with Interaction</h2>
<p>In regression, an interaction occurs when the product of two predictor variables is also a significant predictor (i.e., in addition to the predictor variables themselves). Suppose we have two predictors, u and v, and want to include their interaction in the regression. This is expressed by the following equation:</p>
<p><span class="math display">\[
y_{i} = \beta_{0} + \beta_{1}u_{i} + \beta_{2}v_{i} + \beta_{3}u_{i}v_{i} + \epsilon
\]</span></p>
<p>The R formula is simply y ~ u * v</p>
<p>Likewise, if you have three predictors (u, v, and w) and want to include all their interactions, separate them by asterisks: y ~ u * v * w</p>
<p>This corresponds to the regression equation:</p>
<p><span class="math display">\[
y_{i} = \beta_{0} + \beta_{1}u_{i} + \beta_{2}v_{i} + \beta_{3}w_{i} + \beta_{4}u_{i}v_{i} + \beta_{5}v_{i}w_{i} + \beta_{6}u_{i}w_{i} + \beta_{7}u_{i}v_{i}w_{i} + \epsilon
\]</span></p>
<p>Now we have all the first-order interactions and a second-order interaction</p>
<p>Sometimes, however, you may not want every possible interaction. You can explicitly specify a single product by using the colon operator (:). For example, u:v:w denotes the product term <span class="math inline">\(\beta_{7}u_{i}v_{i}w_{i}\)</span> but without all possible interactions. So the R formula:</p>
<p>It might seem odd that colon (:) means pure multiplication while asterisk (*) means both multiplication and inclusion of constituent terms. Again, this is because we normally incorporate the constituents when we include their interaction, so making that approach the default for asterisk makes sense.</p>
<p>There is some additional syntax for easily specifying many interactions:</p>
<p>(u + v + …)^2 Include all variables (u, v, …, w) and all their first-order interactions.</p>
<p>(u + v + …)^3 Include all variables (u, v, …, w) and all their second-order interactions.</p>
<p>Both the asterisk (*) and the colon (:) follow a “distributive law,” so the following notations are also allowed: x*(u + v + …) x:(u + v + …)</p>
<p>All this syntax gives you some flexibility in writing your formula. For example, these three formulas are equivalent:</p>
<p>y ~ u * v
y ~ u + v + u:v
y ~ (u + v) ^ 2</p>
</div>
<div id="selecting-the-best-regression-variables" class="section level2">
<h2>Selecting the Best Regression Variables</h2>
<p>The <em>step</em> function can perform stepwise regression, either forward or backward. Backward stepwise regression starts with many variables and removes the underperformers:</p>
<pre class="r"><code>full.model &lt;- lm(y ~ x1 + x2 + x3 + x4)

reduced.model &lt;- step(full.model, direction = &quot;backward&quot;)</code></pre>
<p>Forward stepwise regression starts with a few variables and adds new ones to improve the model until it cannot be improved further:</p>
<pre class="r"><code>min.model &lt;- lm(y ~ 1)
fwd.model &lt;-
  step(min.model,
       direction = &quot;forward&quot;,
       scope = (~ x1 + x2 + x3 + x4))</code></pre>
<p>When you have many predictors, it can be quite difficult to choose the best subset. Adding and removing individual variables affects the overall mix, so the search for “the best” can become tedious.</p>
<p>The step function automates that search. Backward stepwise regression is the easiest approach. Start with a model that includes all the predictors. We call that the full model. The model summary, shown here, indicates that not all predictors are statistically significant:</p>
<pre class="r"><code>set.seed(4)
n &lt;- 150
x1 &lt;- rnorm(n)
x2 &lt;- rnorm(n, 1, 2)
x3 &lt;- rnorm(n, 3, 1)
x4 &lt;- rnorm(n,-2, 2)
e &lt;- rnorm(n, 0, 3)
y &lt;- 4 + x1 + 5 * x3 + e

full.model &lt;- lm(y ~ x1 + x2 + x3 + x4)
summary(full.model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2 + x3 + x4)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.0321 -1.7739  0.1579  2.0325  6.6264 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.402245   0.807665   4.212 4.42e-05 ***
## x1           0.539366   0.259346   2.080   0.0393 *  
## x2           0.168305   0.122915   1.369   0.1730    
## x3           5.174102   0.239831  21.574  &lt; 2e-16 ***
## x4          -0.009818   0.129541  -0.076   0.9397    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.924 on 145 degrees of freedom
## Multiple R-squared:  0.7696, Adjusted R-squared:  0.7633 
## F-statistic: 121.1 on 4 and 145 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We want to eliminate the insignificant variables, so we use step to incrementally eliminate the underperformers. The result is called the reduced model:</p>
<pre class="r"><code>reduced.model &lt;- step(full.model, direction=&quot;backward&quot;)</code></pre>
<pre><code>## Start:  AIC=326.79
## y ~ x1 + x2 + x3 + x4
## 
##        Df Sum of Sq    RSS    AIC
## - x4    1       0.0 1239.7 324.80
## - x2    1      16.0 1255.7 326.72
## &lt;none&gt;              1239.6 326.79
## - x1    1      37.0 1276.6 329.20
## - x3    1    3979.1 5218.7 540.41
## 
## Step:  AIC=324.8
## y ~ x1 + x2 + x3
## 
##        Df Sum of Sq    RSS    AIC
## - x2    1      16.1 1255.8 324.73
## &lt;none&gt;              1239.7 324.80
## - x1    1      37.2 1276.9 327.23
## - x3    1    3988.3 5227.9 538.67
## 
## Step:  AIC=324.73
## y ~ x1 + x3
## 
##        Df Sum of Sq    RSS    AIC
## &lt;none&gt;              1255.8 324.73
## - x1    1      44.5 1300.3 327.95
## - x3    1    3974.0 5229.8 536.72</code></pre>
<p>The output from step shows the sequence of models that it explored. In this case, step removed x2 and x4 and left only x1 and x3 in the final (reduced) model. The summary of the reduced model shows that it contains only significant predictors:</p>
<pre class="r"><code>summary(reduced.model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x3)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.1478 -1.8497 -0.0554  2.0257  6.5499 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   3.6481     0.7510   4.857 3.01e-06 ***
## x1            0.5824     0.2552   2.282   0.0239 *  
## x3            5.1469     0.2386  21.568  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.923 on 147 degrees of freedom
## Multiple R-squared:  0.7666, Adjusted R-squared:  0.7635 
## F-statistic: 241.5 on 2 and 147 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Backward stepwise regression is easy, but sometimes it’s not feasible to start with “everything” because you have too many candidate variables. In that case use forward stepwise regression, which will start with nothing and incrementally add variables that improve the regression. It stops when no further improvement is possible.</p>
<p>A model that “starts with nothing” may look odd at first:</p>
<pre class="r"><code>min.model &lt;- lm(y ~ 1)</code></pre>
<p>This is a model with a response variable (y) but no predictor variables. (All the fitted values for y are simply the mean of y, which is what you would guess if no predictors were available.)</p>
<p>We must tell step which candidate variables are available for inclusion in the model. That is the purpose of the scope argument. scope is a formula with nothing on the lefthand side of the tilde (~) and candidate variables on the righthand side:</p>
<pre class="r"><code>fwd.model &lt;- step(
  min.model,
  direction = &quot;forward&quot;,
  scope = (~ x1 + x2 + x3 + x4),
  trace = 0
)</code></pre>
<p>Here we see that x1, x2, x3, and x4 are all candidates for inclusion. (We also included trace = 0 to inhibit the voluminous output from step.) The resulting model has two significant predictors and no insignificant predictors:</p>
<pre class="r"><code>summary(fwd.model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x3 + x1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.1478 -1.8497 -0.0554  2.0257  6.5499 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   3.6481     0.7510   4.857 3.01e-06 ***
## x3            5.1469     0.2386  21.568  &lt; 2e-16 ***
## x1            0.5824     0.2552   2.282   0.0239 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.923 on 147 degrees of freedom
## Multiple R-squared:  0.7666, Adjusted R-squared:  0.7635 
## F-statistic: 241.5 on 2 and 147 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The step-forward algorithm reached the same model as the step-backward model by including x1 and x3 but excluding x2 and x4. This is a toy example, so that is not surprising. In real applications, we suggest trying both the forward and the backward regression and then comparing the results. You might be surprised.</p>
<p>Finally, don’t get carried away with stepwise regression. It is not a panacea, it cannot turn junk into gold, and it is definitely not a substitute for choosing predictors carefully and wisely. You might think: “Oh boy! I can generate every possible interaction term for my model, then let step choose the best ones! What a model I’ll get!”</p>
<hr />
</div>
<div id="using-an-expression-inside-a-regression-formula" class="section level2">
<h2>Using an Expression Inside a Regression Formula</h2>
<p>If you want to regress on the sum of u and v, then this is your regression equation:</p>
<p><span class="math display">\[
y_{i} = \beta_{0} + \beta_{1}(u_{i} + v_{i}) + \epsilon_{i}
\]</span></p>
<p>This will not work: lm(y ~ u + v) ## not quite right</p>
<p>Here R will interpret u and v as two separate predictors, each with its own regression coefficient. Likewise, suppose your regression equation is:</p>
<p><span class="math display">\[
y_{i} = \beta_{0} + \beta_{1}u_{i} + \beta_{2}u_{i}^2 + \epsilon_{i}
\]</span></p>
<p>This will not work: lm(y ~ u + u^2) ## That’s an interaction, not a quadtrac term</p>
<p>The solution is to surround the expressions by the I(…) operator, which inhibits the expressions from being interpreted as a regression formula. Instead, it forces R to calculate the expression’s value and then incorporate that value directly into the regression. Thus, the first example becomes:</p>
<pre class="r"><code>lm(y ~ I(u + v))</code></pre>
<p>In response to that command, R computes u + v and then regresses y on the sum.</p>
<p>For the second example we use:</p>
<pre class="r"><code>lm(y ~ I(u ^ 2))</code></pre>
<p>ere R computes the square of u and then regresses on the sum u + u2.
All the basic binary operators (+, -, *, /, ^) have special meanings inside a regression formula. For this reason, you must use the I(…) operator whenever you incorporate calculated values into a regression.</p>
<p>A beautiful aspect of these embedded transformations is that R remembers them and applies them when you make predictions from the model. Consider the quadratic model described by the second example. It uses u and u^2, but we supply the value of u only and R does the heavy lifting. We don’t need to calculate the square of u ourselves:</p>
<pre class="r"><code>load(&#39;./data/df_squared.rdata&#39;)
m &lt;- lm(y ~ u + I(u ^ 2), data = df_squared)

predict(m, newdata = data.frame(u = 13.4))</code></pre>
<pre><code>##        1 
## 877.1426</code></pre>
</div>
<div id="regressing-a-polynomial" class="section level2">
<h2>Regressing a Polynomial</h2>
<pre class="r"><code>m &lt;- lm(y ~ poly(x, 3, raw = TRUE))</code></pre>
<p>The raw = TRUE is necessary. Without it, the poly function computes orthogonal polynomials instead of simple polynomials.</p>
<p>Beyond the convenience, a huge advantage is that R will calculate all those powers of x when you make predictions from the model. Without that, you are stuck calculating x2 and x3 yourself every time you employ the model.</p>
<p>Here is another good reason to use poly. You cannot write your regression formula in this way:</p>
<pre class="r"><code>lm(y ~ x + x^2 + x^3)     # Does not do what you think!</code></pre>
<p>R will interpret x^2 and x^3 as interaction terms, not as powers of x. The resulting model is a one-term linear regression, completely unlike your expectation.</p>
<p>Just use poly.</p>
</div>
<div id="regressing-on-transformed-data" class="section level2">
<h2>Regressing on Transformed Data</h2>
<p>Problem: You want to build a regression model for x and y, but they do not have a linear relationship.</p>
<p>A critical assumption behind the lm function for regression is that the variables have a linear relationship. To the extent this assumption is false, the resulting regression becomes meaningless.</p>
<p>Fortunately, many datasets can be transformed into a linear relationship before applying lm.</p>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;
## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<div class="figure"><span id="fig:obs-trans"></span>
<img src="/courses/R/LinearRegression_files/figure-html/obs-trans-1.png" alt="Example of a data transform" width="672" />
<p class="caption">
Figure 1: Example of a data transform
</p>
</div>
<p>This figure shows an example of exponential decay. The left panel shows the original data, z. The dotted line shows a linear regression on the original data; clearly, it’s a lousy fit. If the data is really exponential, then a possible model is:</p>
<p><em>z</em> = exp[<em>β</em><sub>0</sub> + <em>β</em><sub>1</sub><em>t</em> + <em>ε</em>]</p>
<p>where <em>t</em> is time and exp[] is the exponential function (<em>e</em><sup><em>x</em></sup>).
This is not linear, of course, but we can linearize it by taking
logarithms:</p>
<p>log(<em>z</em>) = <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub><em>t</em> + <em>ε</em></p>
<p>In R, that regression is simple because we can embed the log transform
directly into the regression formula:</p>
<pre class="r"><code>load(file = &#39;./data/df_decay.rdata&#39;)
z &lt;- df_decay$z
t &lt;- df_decay$time

# transform and model
m &lt;- lm(log(z) ~ t)
summary(m)</code></pre>
<pre><code>## 
## Call:
## lm(formula = log(z) ~ t)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.44790 -0.09926  0.00487  0.09784  0.28022 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.68868    0.03065   22.47   &lt;2e-16 ***
## t           -2.01179    0.03513  -57.27   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.148 on 98 degrees of freedom
## Multiple R-squared:  0.971,  Adjusted R-squared:  0.9707 
## F-statistic:  3280 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>You can embed other functions inside your formula. If you thought the relationship was quadratic, you could use a square-root transformation:</p>
<pre class="r"><code>lm(sqrt(y) ~ month)</code></pre>
<p>You can apply transformations to variables on both sides of the formula, of course. This formula regresses y on the square root of x:</p>
<pre class="r"><code>lm(y ~ sqrt(x))</code></pre>
<p>This regression is for a log-log relationship between x and y:</p>
<pre class="r"><code>lm(log(y) ~ log(x))</code></pre>
</div>
<div id="finding-the-best-power-transformation-box-cox-procedure" class="section level2">
<h2>Finding the best Power Transformation (Box-Cox Procedure)</h2>
<p>To improve a linear model that exhibits residuals that are not normally distributed, consider applying a power transformation to the response variable. By using the Box–Cox procedure, which is implemented by the <code>boxcox</code> function of the <code>MASS</code> package. The procedure will identify a power, <em>λ</em>, such that transforming <em>y</em> into <em>y</em><sup><em>λ</em></sup> will improve the fit of your model:</p>
<pre class="r"><code>library(MASS)
m &lt;- lm(y ~ x)
boxcox(m)</code></pre>
<p>To illustrate the Box–Cox transformation, let’s create some artificial
data using the equation <em>y</em><sup>–1.5</sup> = <em>x</em> + <em>ε</em>, where <em>ε</em> is an error
term:</p>
<pre class="r"><code>set.seed(9)
x &lt;- 10:100
eps &lt;- rnorm(length(x), sd = 5)
y &lt;- (x + eps) ^ (-1 / 1.5)</code></pre>
<p>Then we will (mistakenly) model the data using a simple linear
regression and derive an adjusted <em>R</em><sup>2</sup> of 0.637:</p>
<pre class="r"><code>m &lt;- lm(y ~ x)
summary(m)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.040315 -0.016332 -0.007921  0.009962  0.145160 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.1668848  0.0070779   23.58   &lt;2e-16 ***
## x           -0.0014653  0.0001161  -12.62   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.0291 on 89 degrees of freedom
## Multiple R-squared:  0.6414, Adjusted R-squared:  0.6374 
## F-statistic: 159.2 on 1 and 89 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>When plotting the residuals against the fitted values, we get a clue
that something is wrong. We can get a <code>ggplot</code> residual plot using the <code>broom</code> library. The <code>augment</code> function from <code>broom</code> will put our residuals (and other things) into a data frame for easier plotting. Then we can use <code>ggplot</code> to plot:</p>
<pre class="r"><code>library(broom)
augmented_m &lt;- augment(m)

ggplot(augmented_m, aes(x = .fitted, y = .resid)) + 
  geom_point()</code></pre>
<div class="figure"><span id="fig:fitplot1"></span>
<img src="/courses/R/LinearRegression_files/figure-html/fitplot1-1.png" alt="Fitted values versus residuals" width="672" />
<p class="caption">
Figure 2: Fitted values versus residuals
</p>
</div>
<p>This plot has a clear parabolic shape. A possible fix is a power transformation on <em>y</em>, so we run the Box–Cox procedure:</p>
<pre class="r"><code>library(MASS)</code></pre>
<pre><code>## Warning: package &#39;MASS&#39; was built under R version 3.6.2</code></pre>
<pre><code>## 
## Attaching package: &#39;MASS&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:patchwork&#39;:
## 
##     area</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     select</code></pre>
<pre class="r"><code>bc &lt;- boxcox(m)</code></pre>
<div class="figure"><span id="fig:boxcox"></span>
<img src="/courses/R/LinearRegression_files/figure-html/boxcox-1.png" alt="Output of boxcox on the model (m)" width="672" />
<p class="caption">
Figure 3: Output of boxcox on the model (m)
</p>
</div>
<p>The <code>boxcox</code> function plots values of <em>λ</em> against the log-likelihood of the resulting model. We want to
maximize that log-likelihood, so the function draws a line at the best
value and also draws lines at the limits of its confidence interval. In
this case, it looks like the best value is around –1.5, with a
confidence interval of about (–1.75, –1.25).</p>
<p>Oddly, the <code>boxcox</code> function does not return the best value of <em>λ</em>.
Rather, it returns the (<em>x</em>, <em>y</em>) pairs displayed in the plot. It’s
pretty easy to find the values of <em>λ</em> that yield the largest
log-likelihood <em>y</em>. We use the <code>which.max</code> function:</p>
<pre class="r"><code>which.max(bc$y)</code></pre>
<pre><code>## [1] 13</code></pre>
<p>Then this gives us the position of the corresponding <em>λ</em>:</p>
<pre class="r"><code>lambda &lt;- bc$x[which.max(bc$y)]
lambda</code></pre>
<pre><code>## [1] -1.515152</code></pre>
<p>The function reports that the best <em>λ</em> is –1.52. In an actual
application, we would urge you to interpret this number and choose the
power that makes sense to you—rather than blindly accepting this “best”
value. Use the graph to assist you in that interpretation. Here, we’ll go
with –1.52.</p>
<p>We can apply the power transform to <em>y</em> and then fit the revised model;
this gives a much better <em>R</em><sup>2</sup> of 0.967:</p>
<pre class="r"><code>z &lt;- y ^ lambda
m2 &lt;- lm(z ~ x)
summary(m2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = z ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -13.4589  -3.7111  -0.2282   2.2061  14.1882 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.64263    1.25167  -0.513    0.609    
## x            1.05140    0.02054  51.199   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.146 on 89 degrees of freedom
## Multiple R-squared:  0.9672, Adjusted R-squared:  0.9668 
## F-statistic:  2621 on 1 and 89 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>For those who prefer one-liners, the transformation can be embedded
right into the revised regression formula:</p>
<pre class="r"><code>m2 &lt;- lm(I(y ^ lambda) ~ x)</code></pre>
<p>By default, <code>boxcox</code> searches for values of <em>λ</em> in the range –2 to +2.
You can change that via the <code>lambda</code> argument; see the help page for
details.</p>
<p>Compare the transformed fit residuals with the original (no transformation).</p>
<pre class="r"><code>augmented_m2 &lt;- augment(m2)

ggplot(augmented_m2, aes(x = .fitted, y = .resid)) + 
  geom_point()</code></pre>
<div class="figure"><span id="fig:fitplot"></span>
<img src="/courses/R/LinearRegression_files/figure-html/fitplot-1.png" alt="Fitted values versus residuals: m2" width="672" />
<p class="caption">
Figure 4: Fitted values versus residuals: m2
</p>
</div>
</div>
<div id="diagnosing-a-linear-regression" class="section level2">
<h2>Diagnosing a Linear Regression</h2>
<p>Simply plotting the model object produces several useful diagnostic plots</p>
<p>R fosters the impression that linear regression is easy: just use the lm function. Yet fitting the data is only the beginning. It’s your job to decide whether the fitted model actually works and works well.</p>
<p>Before anything else, you must have a statistically significant model. Check the F statistic from the model summary and be sure that the p-value is small enough for your purposes. Conventionally, it should be less than 0.05 or else your model is likely not very meaningful.</p>
<p>The series of graphs here show the diagnostics for a not-so-good regression:</p>
<pre class="r"><code>load(file = &#39;./data/bad.rdata&#39;)
m &lt;- lm(y2 ~ x3 + x4)
par(mfrow = (c(2, 2)))      # this gives us a 2x2 plot
plot(m)</code></pre>
<p><img src="/courses/R/LinearRegression_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>Observe that the Residuals vs Fitted plot has a definite parabolic shape. This tells us that the model is incomplete: a quadratic factor is missing that could explain more variation in y. Other patterns in residuals would be suggestive of additional problems: a cone shape, for example, may indicate nonconstant variance in y. Interpreting those patterns is a bit of an art, so we suggest reviewing a good book on linear regression while evaluating the plot of residuals.</p>
<p>There are other problems with these not-so-good diagnostics. The normal Q–Q plot has more points off the line than it does for the good regression. Both the Scale–Location and Residuals vs Leverage plots show points scattered away from the center, which suggests that some points have excessive leverage.</p>
<p>Another pattern is that point number 28 sticks out in every plot. This warns us that something is odd with that observation.</p>
</div>
<div id="identifying-influential-observations" class="section level2">
<h2>Identifying Influential Observations</h2>
<p>The title of this recipe could be “Identifying <em>Overly</em> Influential Observations,” but that would be redundant. All observations influence the regression model, even if only a little. When a statistician says that an observation is influential, it means that removing the observation would significantly change the fitted regression model. We want to identify those observations because they might be outliers that distort our model; we owe it to ourselves to investigate them.</p>
<p>The <code>influence.measures</code> function reports several statistics: DFBETAS, DFFITS, covariance ratio, Cook’s distance, and hat matrix values. If any of these measures indicate that an observation is influential, the function flags that observation with an asterisk (<code>*</code>) along the righthand side:</p>
<pre class="r"><code>influence.measures(m)</code></pre>
<pre><code>## Influence measures of
##   lm(formula = y2 ~ x3 + x4) :
## 
##      dfb.1_   dfb.x3   dfb.x4    dffit cov.r   cook.d    hat inf
## 1  -0.18784  0.15174  0.07081 -0.22344 1.059 1.67e-02 0.0506    
## 2   0.27637 -0.04367 -0.39042  0.45416 1.027 6.71e-02 0.0964    
## 3  -0.01775 -0.02786  0.01088 -0.03876 1.175 5.15e-04 0.0772    
## 4   0.15922 -0.14322  0.25615  0.35766 1.133 4.27e-02 0.1156    
## 5  -0.10537  0.00814 -0.06368 -0.13175 1.078 5.87e-03 0.0335    
## 6   0.16942  0.07465  0.42467  0.48572 1.034 7.66e-02 0.1062    
## 7  -0.10128 -0.05936  0.01661 -0.13021 1.078 5.73e-03 0.0333    
## 8  -0.15696  0.04801  0.01441 -0.15827 1.038 8.38e-03 0.0276    
## 9  -0.04582 -0.12089 -0.01032 -0.14010 1.188 6.69e-03 0.0995    
## 10 -0.01901  0.00624  0.01740 -0.02416 1.147 2.00e-04 0.0544    
## 11 -0.06725 -0.01214  0.04382 -0.08174 1.113 2.28e-03 0.0381    
## 12  0.17580  0.35102  0.62952  0.74889 0.961 1.75e-01 0.1406    
## 13 -0.14288  0.06667  0.06786 -0.15451 1.071 8.04e-03 0.0372    
## 14 -0.02784  0.02366 -0.02727 -0.04790 1.173 7.85e-04 0.0767    
## 15  0.01934  0.03440 -0.01575  0.04729 1.197 7.66e-04 0.0944    
## 16  0.35521 -0.53827 -0.44441  0.68457 1.294 1.55e-01 0.2515   *
## 17 -0.09184 -0.07199  0.01456 -0.13057 1.089 5.77e-03 0.0381    
## 18 -0.05807 -0.00534 -0.05725 -0.08825 1.119 2.66e-03 0.0433    
## 19  0.00288  0.00438  0.00511  0.00761 1.176 1.99e-05 0.0770    
## 20  0.08795  0.06854  0.19526  0.23490 1.136 1.86e-02 0.0884    
## 21  0.22148  0.42533 -0.33557  0.64699 1.047 1.34e-01 0.1471    
## 22  0.20974 -0.19946  0.36117  0.49631 1.085 8.06e-02 0.1275    
## 23 -0.03333 -0.05436  0.01568 -0.07316 1.167 1.83e-03 0.0747    
## 24 -0.04534 -0.12827 -0.03282 -0.14844 1.189 7.51e-03 0.1016    
## 25 -0.11334  0.00112 -0.05748 -0.13580 1.067 6.22e-03 0.0307    
## 26 -0.23215  0.37364  0.16153 -0.41638 1.258 5.82e-02 0.1883   *
## 27  0.29815  0.01963 -0.43678  0.51616 0.990 8.55e-02 0.0986    
## 28  0.83069 -0.50577 -0.35404  0.92249 0.303 1.88e-01 0.0411   *
## 29 -0.09920 -0.07828 -0.02499 -0.14292 1.077 6.89e-03 0.0361    
## 30 -0.06440 -0.00352  0.04502 -0.07706 1.116 2.03e-03 0.0391    
## 31 -0.04034 -0.01952  0.02869 -0.05618 1.131 1.08e-03 0.0454    
## 32 -0.11122 -0.03203 -0.04491 -0.13525 1.064 6.16e-03 0.0292    
## 33 -0.08651 -0.09483 -0.00552 -0.14306 1.097 6.93e-03 0.0450    
## 34 -0.13427  0.03897 -0.04952 -0.15255 1.056 7.82e-03 0.0313    
## 35 -0.18901  0.13485  0.02355 -0.21439 1.038 1.53e-02 0.0414    
## 36 -0.10287  0.06496  0.07423 -0.12524 1.122 5.33e-03 0.0539    
## 37 -0.03795  0.04984 -0.03073 -0.07159 1.223 1.75e-03 0.1153    
## 38 -0.12395  0.09508 -0.06710 -0.17250 1.099 1.00e-02 0.0539    
## 39 -0.05975  0.02824 -0.05422 -0.09063 1.132 2.80e-03 0.0529    
## 40  0.03950 -0.01671 -0.04145  0.05383 1.160 9.92e-04 0.0675</code></pre>
<p>This is the model from earlier where we suspected that observation 28 was an outlier. An asterisk is flagging that observation, confirming that it’s overly influential.</p>
<p>This recipe can identify influential observations, but you shouldn’t reflexively delete them. Some judgment is required here. Are those observations improving your model or damaging it?</p>
</div>
<div id="testing-for-autocorrelation" class="section level2">
<h2>Testing for AutoCorrelation</h2>
<p>The Durbin–Watson test is often used in time series analysis, but it was originally created for diagnosing autocorrelation in regression residuals. Autocorrelation in the residuals is a scourge because it distorts the regression statistics, such as the F statistic and the t statistics for the regression coefficients. The presence of autocorrelation suggests that your model is missing a useful predictor variable or that it should include a time series component, such as a trend or a seasonal indicator.</p>
<p>This first example builds a simple regression model and then tests the residuals for autocorrelation. The test returns a p-value well above zero, which indicates that there is no significant autocorrelation:</p>
<pre class="r"><code>load(file = &#39;./data/ac.rdata&#39;)
m &lt;- lm(y1 ~ x)
dwtest(m)</code></pre>
<pre><code>## 
##  Durbin-Watson test
## 
## data:  m
## DW = 1.9391, p-value = 0.3793
## alternative hypothesis: true autocorrelation is greater than 0</code></pre>
<p>This second example exhibits autocorrelation in the residuals. The p-value is near zero, so the autocorrelation is likely positive:</p>
<pre class="r"><code>m &lt;- lm(y ~ x)           # Create a model object
dwtest(m)                # Test the model residuals</code></pre>
<p>The output includes a <em>p</em>-value. Conventionally, if <em>p</em> &lt; 0.05 then
the residuals are significantly correlated, whereas <em>p</em> &gt; 0.05
provides no evidence of correlation.</p>
<p>You can perform a visual check for autocorrelation by graphing the
<em>autocorrelation function</em> (ACF) of the residuals:</p>
<pre class="r"><code>acf(m)                   # Plot the ACF of the model residuals</code></pre>
</div>
<div id="predicting-new-values" class="section level2">
<h2>Predicting New Values</h2>
<p>Once you have a linear model, making predictions is quite easy because
the <code>predict</code> function does all the heavy lifting. The only annoyance is
arranging for a data frame to contain your data.</p>
<p>The <code>predict</code> function returns a vector of predicted values with one
prediction for every row in the data. The example in the Solution
contains one row, so <code>predict</code> returned one value.</p>
<p>If your predictor data contains several rows, you get one prediction per
row:</p>
<pre class="r"><code>preds &lt;- data.frame(
  u = c(3.0, 3.1, 3.2, 3.3),
  v = c(3.9, 4.0, 4.1, 4.2),
  w = c(5.3, 5.5, 5.7, 5.9)
)
predict(m, newdata = preds)</code></pre>
<pre><code>## Warning: &#39;newdata&#39; had 4 rows but variables found have 100 rows</code></pre>
<pre><code>##          1          2          3          4          5          6          7 
##  4.4366790 -1.6188107  2.6168972  8.2481850  5.6365627  5.0261738  6.6949659 
##          8          9         10         11         12         13         14 
##  4.5178481  4.8696991  3.2238086  3.4313099  8.8860294  5.8648789  3.7848935 
##         15         16         17         18         19         20         21 
##  7.5574453 -1.7430886  2.3534564  3.6521235  5.4023593  4.0809937  5.3213960 
##         22         23         24         25         26         27         28 
##  4.1495144  5.2444951  4.0261899  5.1880741  5.5706185  6.5014561  0.6698252 
##         29         30         31         32         33         34         35 
##  5.5600025  5.2520586  4.3655270  2.9430020  6.7649159  5.0565958  4.1109305 
##         36         37         38         39         40         41         42 
##  1.4272224  2.3483102  3.3394606  0.7794939  2.8908775  5.0201420  3.4125128 
##         43         44         45         46         47         48         49 
##  1.7070059  4.2896300  3.8076796  1.5627593  4.5703599  6.1142773  5.4531394 
##         50         51         52         53         54         55         56 
##  0.1593418  3.4142148  3.0020819  2.7564785  6.3891282  2.9100786  4.0315325 
##         57         58         59         60         61         62         63 
##  2.1847603  5.5136680  6.8187920  3.6049572  4.1297954  3.1391164  3.6968115 
##         64         65         66         67         68         69         70 
##  8.0596954  1.8359564  4.6740206  1.8254443  2.2082729  3.1889168  3.3144280 
##         71         72         73         74         75         76         77 
##  2.7429223  3.8845946  2.5983879  7.7418920  1.1888229  1.6682331  2.3369366 
##         78         79         80         81         82         83         84 
##  4.3330659  6.5833494  2.1617367  2.8981761  6.0050109  5.9290120  4.6383584 
##         85         86         87         88         89         90         91 
##  2.5141231  3.2863513  3.8875375  1.4566959  5.8298098  4.2833882  5.2618513 
##         92         93         94         95         96         97         98 
##  7.1955833  4.8906987  2.2494797  6.2144433  4.1813329  5.8632202  3.3672761 
##         99        100 
##  4.8768195  4.0710172</code></pre>
<p>In case it’s not obvious: the new data needn’t contain values for
response variables, only predictor variables. After all, you are trying
to <em>calculate</em> the response, so it would be unreasonable of R to expect
you to supply it.</p>
</div>
<div id="forming-prediction-intervals" class="section level2">
<h2>Forming Prediction Intervals</h2>
<p>Use the <code>predict</code> function and specify <code>interval = "prediction"</code>:</p>
<pre class="r"><code>predict(m, newdata = preds, interval = &quot;prediction&quot;)</code></pre>
</div>
