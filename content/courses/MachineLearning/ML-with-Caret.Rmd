---
author: "Jim Gruman"
output: html_document
date: "12/29/2019"
draft: false
linktitle: ML with Caret
menu:
  example:
    parent: Topic
    weight: 6
title: Machine Learning with Caret
type: docs
weight: 6
---

```{r s, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The caret package (short for **C**lassification **A**nd **RE**gression **T**raining) provides a unified interface and contains functions to streamline the model training process for complex regression and classification problems. It utilizes a number of other R packages but tries not to load them all at package start-up. Caret has built-in parallel processing for increased computational efficiency. 

As of this writing, version 6.0-84 was available at CRAN, currently maintained by Max Kuhn. The main help pages for the package are at [Caret at Github](https://topepo.github.io/caret/). Max includes extended examples and a large amount of information in the package vignettes.

```{r setup}
library(caret)
library(mlbench)
library(dplyr)
library(parallel)
library(doParallel)
cores<-detectCores()
doParallel::registerDoParallel(cores = cores)
library(pROC)
```

Caret has several functions that attempt to streamline the model building and evaluation process, as well as feature selection and other techniques.

One of the primary tools in the package is the train function which are be used to

> evaluate, using resampling, the effect of model tuning parameters on performance

> choose the ``optimal’’ model across these parameters

> estimate model performance 

Caret includes options for customizing almost every step of this process (e.g. resampling technique, choosing the optimal parameters etc).

As a quick demonstration, a linear model can be built on the widely known mtcars data to model mpg by hp as follows:

```{r lm}
data("mtcars")
set.seed(42)

model<-train(
    mpg ~ hp, mtcars,
    method = "lm",
)

model$method

model$finalModel$coefficients

model$coefnames

model$results

newdata<-data.frame(hp = seq(50,300,10))

newdata$mpg<-predict(model, newdata)

ggplot(mtcars, aes(hp, mpg))+
  geom_point()+
  geom_smooth(method = "lm", color = "red") +
  theme_minimal() +
  theme(legend.position = 'none')

```

For a slightly more complex demonstration, we look at Housing data for 506 census tracts of Boston from the 1970 census. 

```{r medv dataviz}
data("BostonHousing")

regVar <- c("age", "lstat", "tax")

featurePlot(x = BostonHousing[, regVar], 
            y = BostonHousing$medv, 
            plot = "scatter", 
            layout = c(3, 1))
```

In this case, we will model median home value medv in a regression by 12 numeric and one 2-level categorical feature. In this case, we will apply 5-fold cross validation. Simple bootstrap resampling is the default in trainControl

```{r medv lm}
# Fit lm model using 5-fold CV: model
model <- train(
  medv~., 
  BostonHousing,
  method = "lm",
  trControl = trainControl(
    method = "cv", 
    number = 5,
    verboseIter = TRUE
  )
)

# Print model to console
model$method

model$finalModel$coefficients

model$results

```

Another method, "repeatedcv", is used to specify repeated K-fold cross-validation (and the argument **repeats** controls the number of repetitions). **K** is controlled by the number argument and defaults to 10. The new syntax for an lm model using 5 x 5-fold CV: model is then:

```{r medv2 lm}
model <- train(
  medv ~ ., 
  BostonHousing,
  method = "lm",
  trControl = trainControl(
    method = "repeatedcv", 
    number = 5,
    repeats = 5, 
    verboseIter = TRUE
  )
)

# Print model to console
model$method

model$finalModel$coefficients

model$results

```

-----------------------

# Handling missing data

Most models require numbers and cannot handle missing data. A common approach is to remove rows entirely with missing data, which can create biases and generate over-confident models. A much better strategy is to replace the missing values with the median, if and only if the data are missing at random.

For linear regression models, it is a best practice to preProcess with medianImpute, centering, scaling, and then fit the glm model. Another option is to preProcess with knnImpute.

In the real world, some variables do non contain much information. For example, variables that are mostly constant should be removed because one fold could end up with that column. Nearly constant columns should also be removed. 

For example:
   preProcess = c("zv", "medianImpute", "center", "scale", "pca")

A Preprocessing cheat sheet:  

1) start with median imputation

2) consider knn imputation if the data missing is not at random

3) for linear models, center and scale

4) try PCA and spatial sign

> example preProcess = c("nzv", "center", "scale", "pca") 

```{r BloodBrain}
data("BloodBrain")

bloodbrain_x<-bbbDescr
bloodbrain_y<-logBBB

# Identify near zero variance predictors: remove_cols
remove_cols <- nearZeroVar(bloodbrain_x, names = TRUE, 
                           freqCut = 2, uniqueCut = 20)

# Get all column names from bloodbrain_x: all_cols
all_cols<-names(bloodbrain_x)

# Remove from data: bloodbrain_x_small
bloodbrain_x_small <- bloodbrain_x[ , setdiff(all_cols, remove_cols)]
```

OR  --- use train with preProcess and "nzv" in the character string

```{r BloodBrain model}
# Fit model on reduced data: model
model <- train(
  x = bloodbrain_x_small, 
  y = bloodbrain_y, 
  method = "glm"
)

# Print model to console
model 
```

## Principal components analysis as a pre-processing step in regression models

1) Combines low-variance and correlated variables

2) Aggregates into a single set of high-variance, perpendicular predictors

3) Prevents collinearity

```{r BloodBrain PCA}
set.seed(42)

model <- train(
  bbbDescr,
  logBBB,
  method = "glm",
  trControl = trainControl(
    method = "cv", number = 10, verbose = TRUE,
  ),
  preProcess = c("zv", "center", "scale", "pca")
)

min(model$results$RMSE)
```

-----------------

# The Sonar Dataset

## Partial Least Squares and Regularized Discriminant Analysis

The mlbench **Sonar** data set was used initially in a study of the classification of sonar signals to train a network to discriminate between signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock. Each of 208 observations are patterns in a set with 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.

The label associated with each record contains the letter "R" if the object is a rock and "M" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.

First, we split the data into two groups: a training set and a test set with the **createDataPartition** function. By default, **createDataPartition** does a stratified random split of the data. Here, a partial least squares discriminant analysis (PLSDA) model will be tuned over the number of PLS components that should be retained. 

We would like to customize it in a few ways:
- expand the set of PLS models that the function evaluates. By default, the function will tune over three values of each tuning parameter.
- the type of resampling used. The simple bootstrap is used by default. We will have the function use three repeats of 10-fold cross-validation.
- the methods for measuring performance. If unspecified, overall accuracy and the Kappa statistic are computed. For regression models, root mean squared error and R2 are computed. Here, the function will be altered to estimate the area under the ROC curve, the sensitivity and specificity

To change the candidate values of the tuning parameter, either of the **tuneLength** or **tuneGrid** arguments can be used. The train function can generate a candidate set of parameter values and the tuneLength argument controls how many are evaluated. In the case of PLS, the function uses a sequence of integers from 1 to tuneLength. If we want to evaluate all integers between 1 and 15, setting tuneLength = 15 would achieve this. The **tuneGrid** argument is used when specific values are desired. A data frame is used where each row is a tuning parameter setting and each column is a tuning parameter. An example is used below to illustrate this.

The function will pick the tuning parameters associated with the best results. Since we are using custom performance measures, the criterion that should be optimized must also be specified. In the call to train, we can use **metric** = "ROC" to do this.

```{r sonar}
# Create trainControl object: myControl
data("Sonar")
set.seed(42)

inTrain <- createDataPartition(
  y = Sonar$Class,
  ## the outcome data are needed
  p = .75,
  ## The percentage of data in the
  ## training set
  list = FALSE
)

training <- Sonar[ inTrain,]
testing  <- Sonar[-inTrain,]

myControl <- trainControl(
  method = "repeatedcv",
  repeats = 3,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!   must have to calculate AUC or logloss
#  verboseIter = TRUE
)

# Train pls with custom trainControl: model
plsModel<-train(
  Class~., 
  data = training,
  method = "pls",
  trControl = myControl,
  tuneLength = 15,
  preProc = c("center","scale"),
  metric = "ROC")

# Print model to console
plsModel

ggplot(plsModel)

# Let’s save the model. If you want to deploy it, you can push .rda file with your code to production.
# save(plsModel, file = "./plsModel.rda")

# Once you successfully save it, close the current R session. Then, you can load it back in the new session. It’s ready for use.
# load("plsModel.rda")
```

In this output the grid of results are the average resampled estimates of performance. The note at the bottom tells the user that 4 PLS components were found to be optimal. Based on this value, a final PLS model is fit to the whole data set using this specification and this is the model that is used to predict future samples.

The package has several functions for visualizing the results. One method for doing this is the ggplot function for train objects. The command ggplot(plsFit) produces the results seen in Figure and shows the relationship between the resampled performance values and the number of PLS components.

```{r sonar prediction}
plsClasses <- predict(plsModel, newdata = testing)
str(plsClasses)

plsProbs <- predict(plsModel, newdata = testing, type = "prob")
head(plsProbs)

```

**Caret** contains a function to compute the confusion matrix and associated statistics for the model fit:

```{r sonar confusion matrix pls}
confusionMatrix(data = plsClasses, testing$Class)

```

To fit an another model to the data, train can be invoked with minimal changes. Lists of the wide variety of models available can be found at (https://topepo.github.io/caret/available-models.html) or (https://topepo.github.io/caret/train-models-by-tag.html). For example, to fit a regularized discriminant model to these data, the following syntax is used:

```{r sonar rdm}
## To illustrate, a custom grid is used
rdaGrid = data.frame(gamma = (0:4)/4, lambda = 3/4)
set.seed(42)
rdaModel<- train(
  Class ~ .,
  data = training,
  method = "rda",
  tuneGrid = rdaGrid,
  trControl = myControl,
  metric = "ROC"
)
rdaModel

rdaClasses <- predict(rdaModel, newdata = testing)
confusionMatrix(rdaClasses, testing$Class)
```

How do these models compare in terms of their resampling results? The **resamples** function can be used to collect, summarize and contrast the resampling results. Since the random number seeds were initialized to the same value prior to calling train, the same folds were used for each model. To assemble them:

```{r resamples Sonar}
resamps <- resamples(list(pls = plsModel, rda = rdaModel))
summary(resamps)

xyplot(resamps, what = "BlandAltman")
```

The results look similar. Since, for each resample, there are paired results a paired t-test can be used to assess whether there is a difference in the average resampled area under the ROC curve. The diff.resamples function can be used to compute this:

```{r resamples Sonar ttest}
diffs <- diff(resamps)
summary(diffs)

```

Based on this analysis, the difference between the models is -0.013 ROC units (the RDA model is slightly higher) and the two-sided p-value for this difference is 0.000.

---------------------------------------------------

# SGI Simulated Dataset of Telecom Churn

The C50 dataset comes with pre-defined churnTrain and churnTest partitions. Note that the observations are un-balanced, with responses that churn = "yes" at 14.4% 

## Generalized Boosted Models

```{r sgi churn}
library(caret)
library(C50)
data(churn)

str(churnTrain)

table(churnTrain$churn) / nrow(churnTrain)
```

The caret package will seed to predict the first factor of the churn feature - "yes" - which happens to be coded as 1 in this dataset.

```{r sgi churn predictors}
predictors<- names(churnTrain)[names(churnTrain) != "churn"]
```

*preProcess* calculates values that can be used to apply to any data set. They include centering, scaling, spatial sign imputation, PCA or ICA, Box-Cox transformations, and others. YeoJohnson is like Box-Cox, but does not require positive values always.

The prediction function can be called on the preProcess object and a data set to observe the preProcess transformation. The sequence of preProcess transformations is handled properly by caret internally.

```{r sgi churn preprocess}
numerics<- c("account_length","total_day_calls", "total_night_calls")
procValues<- preProcess(churnTrain[,numerics],
                        method = c("center", "scale", "YeoJohnson"))
trainScaled<- predict(procValues, churnTrain[,numerics])
testScaled<- predict(procValues, churnTest[,numerics])

procValues

```

Modeling often requires specific package knowledge to build the response variable and arrive at optimal tuning. In this demonstration with the Generalized Boosted Regression Model **gbm**, the caret package greatly simplifies the setup and interface.  In the basic call to **train**, we specify the predictor set, the outcome data, and the modeling technique. Note that the **train** is able to use the original factor input for classification. Again, for binary outcomes, the function models the probability of the first factor level. A numeric object would indicate that we are doing regression. 

One problem is that **gbm** spits out a lot of information during model fitting. Let's omit theh functions logging by using the option verbose = FALSE.

Also, the default **gbm** re-sampling is bootstrap. We can better control for and compare algorithms by creating a **trainControl** object to run 5 repeats of 10-fold cross validation.

The default **gbm** performance metrics for classification are Cohen's kappa. Suppose we wanted to estimate sensitivity, specificty, and AUC of ROC. We need to tell **train** to produce class probabilities, estimate these statistics, and ran models by ROC AUC. In trainControl, indicating classProbs = TRUE and summaryFunction = twoClassSummary will ensure that the output model has these details.

By default, **train** uses a minimal search grid:  3 values for each tuning parameter. We might also want to expand the scope of possible **gbm** models to test. Let's look at tree depths from 1:7, boosting iterations from 100 to 1,000, and two different learning rates.  See ?train for the tunable parameters by model type.

```{r sgi churn gbm model 1}
ctrl<- trainControl(method = "repeatedcv",
                    repeats = 5,
                    classProbs = TRUE,
                    summaryFunction = twoClassSummary)

grid<- expand.grid(interaction.depth = seq(1,7, by = 2),
                   n.trees = seq(100, 1000, by = 50),
                   shrinkage = c(0.01, 0.1),
                   n.minobsinnode = 5)

gbmTune <- train(x=churnTrain[,predictors],
                 y= churnTrain$churn,
                 method = "gbm",
                 metric = "ROC",
                 tuneGrid = grid,
                 verbose = FALSE,
                 trControl = ctrl,
                 preProcess = )

# an alternate formula interface is train(churn~., data = churnTrain, method="gbm")

```

Let's have a look at the predictions and performance measures.

```{r sgi churn gbm model}
gbmPred<- predict(gbmTune, churnTest)

str(gbmPred)

gbmProbs<- predict(gbmTune, churnTest, type = "prob")

confusionMatrix(gbmPred, churnTest$churn)

ggplot(gbmTune) + theme(legend.position = "top")
```

The train function runs the model one last time on all training data for the winning set of hyperparameters. 

```{r churn gbm model roc}
rocCurve<- roc(response = churnTest$churn,
               predictor = gbmProbs[, "yes"],
               levels = rev(levels(churnTest$churn)))
rocCurve

plot(rocCurve,
     print.thres = c(.5,.2),
     print.thres.pch = 16,
     print.thres.cex = 1.2)
```

Next step:  model a Support Vector Machine and a Flexible discriminant model

```{r churn svm and fda models}
svmTune<-train(churn ~ ., data = churnTrain,
               method = "svmRadial",
               preProc = c("center","scale"),
               tuneLength = 10,
               trControl = ctrl,
               metric = "ROC")

fdaTune<-train(churn ~ ., data = churnTrain,
               method = "fda",
               tuneLength = 10,
               trControl = ctrl,
               metric = "ROC")
```

```{r churn svm and fda model roc}
svmPred<- predict(svmTune, churnTest)

svmProbs<- predict(svmTune, churnTest, type = "prob")

confusionMatrix(svmPred, churnTest$churn)

ggplot(svmTune) + theme(legend.position = "top")

rocCurve<- roc(response = churnTest$churn,
               predictor = svmProbs[, "yes"],
               levels = rev(levels(churnTest$churn)))
plot(rocCurve,
     print.thres = c(.5,.2),
     print.thres.pch = 16,
     print.thres.cex = 1.2)

fdaPred<- predict(fdaTune, churnTest)

fdaProbs<- predict(fdaTune, churnTest, type = "prob")

confusionMatrix(fdaPred, churnTest$churn)

ggplot(fdaTune) + theme(legend.position = "top")

rocCurve<- roc(response = churnTest$churn,
               predictor = fdaProbs[, "yes"],
               levels = rev(levels(churnTest$churn)))
plot(rocCurve,
     print.thres = c(.5,.2),
     print.thres.pch = 16,
     print.thres.cex = 1.2)

```

## GLMnet

Recall the glmnet, which is almost always the first model to try on new datasets. It fits quickly and provides linear regression components while ignoring noisy coefficients.

```{r churn glmnet}
grid<- expand.grid(alpha = 0:1,
                   lambda = 0:10 / 10)

glmnetTune <- train(churn ~ ., data = churnTrain,
  metric = "ROC",
  method = "glmnet",
  tuneGrid = grid,
  trControl = ctrl)

plot(glmnetTune)
```

## Random Forest

A random forest may be a good practice to try as second model, as it is easier to tune, but it is slower

```{r churn rf}
grid<- expand.grid(mtry = seq(25,45, by = 5),
                   splitrule = "extratrees",
                   min.node.size = c(1, 5, 20))

rfTune<- train(churn ~ ., data = churnTrain,
  metric = "ROC",
  method = "ranger",
  trControl = ctrl,
  tuneGrid = grid
)
rfTune
plot(rfTune)
```

Often when comparing models, the **resamples** function is your friend. 

```{r model comparisons}

model_list<- list(
  gbm = gbmTune,
  svm = svmTune,
  fda = fdaTune,
  glmnet = glmnetTune,
  rf = rfTune
)

resamps <- resamples(model_list)
resamps

summary(resamps)

# densityplot(resamps, metric = "ROC") 

bwplot(resamps, metric = "ROC") 

# xyplot(resamps, metric = "ROC") 

```

