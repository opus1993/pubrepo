---
author: "Jim Gruman"
date: '`r format(Sys.Date())`'
output: html_document
draft: true
linktitle: Predict Voting Behavior with Machine Learning
menu:
  example:
    parent: Machine Learning in R
    weight: 16
title: Get Out The Vote
type: docs
weight: 17
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(skimr)
library(viridis)
library(broom)
library(caret)
library(yardstick)
library(doParallel)
library(janitor)
cores<-parallel::detectCores()
doParallel::registerDoParallel(cores)
theme_set(theme_light())
```

## Get Out The Vote Case Study

Adapted from the Julia Silge [Supervised ML Course](https://supervised-ml-course.netlify.com/chapter3)

Objective: use data on attitudes and beliefs in the United States to predict the 2016 voter turnout. Apply your skills in dealing with imbalanced data and explore more resampling options.

The Data Source is the [Democracy Fund Voter Survey ](https://www.voterstudygroup.org/publication/2016-voter-survey)

![Democracy Fund](https://www.voterstudygroup.org/assets/build/img/main-logo.svg)

```{r Download, eval=FALSE, include=FALSE}

url<-"https://www.voterstudygroup.org/download?lv=ISF17V%252Fsb6OLPogCy%252Be6vm0tp%252F1AEpeRqVfSlwcUnpjQrkJDj4Dypbf%252F5s7ZpHSo6p%252Blhk7VAE7zizmvIPAMo7UyGeF6VZ6D0gEhmWKuD6GYilvVbWuUMFBORZcoSkKVyR0UymC%252BNoYPDroyK1cFyjrJJbbKZo7hJBBIsi86pvucEAmrA1PEaSdVs7c1vYJo3f2NsuF1hT50SQFvyeCpUM%252BJAdUkaIFCYWodawzFqsCdClIfjeOX2PDe8enAF1t3XOZh7apP6tLxqRg3bIneO9Dgc3DqUvxn0IuqwoDAJx8%253D"

download.file(url = url,
              destfile = "./dfvsg_2016_VOTER_Survey-csv.zip",
              mode = "wb")

unzip("dfvsg_2016_VOTER_Survey-csv.zip", files = "VOTER_Survey_December16_Release1.csv")

cols<-c(12,62:79,95:180)

voters<-read_csv("./VOTER_Survey_December16_Release1.csv") %>% clean_names()

voters<-voters[,cols]

file.remove(c("./VOTER_Survey_December16_Release1.csv","./dfvsg_2016_VOTER_Survey-csv.zip" ))

voters<-voters %>%
  filter(!is.na(turnout16_2016)) %>%
  mutate(turnout16_2016 = fct_recode(as_factor(turnout16_2016),
                                       "Did.not.vote"="2",
                                       "Voted"="1"))
voters%>%
  count(turnout16_2016)
```

Views captured in the survey included: 

1. Life in America today for people like you compared to fifty years ago is better? about the same? worse?

1. Was your vote primarily a vote in favor of your choice or was it mostly a vote against his/her opponent?

1. How important are the following issues to you?

- Crime
- Immigration
- The environment
- Gay rights

How do the reponses on the survey vary with voting behavior, at least on three survey metrics?

```{r}
voters %>%
    group_by(turnout16_2016) %>%
    summarise(`Elections don't matter` = mean(rigged_system_1_2016 <= 2, na.rm=TRUE),
              `Economy is getting better` = mean(econtrend_2016 == 1, na.rm = TRUE),
              `Crime is very important` = mean(imiss_a_2016 == 2, na.rm = TRUE))

```

title = "Overall, is the economy getting *better* or *worse*",
       subtitle = "for those that <b><span style='font-size:13pt'>Voted</span></b><br> and <b><span style='font-size:13pt'>Did not vote</span></b><br> ?"



```{r warning=FALSE}
library(ggplot2)
library(ggtext)
scales::show_col(viridis_pal()(2))

voters %>%
  ggplot(aes(econtrend_2016, ..density.., fill = turnout16_2016))+
  geom_histogram(alpha = 0.5, position = "identity", binwidth = 1)+
  labs(title = "Overall, is the economy getting *better* or *worse* for those that <b><span style='color:#440154FF'>Voted</span></b><br> and those that <b><span style='color:#FDE725FF'>Did not vote</span></b>?", 
       caption = str_c("Jim Gruman, ", Sys.Date()),
       x = element_blank()) +
  theme(legend.position = "none"  ,
        plot.title.position = "plot",
         plot.title = element_textbox_simple(
      size = 10,
      padding = margin(5.5, 5.5, 5.5, 5.5),
      margin = margin(0, 0, 5.5, 0),
      fill = "white"
    ))+     #tip from William R Chase at RStudio Conference 2020
  scale_fill_viridis(discrete = TRUE)

```

Check the data missing-ness and find features with nearly zero variance

```{r message=FALSE, warning=FALSE}
nearZeroVar(voters, names = TRUE)

Amelia::missmap(voters[,-1])
```

First fit a simple model, to take a quick look under the hood

```{r}
# Remove the case_indetifier column

# Build a simple logistic regression model
simple_glm <- glm(turnout16_2016 ~ .,
                  family = "binomial", 
                  data = voters)

# Print a summary of significant features             

tidy(simple_glm) %>%
  filter(p.value < 0.05) %>%
  arrange(desc(estimate))

```

Splitting training and testing data for modeling, at 80/20%, stratifying evenly on the response class turnout16_2016

```{r}
library(rsample)

vote_split<- rsample::initial_split(voters, 0.8, strata = "turnout16_2016")

vote_train <- training(vote_split)
vote_test <- testing(vote_split)

vote_train%>%
  count(turnout16_2016)

prop.table(table(vote_train$turnout16_2016))

prop.table(table(vote_test$turnout16_2016))

```

Impute the missing feature values within the training set with k-nearest neighbors.

```{r}
preProcValues <- preProcess(vote_train, method = c("knnImpute"))

vote_train<-predict(preProcValues, vote_train)

preProcValues <- preProcess(vote_test, method = c("knnImpute"))

vote_test<- predict(preProcValues, vote_test)

Amelia::missmap(vote_train)
```

Model with upsampling to adjust for the unbalanced class distributions

```{r}
vote_glm <- train(turnout16_2016 ~ .,
                  method = "glm",
                  family = "binomial",
                  data = vote_train,
                  metric = "Kappa",
                  trControl = trainControl(method ="none",
                                           sampling = "up",
                                           summaryFunction = twoClassSummary,
                                           classProbs = TRUE
                                           ))
vote_glm
```

A better re-sampling approach, with cross validation, for the glm logistic model

```{r}
vote_glm <- train(turnout16_2016 ~ ., 
                  method = "glm", 
                  family = "binomial",
                  data = vote_train,
                  metric = "ROC",
                  trControl = trainControl(method = "repeatedcv", 
                         summaryFunction = twoClassSummary,
                         classProbs = TRUE,
                         repeats = 2,
                         sampling = "up"))

vote_glm

```

The same re-sampling approach, with cross validation, for a random forest model

```{r}
vote_randomf <- train(turnout16_2016 ~ ., 
                  method = "rf", 
                  data = vote_train,
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 2, 
                 summaryFunction = twoClassSummary,
                 classProbs = TRUE,
                 sampling = "up"))

vote_randomf

plot(vote_randomf)
```



```{r}
vote_train %>%
    mutate(`Logistic regression` = predict(vote_glm, vote_train)) %>%
    conf_mat(truth = turnout16_2016, estimate = "Logistic regression")

vote_train %>%
    mutate(`Random forest` = predict(vote_randomf, vote_train)) %>%
    conf_mat(truth = turnout16_2016, estimate = "Random forest")

vote_test %>%
    mutate(`Logistic regression` = predict(vote_glm, vote_test)) %>%
    conf_mat(truth = turnout16_2016, estimate = "Logistic regression")

vote_test%>%
    mutate(`Random forest` = predict(vote_randomf, vote_test)) %>%
    conf_mat(truth = turnout16_2016, estimate = "Random forest")

```

Which model is best? Simplest?
Consider also calling sens() and spec()


```{r}
library(h2o)
h2o.init()

voters_hf <- as.h2o(voters)

y<- "turnout16_2016"
x <- setdiff(colnames(voters_hf),y)

voters_hf[,y] <- as.factor(voters_hf[,y])

sframe <- h2o.splitFrame(data  = voters_hf,
                         ratios = c(0.7,0.15),
                         seed = 42)

train <- sframe[[1]]
valid <- sframe[[2]]
test <- sframe[[3]]

h2o.describe(voters_hf)

summary(train$turnout16_2016, exact_quantiles = TRUE)
```


```{r}
# Train Gradient Boosted Model
gbm_model <- h2o.gbm(x = x, 
                     y = y,
                     training_frame = train, 
                     validation_frame = valid)

perf <- h2o.performance(gbm_model, test)

# Extract confusion matrix
h2o.confusionMatrix(perf)

# Extract logloss
h2o.logloss(perf)
```

```{r}
# Train random forest model
rf_model <- h2o.randomForest(x = x,
                             y = y,
                             training_frame = train,
                             validation_frame = valid)

# Calculate model performance
perf <- h2o.performance(rf_model, valid = TRUE)

perf <- h2o.performance(gbm_model, test)

h2o.confusionMatrix(perf)

# Extract logloss
h2o.logloss(perf)
```

```{r}

stopping_params <- list(strategy = "RandomDiscrete",
                        stopping_metric = "misclassification",
                        stopping_rounds = 2,
                        stopping_tolerance = 0.1,
                        seed = 42)

###
###            AutoML
###                     must have stop criteria

automl_model <- h2o.automl(x=x, y=y,
                           training_frame = train,
                           nfolds = 6,
                           # validation_frame = valid, field is ignored, automl assumes 5 nfolds cv
                           max_runtime_secs = 1200,      ### pick something much longer than 60 seconds
                           sort_metric = "logloss",
                           seed = 42)

lb<- automl_model@leaderboard

lb

model_ids <- as.data.frame(lb)$model_id

aml_leader <-automl_model@leader

summary(aml_leader)
```



```{r}

# Run automatic machine learning
automl_model <- h2o.automl(x = x, 
                           y = y,
                           training_frame = train,
                           max_runtime_secs = 1200,
                           sort_metric = "mean_per_class_error",
                           nfolds = 5,
                           validation_frame = valid,
                           seed = 42)

lb<- automl_model@leaderboard

lb

aml_leader <-automl_model@leader

h2o.performance(aml_leader, test)

summary(aml_leader)

# Get model ids for all models in the AutoML Leaderboard

model_ids <- as.data.frame(automl_model@leaderboard$model_id)[,1]

# Get the "All Models" Stacked Ensemble model

se <- h2o.getModel(grep("StackedEnsemble_AllModels", model_ids, value = TRUE)[1])

# Get the Stacked Ensemble metalearner model

metalearner <- h2o.getModel(se@model$metalearner$name)

h2o.varimp(metalearner)

h2o.varimp_plot(metalearner)

# h2o.saveModel(aml@leader, path = "./")

# h2o.download_mojo(aml@leader, path = "./")

h2o.shutdown()


```




