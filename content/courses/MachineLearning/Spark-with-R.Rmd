---
author: "Jim Gruman"
output: html_document
date: "12/31/2019"
draft: false
linktitle: Spark with R
menu:
  example:
    parent: Machine Learning in R
    weight: 17
title: Spark with R
type: docs
weight: 17
---

```{r s, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("r/render.R")
library(ggplot2)
library(corrr)
library(dbplot)
library("ggmosaic")
library("forcats")
library("FactoMineR")
```
In a world where information is growing exponentially, tools like Apache Spark provide support to solve many of the relvant problems we face today. From companies looking for ways to improve based on data-driven decisions, to research organizations solving problems in health-care, finance, education, and energy, Spark enables analyzing much more information faster and more reliably than ever before.

# Introduction

Javier Luraschi, Kevin Kuo, and Edgar Ruis have published a brand new book in 2020 on learning how to use Apache Spark with R. The book intends to take someone unfamiliar with Spark or R and help you become proficient by teaching you a set of tools, skills and practices applicable to large-scale data science.

[![Mastering Spark with R](./mastering-spark-with-r.png)](https://therinspark.com/)

[Mastering Spark with R Github](https://github.com/r-spark/the-r-in-spark/)

[Mastering Spark with R Code Examples](https://github.com/r-spark/the-r-in-spark/releases)

Buy the book here:

[Mastering Spark with R](https://www.amazon.com/gp/product/149204637X/)

Another excellent online resource for learning Apache Spark with R is

[![sparklyr: R interface for Apache Spark](img/sparklyr-v2.png)](https://spark.rstudio.com/)

Be aware that the Rstudio content is not intended to be read from start to finish and assumes that you, the reader, have some knowledge of Apache Spark, R, and cluster computing.

Richie Cotton of DataCamp also offers an overview course:

[![Introduction to Spark in R using sparklyr](./shield_image_course_3309_20180726-12-1ek0ctx.png)]
(https://www.datacamp.com/courses/introduction-to-spark-in-r-using-sparklyr)

There are many additional resources that can help you to troubleshoot particular issues while getting started and, in general, introduce you to the broader Spark and R communities to help you get specific answers, discuss topics, and connect with many users actively using Spark with R. For general sparklyr questions, you can post in the [RStudio Community](http://bit.ly/2PfNqzN) tagged as sparklyr. For general Spark questions, [Stack Overflow](http://bit.ly/2TEfU4L) is a great resource; there are also many topics specifically about sparklyr. If you believe something needs to be fixed, open a [GitHub](http://bit.ly/30b5NGT) issue or send a pull request. 

---------------------

To develop a definition of Apache Spark, we break it down as follows:

**Unified**

>Spark supports many libraries, cluster technologies, and storage systems. 

**Analytics**

>Analytics is the discovery and interpretation of data to produce and communicate information. 

**Engine**

>Spark is expected to be efficient and generic. 

**Large-Scale**

>You can interpret large-scale as cluster-scale, a set of connected computers working together. 

-------------------

## Audience

When thinking of who should use sparklyr, the following roles come to mind:

**New Users**

For new users, it is our belief that sparklyr provides the easiest way to get started with Spark. Our hope is that the early chapters of this book will get you up and running with ease and set you up for long-term success. 

**Data Scientists**

For data scientists who already use and love R, sparklyr integrates with many other R practices and packages like dplyr, magrittr, broom, DBI, tibble, rlang, and many others, which will make you feel at home while working with Spark. For those new to R and Spark, the combination of high-level workflows available in sparklyr and low-level extensibility mechanisms make it a productive environment to match the needs and skills of every data scientist. 

**Expert Users**

For those users who are already immersed in Spark and can write code natively in Scala, consider making your Spark libraries available as an R package to the R community, a diverse and skilled community that can put your contributions to good use while moving open science forward. 

sparklyr is the R package that brings together these communities, expectations, future directions, packages, and package extensions.

----------------

Tip:  When using Windows, avoid directories with spaces in their path. If running getwd() from R returns a path with spaces, consider switching to a path with no spaces using setwd("path") or by creating an RStudio project in a path with no spaces.

Additionally, because Spark is built in the Scala programming language, which is run by the Java Virtual Machine (JVM), you also need to install Java 8 on your system. It is likely that your system already has Java installed, but you should still check the version and update or downgrade as described in Installing Java. You can use the following R command to check which version is installed on your system:

```{r eval=FALSE}
getwd()

system("java -version")
```

[1] "C:/Users/jimgr/Documents/R/pubrepo/content/courses/MachineLearning"
java version "1.8.0_231"
Java(TM) SE Runtime Environment (build 1.8.0_231-b11)
Java HotSpot(TM) Client VM (build 25.231-b11, mixed mode, sharing)
[1] 0

## Installation

As with many other R packages, you can install sparklyr from CRAN as follows:

```{r install sparklyr, eval=FALSE}
install.packages("sparklyr")

packageVersion("sparklyr")
```

Start by loading sparklyr:

```{r load sparklyr}
library(sparklyr)
```

You can easily install Spark by running spark_install(). This downloads, installs, and configures the latest version of Spark locally on your computer.

```{r spark install, eval=FALSE}
spark_install()

spark_version()

spark_available_versions()
```

Note: The default installation paths are ~/spark for macOS and Linux, and %LOCALAPPDATA%/spark for Windows. To customize the installation path, you can run options(spark.install.dir = "installation-path") before spark_install() and spark_connect()

## Connecting

```{r connecting}
sc <- spark_connect(master = "local", version = "2.4")
```

## Using Spark

Now that you are connected, we can run a few simple commands. For instance, let’s start by copying the mtcars dataset into Apache Spark by using copy_to():

```{r mtcars}
cars <- copy_to(sc, mtcars)

```

The data was copied into Spark, but we can access it from R using the cars reference. To print its contents, we can simply type *cars*:

```{r mtcars 1}
cars
```

## Web Interface

Most of the Spark commands are executed from the R console; however, monitoring and analyzing execution is done through Spark’s web interface. This interface is a web application provided by Spark that you can access by running:

```{r web interface, eval=FALSE}
spark_web(sc)
```

## Analysis

When using Spark from R to analyze data, you can use SQL (Structured Query Language) or dplyr (a grammar of data manipulation). You can use SQL through the DBI package; for instance, to count how many records are available in our cars dataset, we can run the following:

```{r analysis}
library(DBI)
dbGetQuery(sc, "SELECT count(*) FROM mtcars")
```

When using dplyr, you write less code, and it’s often much easier to write than SQL. This is precisely why we won’t make use of SQL in this book; however, if you are proficient in SQL, this is a viable option for you. For instance, counting records in dplyr is more compact and easier to understand:

```{r analysis with dplyr}
library(dplyr)
count(cars)
```

In general, we usually start by analyzing data in Spark with dplyr, followed by sampling rows and selecting a subset of the available columns. The last step is to collect data from Spark to perform further data processing in R, like data visualization. Let’s perform a very simple data analysis example by selecting, sampling, and plotting the cars dataset in Spark:

```{r analysis with dplyr2}
select(cars, hp, mpg) %>%
  sample_n(100) %>%
  collect() %>%
  plot()
```

## Modeling

```{r modeling1}
model <- ml_linear_regression(cars, mpg ~ hp)
model
```

Now we can use this model to predict values that are not in the original dataset. For instance, we can add entries for cars with horsepower beyond 250 and also visualize the predicted values

```{r modeling2}
model %>%
  ml_predict(copy_to(sc, data.frame(hp = 250 + 10 * 1:10))) %>%
  transmute(hp = hp, mpg = prediction) %>%
  full_join(select(cars, hp, mpg)) %>%
  collect() %>%
  plot()
```

## Extensions

In the same way that R is known for its vibrant community of package authors, at a smaller scale, many extensions for Spark and R have been written and are available. For instance, the sparkly.nested extension is an R package that extends sparklyr to help you manage values that contain nested information. A common use case involves JSON files that contain nested lists that require preprocessing before you can do meaningful data analysis. To use this extension, we first need to install it as follows:

```{r extensions, eval=FALSE}
install.packages("sparklyr.nested")
```

Then, we can use the sparklyr.nested extension to group all of the horsepower data points over the number of cylinders:

```{r extensions 2}
library(sparklyr.nested)
sparklyr.nested::sdf_nest(cars, hp) %>%
  group_by(cyl) %>%
  summarise(data = collect_list(data))
```

## Streaming

While processing large static datasets is the most typical use case for Spark, processing dynamic datasets in real time is also possible and, for some applications, a requirement. You can think of a streaming dataset as a static data source with new data arriving continuously, like stock market quotes. Streaming data is usually read from Kafka (an open source stream-processing software platform) or from distributed storage that receives new data continuously.

To try out streaming, let’s first create an input/ folder with some data that we will use as the input for this stream:

```{r streaming 1}
dir.create("input")
write.csv(mtcars, "input/cars_1.csv", row.names = F)
```

Then, we define a stream that processes incoming data from the input/ folder, performs a custom transformation in R, and pushes the output into an output/ folder:

```{r streaming 2}
stream <- stream_read_csv(sc, "input/") %>%
    select(mpg, cyl, disp) %>%
    stream_write_csv("output/")
```

As soon as the stream of real-time data starts, the input/ folder is processed and turned into a set of new files under the output/ folder containing the new transformed files. Since the input contained only one file, the output folder will also contain a single file resulting from applying the custom spark_apply() transformation.

```{r streaming 3}
dir("output", pattern = ".csv")
```

Up to this point, this resembles static data processing; however, we can keep adding files to the input/ location, and Spark will parallelize and process data automatically. Let’s add one more file and validate that it’s automatically processed:

```{r streaming 4}
# Write more data into the stream source
write.csv(mtcars, "input/cars_2.csv", row.names = F)
```

Wait a few seconds and validate that the data is processed by the Spark stream:

```{r streaming 5}
# Check the contents of the stream destination
dir("output", pattern = ".csv")
```
You should then stop the stream:

```{r streaming 6}
stream_stop(stream)
```

You can use dplyr, SQL, Spark models, or distributed R to analyze streams in real time. In Chapter 12 we properly introduce you to all the interesting transformations you can perform to analyze real-time data.

## Logs

Logging is definitely less interesting than real-time data processing; however, it’s a tool you should be or become familiar with. A log is just a text file to which Spark appends information relevant to the execution of tasks in the cluster. For local clusters, we can retrieve all the recent logs by running the following:

```{r logging}
spark_log(sc, filter = "sparklyr")
```

Most of the time, you won’t need to worry about Spark logs, except in cases for which you need to troubleshoot a failed computation; in those cases, logs are an invaluable resource to be aware of. Now you know.

## Disconnecting

For local clusters (really, any cluster), after you are done processing data, you should disconnect by running the following:

```{r disconnecting}
spark_disconnect_all()

```

-------

# Analysis

This chapter introduces concepts to perform data analysis in Spark from R. Spoiler alert: these are the same tools used with plain R! This is not a mere coincidence; rather, we want data scientists to live in a world where technology is hidden from them, where you can use the R packages you know and love, and they “just work” in Spark! 

In a data analysis project, the main goal is to understand what the data is trying to “tell us”, hoping that it provides an answer to a specific question. Most data analysis projects follow the same set of steps, from importing data, to wrangling>visualizing>modeling, to communicating results.

When working with not-large-scale datasets—as in datasets that fit in memory—we can perform all those steps from R, without using Spark. However, when data does not fit in memory or computation is simply too slow, we can slightly modify this approach by incorporating Spark. But how?

For data analysis, the ideal approach is to let Spark do what it’s good at. Spark is a(((“parallel execution”))) parallel computation engine that works at a large scale and provides a SQL engine and modeling libraries. You can use these to perform most of the same operations R performs. Such operations include data selection, transformation, and modeling. 

Data *import*, *wrangling*, and *modeling* can be performed inside Spark. The idea is to use R to tell Spark what data operations to run, and then only bring the results into R. As illustrated here, the ideal method *pushes compute* to the Spark cluster, and then *collects results* into R.

The **sparklyr** package aids in using the “push compute, collect results” principle. Most of its functions are wrappers on top of Spark API calls. This allows us to take advantage of Spark’s analysis components, instead of R’s. For example, when you need to fit a linear regression model, instead of using R’s familiar `lm()` function, you would use Spark’s `ml_linear_regression()` function. This R function then calls Spark to create this model.

For more common data manipulation tasks, sparklyr provides a backend for dplyr. This means you can use dplyr verbs with which you’re already familiar in R, and then sparklyr and dplyr will translate those actions into Spark SQL statements, which are generally more compact and easier to read than SQL statements. So, if you are already familiar with R and dplyr, there is nothing new to learn. This might feel a bit anticlimactic—indeed, it is—but it’s also great since you can focus that energy on learning other skills required to do large-scale computing.

To start, load the sparklyr and dplyr packages and then open a new local connection.

```{r message=FALSE, warning=FALSE}
library(sparklyr)
library(dplyr)

sc<- spark_connect(master = "local", version = "2.4")
```

When using Spark with R, you need to approach importing data differently. Usually, importing means that R will read files and load them into memory; when you are using Spark, the data is imported into Spark, not R. In Figure 3.5, notice how the data source is connected to Spark instead of being connected to R.

```{r analysis-access, echo=FALSE, out.width='100%', out.height='220pt', fig.cap='Import Data to Spark not R', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#padding: 14
#arrowSize: 0.4
[R]->[<label>Push Compute]
[Push Compute]->[Spark]
[R]<-[<label>Collect Results]
[Collect Results]<-[Spark]
[Spark]<-[<label> Import]
[Import]<-[Data Source]
", "images/analysis-import-data-to-spark.png")
```

**Note:** When you doing analysis over large-scale datasets, the vast majority of the necessary data will be already available in your Spark cluster (which is usually made available to users via Hive tables, or by accessing the file system directly), the Data chapter will cover this extensively.

Rather than importing all data into Spark, you can also request Spark to access the data source without importing it -- this is a decision you should make based on speed and performance. Importing all of the data into the Spark session will incur a up-front cost, once; since Spark needs to wait for the data to be loaded before analyzing it. If the data is not imported, you usually incur a cost with every Spark operation since Spark needs to retrieve a subset from the cluster's storage, which is usually disk drives that happen to be much slower than reading from Spark's memory. More will be covered in the Tuning chapter.

Let's prime the session with some data by importing `mtcars` into Spark using `copy_to()`; you can also import data from distributed files in many different file formats, which you'll learn in the Data chapter.

```{r analysis-copy-to}
cars <- copy_to(sc, mtcars)
```

**Note:** In an enterprise setting, `copy_to()` should only be used to transfer small tables from R, large data transfers should be performed with specialized data transfer tools. 

The data is now accessible to Spark and transformations can now be applied with ease; the next section will cover how to wrangle data by running transformations inside Spark, using `dplyr`.

Data wrangling uses transformations to understand the data, it is often referred to as the process of transforming data from one "raw" data form into another format with the intent of making it more appropriate for data analysis.

In the R environment, *cars* can be treated as if it is a local data frame, so `dplyr` verbs can be used. For instance, we can find out the mean of all columns as with `summarise_all()`:

```{r analysis-summarize-all, eval = FALSE}
summarize_all(cars, mean)
```

While this code is exactly the same as the code you would run when using `dplyr` without Spark, a lot is happening under the hood! The data is NOT being imported into R; instead,`dplyr` converts this task into SQL statements that are then sent to Spark. The `show_query()` command makes it possible to peer into the SQL statement that `sparklyr` and `dplyr` created and sent to Spark. We can also use this time to introduce the pipe (`%>%`) operator, a custom operator from the `magrittr` package that takes pipes a computation into the first argument of the next function, making your data analysis much easier to read.

```{r analysis-summarize-show-query, eval = FALSE}
summarize_all(cars, mean) %>%
  show_query()
```

As it is evident, `dplyr` is much more concise than SQL; but rest assured, you will not have to see nor understand SQL when using `dplyr`. Your focus can remain on obtaining insights from the data, as opposed to figuring out how to express a given set of transformation in SQL. Here is another example that groups the cars dataset by "transmission" type.

```{r analysis-group-summarize}
cars %>%
  mutate(transmission = ifelse(am == 0, "automatic", "manual")) %>%
  group_by(transmission) %>%
  summarise_all(mean)
```

### Built-in Functions

Spark SQL is based on Hive's SQL conventions and functions and it is possible to call all these functions using `dplyr` as well. This means that we can use any Spark SQL functions to accomplish operations that may not be available via `dplyr`. The functions can be accessed by calling them as if they were R functions. Instead of failing, `dplyr` passes functions it does not recognize "as-is" to the query engine. This gives us a lot of flexibility on the function we can use! 

For instance, the *percentile* function returns the exact percentile of a column in a group. The function expects a column name, and either a single percentile value, or an array of multiple percentile values. We can use this Spark SQL function from `dplyr` as follows:

```{r analysis-percentile}
summarise(cars, mpg_percentile = percentile(mpg, 0.25))
```

There is no `percentile()` function in R, so `dplyr` passes the that portion of the code, "as-is", to the resulting SQL query.

```{r analysis-percentile-query}
summarise(cars, mpg_percentile = percentile(mpg, 0.25)) %>%
  show_query()
```

### Correlations

A very common exploration technique is to calculate and visualize correlations, which we often calculate to find out what kind of statistical relationship exists between paired sets of variables. Spark provides functions to calculate correlations across the entire dataset and returns the results to R as a DataFrame object:

```{r}
ml_corr(cars)
```

The corrr R package specializes in correlations. It contains friendly functions to prepare and visualize the results. Included inside the package is a backend for Spark, so when a Spark object is used in corrr, the actual computation also happens in Spark. In the background, the correlate() function runs sparklyr::ml_corr(), so there is no need to collect any data into R prior to running the command:

```{r}
corrr::correlate(cars, use = "pairwise.complete.obs", method = "pearson")
```

We can pipe the results to other corrr functions. For example, the shave() function turns all of the duplicated results into NAs. Again, while this feels like standard R code using existing R packages, Spark is being used under the hood to perform the correlation.

Additionally, as shown in Figure 3.6, the results can be easily visualized using the rplot() function, as shown here:

```{r analysis-corrr-show}
corrr::correlate(cars, use = "pairwise.complete.obs", method = "pearson") %>%
  corrr::shave() %>%
  corrr::rplot()
```

### Visualize

R is great at data visualizations. Its capabilities for creating plots is extended by the many R packages that focus on this analysis step.   Unfortunately, the vast majority of R functions that create plots depend on the data already being in local memory within R, so they fail when using a remote table inside Spark. 

It is possible to create visualizations in R from data source from Spark. To understand how to do this, let's first break down how computer programs build plots:  It takes the raw data and performs some sort of transformation.  The transformed data is then mapped to a set of coordinates. Finally, the mapped values are drawn in a plot. Figure \@ref(fig:analysis-plot) summarizes each of the steps.

```{r analysis-plot, echo=FALSE, out.width='100%', out.height='100pt', fig.cap='Stages of an R plot', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#edgeMargin: 4
#padding: 25
#fontSize: 18

[Raw data] -> [Transformed Data]
[Transformed Data] -> [Map data to coordinates]
[Map data to coordinates] -> [Draw plot]
", "images/analysis-stages-of-a-plot.png")
```

In essence, the approach for visualizing is the same as in wrangling, push the computation to Spark, and then collect the results in R for plotting.  The heavy lifting of preparing the data, such as in aggregating the data by groups or bins, can be done inside Spark, and then collect the much smaller data set into R.  Inside R, the plot becomes a more basic operation.  For example, to plot a histogram, the bins are calculated in Spark, and then in R, use a simple column plot, as opposed to a histogram plot, because there is no need for R to re-calculate the bins. 

```{r analysis-spark-plot, echo=FALSE, out.width='100%', out.height='200pt', fig.cap='Plotting with Spark and R', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#edgeMargin: 4
#padding: 25
#fontSize: 18
[Spark |
  [Raw data] -> [Transformed Data]
]
[R |
  [Map to coordinates] -> [Draw plot]
]
[Spark] -> [R]
", "images/analysis-plotting-with-spark-and-r.png")
```

Using this conceptual model, let's apply this when using `ggplot2`.

### Using ggplot2

To create a bar plot using `ggplot2`, we simply call a function:

```{r analysis-ggplot2-simple, eval = TRUE, out.width='500pt', out.height='400pt', fig.cap='Plotting inside R', fig.align = 'center'}
ggplot(aes(as.factor(cyl), mpg), data = mtcars) + geom_col()
```

In this case, the `mtcars` raw data was *automatically* transformed into three discrete aggregated numbers, then each result was mapped into an `x` and `y` plane, and then the plot was drawn. As R users, all of the stages of building the plot are conveniently abstracted for us.

In Spark, there are a couple of key steps when codifying the "push compute, collect results" approach. First, ensure that the transformation operations happen inside Spark. In the example below, `group_by()` and `summarise()` will run as inside Spark. The second is to bring the results back into R after the data has been transformed.  Make sure to transform and then collect, in that order; if `collect()` is run first, then R will try to ingest the entire data set from Spark. Depending on the size of the data, collecting all of the data will slow down or may even bring down your system.

```{r analysis-ggplot2-simmarise}
car_group <- cars %>%
  group_by(cyl) %>%
  summarise(mpg = sum(mpg, na.rm = TRUE)) %>%
  collect() %>%
  print()
```

In this example, now that the data has been pre-aggregated and collected into R, only three records are passed to the plotting function.

```{r analysis-ggplot2-groups}
ggplot(aes(as.factor(cyl), mpg), data = car_group) + 
  geom_col(fill = "#999999") + coord_flip()
```

### Using dbplot

The `dbplot` package provides helper functions for plotting with remote data.  The R code `dbplot` uses to transform the data is written so that it can be translated into Spark.  It then uses those results to create a graph using the `ggplot2` package where data transformation and plotting are both triggered by a single function.  

The `dbplot_histogram()` function makes Spark calculate the bins and the count per bin and outputs a `ggplot` object which can be further refined by adding more steps to the plot object. `dbplot_histogram()` also accepts a `binwidth` argument to control the range used to compute the bins

```{r  analysis-dbplot-simple}
library(dbplot)

cars %>%
dbplot_histogram(mpg, binwidth = 3) +
  coord_flip()+
labs(title = "MPG Distribution",
     subtitle = "Histogram over miles per gallon")
```

Histograms provide a great way to analyze a single variable. To analyze two variables, a scatter or raster plot is commonly used. 

Scatter plots are used to compare the relationship between two continuous variables. For example, a scatter plot will display the relationship between the weight of a car and its gas consumption.  The plot will show that the higher the weight, the higher the gas consumption because the dots clump together into almost a line that goes from the top left towards the bottom right. See Figure \@ref(fig:analysis-point) for an example of the plot. 

```{r analysis-scatter-rendeer, echo = FALSE}
ggplot(aes(mpg, wt), data = mtcars) + 
  geom_point() +
  labs(title = "Weigth over MPG", subtitle = "A scatter plot visualizing car weigth over miles per gallon") +
  geom_hline(yintercept = 0, size = 1, colour = "#333333") +
  scale_y_continuous(breaks = 0:5) 
```

However, for scatter plots, no amount of "pushing the computation" to Spark will help with this problem because the data has to be plotted in individual dots.  

The best alternative is to find a plot type that represents the x/y relationship and concentration in a way that it is easy to perceive and to "physically" plot.  The *raster* plot may be the best answer.  It returns a grid of x/y positions and the results of a given aggregation usually represented by the color of the square.

You can use `dbplot_raster()` to create a scatter-like plot in Spark, while only retrieving a small subset of the remote dataset:

```{r  analysis-rasteer-render, eval = FALSE, echo = FALSE}
db_compute_raster2(cars, mpg, wt, resolution = 16) %>%
  ggplot() +
  scale_fill_gradientn(colours = grey.colors(10, start = 0.6, end = 0.2)) +
  geom_raster(aes(mpg, wt, fill = `n()`)) +
  scale_y_continuous(breaks = 0:5) +
  geom_hline(yintercept = 0, size = 1, colour = "#333333") +
  labs(title = "Weigth over MPG", subtitle = "A raster plot visualizing car weigth over miles per gallon using Spark") 
```

**Tip:** You can also use `dbplot` to retrieve the raw data and visualize by other means; to retrieve the aggregates but not the plots use: `db_compute_bins()`, `db_compute_count()`, `db_compute_raster()` and `db_compute_boxplot()`.

## Model

First, an analysis project goes through as many transformations and models to find the answer. The ideal data analysis language enables you to quickly adjust over each wrangle-visualize-model iteration. Fortunately, this is the case when using Spark and R.

To illustrate how easy it is to iterate over wrangling and modeling in Spark, consider the following example. We will start by performing a linear regression against all features and predict MPG:

```{r analysis-model-linear, eval = FALSE}
cars %>% 
  ml_linear_regression(mpg ~ .) %>%
  summary()
```

It is also very easy to iterate with other kinds of models. The following one replaces the linear model with a generalized linear model:

```{r analysis-model-glm, eval = FALSE}
cars %>% 
  ml_generalized_linear_regression(mpg ~ hp + cyl) %>%
  summary()
```

### Caching

The examples in this chapter are built using a very small data set. In real-life scenarios, large amounts of data are used for models. If the data needs to be transformed first, the volume of the data could exact a heavy toll on the Spark session. Before fitting the models, it is a good idea to save the results of all the transformations in a new table inside Spark memory.

The `compute()` command can take the end of a `dplyr` piped command set and save the results to Spark memory.

```{r analysis-caching-compute}
cached_cars <- cars %>% 
  mutate(cyl = paste0("cyl_", cyl)) %>%
  compute("cached_cars")
```

```{r analysis-caching-linear}
cached_cars %>%
  ml_linear_regression(mpg ~ .) %>%
  summary()
```

As more insights are gained from the data, more questions may be raised.  That is why we expect to iterate through data *wrangle*, *visualize*, and *model* multiple times.  Each iteration should provide incremental insights of what the data is "telling us". There will be a point where we reach a satisfactory level of understanding.  It is at this point that we will be ready to share the results of the analysis, this is the topic of the next section.

## Communicate

It is important to clearly communicate the analysis results -- as important as the analysis work itself! The public, colleagues or stakeholders need to understand what you found out and how. 

To communicate effectively we need to use artifacts, such as reports and presentations; these are common output formats that we can create in R, using R Markdown.

R Markdown documents allow weave narrative text and code together.  The amount of output formats provides a very compelling reason to learn and use R Markdown.  There are many available output formats like HTML, PDF, PowerPoint, Word, web slides, Websites, books and so on.

Since an R Markdown document is self-contained and meant to be reproducible, before rendering documents we should first disconnect from Spark to free resources:

```{r analysis-disconnect}
spark_disconnect(sc)
```
-------------------

## Modeling

The examples here will utilize the OkCupid dataset [@kim2015okcupid], available at [github.com/r-spark/okcupid](https://github.com/r-spark/okcupid). The dataset consists of user profile data from an online dating site, and contains a diverse set of features, including biographical characteristics such as gender and profession, and free text fields related to personal interests. There are about 60,000 profiles in the dataset, which fits comfortably into memory on a modern laptop and wouldn't be considered "big data", so you can easily follow along running Spark local mode.

Download this dataset as follows:

```{r modeling-download-data, eval=FALSE}
download.file(
  "https://github.com/r-spark/okcupid/raw/master/profiles.csv.zip",
  "okcupid.zip")

unzip("okcupid.zip", exdir = "data")
unlink("okcupid.zip")
```

Consider the following problem:

> Predict whether someone is actively working, i.e. not retired, a student, or unemployed.

Next up, we will explore this dataset.

## Exploratory Data Analysis

Exploratory data analysis (EDA), in the context of predictive modeling, is the exercise of looking at excerpts and summaries of the data. The specific goals of the EDA stage is informed by the business problem, but here are some common objectives:

- Check for data quality --- confirm meaning and prevalence of missing values and reconcile statistics against existing controls,
- Understand univariate relationships between variables, and
- Perform an initial assessment on what variables to include and what transformations need to be done on them.

We'll first connect to Spark, load libraries and read in the data.

```{r modeling-connect, include=TRUE, warning=FALSE, message=FALSE}
library(sparklyr)
library(ggplot2)
library(dbplot)
library(dplyr)

sc <- spark_connect(master = "local", version = "2.4")

okc <- spark_read_csv(
  sc, 
  "d:/the-r-in-spark-1.0.0/profiles.csv",
#  "data/profiles.csv", 
  escape = "\"", 
  memory = FALSE,
  options = list(multiline = TRUE)
) %>%
  mutate(
    height = as.numeric(height),
    income = ifelse(income == "-1", NA, as.numeric(income))
  ) %>%
  mutate(sex = ifelse(is.na(sex), "missing", sex)) %>%
  mutate(drinks = ifelse(is.na(drinks), "missing", drinks)) %>%
  mutate(drugs = ifelse(is.na(drugs), "missing", drugs)) %>%
  mutate(job = ifelse(is.na(job), "missing", job))
```

We specify `escape = "\""` and `options = list(multiline = TRUE)` here to accommodate for embedded quote characters and newlines in the essay fields. We also convert the `height` and `income` columns to numeric types, and recode missing values in the string columns. Note that it may very well take a few tries of specifying different parameters to get the initial data ingest correct, and sometimes you may have to revisit this step after you learn more about the data during modeling.

We can now take a quick look at our data with `glimpse()`:

```{r modeling-eda-glimpse}
glimpse(okc)
```

Now we will add our response variable as a column in the dataset and look at its distribution

```{r modeling-eda-tally}
okc <- okc %>%
  mutate(
    not_working = ifelse(job %in% c("student", "unemployed", "retired"), 1 , 0)
  )

okc %>% 
  group_by(not_working) %>% 
  tally()
```

Before we proceed further, let us perform an initial split of our data into a training set and a testing set and put away the latter. In practice, this is a crucial step because we would like to have a holdout set that we set aside at the end of the modeling process to evaluate model performance. If we were to include the entire dataset during EDA, information from the testing set could "leak" into the visualizations and summary statistics, and bias our model building process even though the data is not used directly in a learning algorithm. This would undermine the credibility of our performance metrics. Splitting the data can be done easily by using the `sdf_partition()` function:

```{r modeling-eda-splits}
data_splits <- sdf_random_split(okc, training = 0.8, testing = 0.2, seed = 42)
okc_train <- data_splits$training
okc_test <- data_splits$testing
```

We can quickly look at the distribution of our response variable:

```{r modeling-eda-dist}
okc_train %>%
  group_by(not_working) %>%
  tally() %>%
  mutate(frac = n / sum(n))
```

Using the `sdf_describe()` function, we can obtain numerical summaries of specific columns:

```{r modeling-describe}
sdf_describe(okc_train, cols = c("age", "income"))

```

Like we saw in the Analysis chapter, we can also utilize the dbplot package to plot distributions of these variables. For example, a histogram of the distribution of the `age` variable:

```{r modeling-eda-bins, echo=FALSE, eval=FALSE}
db_compute_bins(okc_train, age) %>%
  ggplot() +
  geom_col(aes(age, count), fill = "#878786") +
  geom_hline(yintercept = 0, size = 1, colour = "#333333") +
  labs(title = "Distribution of Age", subtitle = "Age histogram in OKCupid dataset") 
```
```{r modeling-eda-hist}
dbplot_histogram(okc_train, age) +
  geom_hline(yintercept = 0, size = 1, colour = "#333333") +
  labs(title = "Distribution of Age", subtitle = "Age histogram in OKCupid dataset")
```

A common EDA exercise is to look at the relationships between the response and the individual predictors. Often, you may have prior business knowledge on what these relationships should be, so this can serve as a data quality check. Also, unexpected trends can inform variable interactions you might want to include in the model. As an example, we can explore the `religion` variable:

```{r modeling-eda-prop}
prop_data <- okc_train %>%
  mutate(religion = regexp_extract(religion, "^\\\\w+", 0)) %>% 
  group_by(religion, not_working) %>%
  tally() %>%
  group_by(religion) %>%
  summarize(
    count = sum(n),
    prop = sum(not_working * n) / sum(n)
  ) %>%
  mutate(se = sqrt(prop * (1 - prop) / count)) %>%
  collect()

prop_data
```

Note that `prop_data` is a small data frame that has been collected into memory in our R session, we can take advantage of ggplot2 to create an informative visualization.

```{r modeling-eda-prop-plot, echo=FALSE, eval=FALSE}
prop_data %>%
  ggplot(aes(x = religion, y = prop)) + 
  theme(axis.text.x = element_text(size = 12)) +
  geom_errorbar(aes(ymin = prop - 1.96 * se, ymax = prop + 1.96 * se), width = .1) +
  geom_point(size = 2) +
  geom_hline(yintercept = sum(prop_data$prop * prop_data$count) / sum(prop_data$count), linetype = "dashed", color = "#888888") 
```
```{r modeling-eda-prop-code}
prop_data %>%
  ggplot(aes(x = religion, y = prop)) + geom_point(size = 2) +
  geom_errorbar(aes(ymin = prop - 1.96 * se, ymax = prop + 1.96 * se),
                width = .1) +
  geom_hline(yintercept = sum(prop_data$prop * prop_data$count) /
                              sum(prop_data$count))
```

Next, we take a look at the relationship between a couple of predictors: alcohol use and drug use. We would expect there to be some correlation between them. You can compute a contingency table via `sdf_crosstab()`:

```{r modeling-eda-contingency}
contingency_tbl <- okc_train %>% 
  sdf_crosstab("drinks", "drugs") %>%
  collect()

contingency_tbl
```


We can visualize this contingency table using a mosaic plot:

```{r modeling-eda-contingency-code}
library(ggmosaic)
library(forcats)
library(tidyr)

contingency_tbl %>%
  rename(drinks = drinks_drugs) %>%
  gather("drugs", "count", missing:sometimes) %>%
  mutate(
    drinks = as_factor(drinks) %>% 
      fct_relevel("missing", "not at all", "rarely", "socially", 
                  "very often", "desperately"),
    drugs = as_factor(drugs) %>%
      fct_relevel("missing", "never", "sometimes", "often")
  ) %>%
  ggplot() +
  geom_mosaic(aes(x = product(drinks, drugs), fill = drinks, 
                  weight = count))
```

```{r modeling-eda-contingency-plot, echo=FALSE, eval=FALSE}
library(ggmosaic)
library(forcats)
library(tidyr)

contingency_tbl %>%
  rename(drinks = drinks_drugs) %>%
  gather("drugs", "count", missing:sometimes) %>%
  mutate(
    drinks = as_factor(drinks) %>% 
      forcats::fct_relevel("missing", "not at all", "rarely", "socially", "very often", "desperately"),
    drugs = as_factor(drugs) %>%
      forcats::fct_relevel("missing", "never", "sometimes", "often")
  ) %>%
  ggplot() + guides(fill = FALSE) +
  geom_mosaic(aes(x = product(drinks, drugs), fill = drinks, weight = count)) +
  xlab("Drugs") + ylab("Drinks") +
  scale_fill_grey(start = 0.6, end = 0.2) +
  labs(title = "Contingency Table", subtitle = "Mosaic plot of drug and alcohol use") +
  theme(
    axis.text.x = element_text(vjust = 1, hjust = 1)
  ) 
```

To further explore the relationship between these two variables, we can perform correspondence analysis using the FactoMineR package. This technique enables us to summarize the relationship between the high dimensional factor levels by mapping each level to a point on the plane. We first obtain the mapping using `FactoMineR::CA()` as follows:

```{r modeling-eda-factominer-map}
dd_obj <- contingency_tbl %>% 
  tibble::column_to_rownames(var = "drinks_drugs") %>%
  FactoMineR::CA(graph = FALSE)
```

We can then plot the results using ggplot:

```{r modeling-eda-factominer-code}
dd_drugs <-
  dd_obj$row$coord %>%
  as.data.frame() %>%
  mutate(
    label = gsub("_", " ", rownames(dd_obj$row$coord)),
    Variable = "Drugs"
  )

dd_drinks <-
  dd_obj$col$coord %>%
  as.data.frame() %>%
  mutate(
    label = gsub("_", " ", rownames(dd_obj$col$coord)),
    Variable = "Alcohol"
  )
  
ca_coord <- rbind(dd_drugs, dd_drinks)
  
ggplot(ca_coord, aes(x = `Dim 1`, y = `Dim 2`, 
                     col = Variable)) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  geom_text(aes(label = label)) +
  coord_equal()
```

## Feature Engineering

The feature engineering exercise comprises transforming the data to increase the performance of the model. This can include things like centering and scaling numerical values and performing string manipulation to extract meaningful variables. It also often includes variable selection --- the process of selecting which predictors are used in the model.

In the histogram, we see that the `age` variable has a range from 18 to over 60. Some algorithms, especially neural networks, train faster if we normalize our inputs so that they are of the same magnitude. Let's now normalize the `age` variable by removing the mean and scaling to unit variance, beginning by calculating its mean and standard deviation:

```{r modeling-eda-scale}
scale_values <- okc_train %>%
  summarize(
    mean_age = mean(age),
    sd_age = sd(age)
  ) %>%
  collect()

scale_values
```

We can then use these to transform the dataset:

```{r modeling-eda-scale-age}
okc_train <- okc_train %>%
  mutate(scaled_age = (age - !!scale_values$mean_age) /
           !!scale_values$sd_age)
```
```{r modeling-eda-scale-code}
dbplot_histogram(okc_train, scaled_age)+
  geom_hline(yintercept = 0, size = 1, colour = "#333333") +
  labs(title = "Distribution of Scaled Age", subtitle = "Scaled age histogram in OKCupid dataset") 
```
```{r modeling-eda-scale-plot, echo=FALSE, eval=FALSE}
db_compute_bins(okc_train, scaled_age) %>%
  ggplot() +
  geom_col(aes(scaled_age, count), fill = "#878787") +
  geom_hline(yintercept = 0, size = 1, colour = "#333333") +
  labs(title = "Distribution of Scaled Age", subtitle = "Scaled age histogram in OKCupid dataset") 
```

Since some of the profile features are multiple-select, in other words, a person can choose to associate with multiple options for a variable, we need to process them before we can build meaningful models. If we take a look at the ethnicity column, for example, we see that there are many different combinations:

```{r modeling-eda-ethnicity}
okc_train %>%
  group_by(ethnicity) %>%
  tally()
```

One way to proceed would be to treat each combination of races as a separate level, but that would lead to a very large number of levels which becomes problematic in many algorithms. To better encode this information, we can create dummy variables for each race, as follows:

```{r modeling-eda-ethnicity-glimpse}
ethnicities <- c("asian", "middle eastern", "black", "native american", "indian", 
                 "pacific islander", "hispanic / latin", "white", "other")
ethnicity_vars <- ethnicities %>% 
  purrr::map(~ expr(ifelse(like(ethnicity, !!.x), 1, 0))) %>%
  purrr::set_names(paste0("ethnicity_", gsub("\\s|/", "", ethnicities)))
okc_train <- mutate(okc_train, !!!ethnicity_vars)
okc_train %>% 
  select(starts_with("ethnicity_")) %>%
  glimpse()
```

For the free text fields, a straightforward way to extract features is counting the total number of characters. We will store the train dataset in Spark's memory with `compute()` to speed up computation.

```{r modeling-eda-essay-compute}
okc_train <- okc_train %>%
  mutate(
    essay_length = char_length(paste(!!!syms(paste0("essay", 0:9))))
  ) %>% compute()
```
```{r modeling-eda-essay-plot, echo=FALSE, eval=FALSE}
db_compute_bins(okc_train, essay_length, bins = 100) %>%
  ggplot() +
  geom_col(aes(essay_length, count), fill = "#878787") +
  geom_hline(yintercept = 0, size = 1, colour = "#333333") +
  labs(title = "Distribution of Essay Length", subtitle = "Essay length histogram in OKCupid dataset")
```
```{r modeling-eda-essay-render}
dbplot_histogram(okc_train, essay_length, bins = 100)
```








# sparklyr has some functions such as spark_read_csv() that will read a CSV file into Spark
# Load dplyr
library(dplyr)
# Explore track_metadata structure
str(track_metadata)
# Connect to your Spark cluster
spark_conn <- spark_connect(master = "local")
# Copy track_metadata to Spark
track_metadata_tbl <- copy_to(spark_conn, track_metadata, overwrite = TRUE)
# List the data frames available in Spark
src_tbls(spark_conn)
# Disconnect from Spark
spark_disconnect(sc = spark_conn)
###################################
# Link to the track_metadata table in Spark
track_metadata_tbl <- tbl(spark_conn, "track_metadata")
# See how big the dataset is
dim(track_metadata_tbl)
# See how small the tibble is         ( from the r pryr package)
object_size(track_metadata_tbl)
################################
# Print 5 rows, all columns
print(track_metadata_tbl, n=5, width=Inf)
# Examine structure of tibble
str(track_metadata_tbl)
# Examine structure of data
glimpse(track_metadata_tbl)
#############################################
Before you try the exercise, take heed of two warnings. Firstly, don't mistake dplyr's filter() function with the stats package's filter() function. Secondly, sparklyr converts your dplyr code into SQL database code before passing it to Spark. That means that only a limited number of filtering operations are currently supported. For example, you can't filter character rows using regular expressions with code like
a_tibble %>%
  filter(grepl("a regex", x))
The help page for translate_sql() describes the functionality that is available. You are OK to use comparison operators like >, !=, and %in%; arithmetic operators like +, ^, and %%; and logical operators like &, | and !. Many mathematical functions such as log(), abs(), round(), and sin() are also supported.
As before, square bracket indexing does not currently work.
######################################
# track_metadata_tbl has been pre-defined
track_metadata_tbl
# Manipulate the track metadata
track_metadata_tbl %>%
  # Select columns
  select(title, duration) %>%
  # Mutate columns
  mutate(duration_minutes = duration/60 )
In case you hadnt got the message already that base-R functions dont work with Spark tibbles, you cant use within() or transform() for this purpose.
##############################

# track_metadata_tbl has been pre-defined
track_metadata_tbl
track_metadata_tbl %>%
  # Select columns starting with artist
  select(starts_with("artist"))
track_metadata_tbl %>%
  # Select columns ending with id
  select(ends_with("id"))


#####################
# track_metadata_tbl has been pre-defined
track_metadata_tbl
track_metadata_tbl %>%
  # Select columns containing ti
  select(contains("ti"))
track_metadata_tbl %>%
  # Select columns matching ti.?t
  select(matches("ti.?t"))

#########################################
#for factors, can use levels() function
track_metadata_tbl %>%
  # Only return rows with distinct artist_name
  distinct(artist_name)

######################################
# table is not supported in sparklyr
# track_metadata_tbl has been pre-defined
track_metadata_tbl
track_metadata_tbl %>%
  # Count the artist_name values
  count(artist_name, sort = TRUE) %>%
  # Restrict to top 20
  top_n(20)

##########################
# copy_to()   moves data from R to spark.   collect() moves it back

# track_metadata_tbl has been pre-defined
track_metadata_tbl
results <- track_metadata_tbl %>%
  # Filter where artist familiarity is greater than 0.9
  filter(artist_familiarity > 0.9)
# Examine the class of the results
class(results)
# Collect your results
collected <- results %>%
  collect()
# Examine the class of the collected results
class(collected)

###################################
# use compute() to compute the calculation, but store the results in a temporary data frame on Spark. Compute takes two arguments: a tibble, and a variable name for the Spark data frame that will store the results.
# track_metadata_tbl has been pre-defined
track_metadata_tbl
computed <- track_metadata_tbl %>%
  # Filter where artist familiarity is greater than 0.8
  filter(artist_familiarity > 0.8) %>%
  # Compute the results
  compute("familiar_artists")
# See the available datasets
src_tbls(spark_conn)
# Examine the class of the computed results
class(computed)

##############################
# Note that the columns passed to group_by() should typically be categorical variables. For example, if you wanted to calculate the average weight of people relative to their height, it doesn't make sense to group by height, since everyone's height is unique. You could, however, use cut() to convert the heights into different categories, and calculate the mean weight for each category.
# track_metadata_tbl has been pre-defined
track_metadata_tbl
duration_by_artist <- track_metadata_tbl %>%
  # Group by artist
  group_by(artist_name) %>%
  # Calc mean duration
  summarize(mean_duration = mean(duration))
duration_by_artist %>%
  # Sort by ascending mean duration
  arrange(mean_duration)
duration_by_artist %>%
  # Sort by descending mean duration
  arrange(desc(mean_duration))

##############################
# track_metadata_tbl has been pre-defined
track_metadata_tbl
track_metadata_tbl %>%
  # Group by artist
  group_by(artist_name) %>%
  # Calc time since first release
  mutate(time_since_first_release = year - min(year)) %>%
  # Arrange by descending time since first release
  arrange(desc(time_since_first_release))

#############################
As previously mentioned, when you use the dplyr interface, sparklyr converts your code into SQL before passing it to Spark. Most of the time, this is what you want. However, you can also write raw SQL to accomplish the same task. Most of the time, this is a silly idea since the code is harder to write and harder to debug. However, if you want your code to be portable – that is, used outside of R as well – then it may be useful. For example, a fairly common workflow is to use sparklyr to experiment with data processing, then switch to raw SQL in a production environment. By writing raw SQL to begin with, you can just copy and paste your queries when you move to production.
SQL queries are written as strings, and passed to dbGetQuery() from the DBI package. The pattern is as follows.
query <- "SELECT col1, col2 FROM some_data WHERE some_condition"
a_data.frame <- dbGetQuery(spark_conn, query)
Note that unlike the dplyr code you've written, dbGetQuery() will always execute the query and return the results to R immediately. If you want to delay returning the data, you can use dbSendQuery() to execute the query, then dbFetch() to return the results. That's more advanced usage, not covered here. Also note that DBI functions return data.frames rather than tibbles, since DBI is a lower-level package.

# Write SQL query
query <- "SELECT * FROM track_metadata WHERE year < 1935 AND duration > 300"
# Run the query
(results <- dbGetQuery(spark_conn, query))
###################################
# track_metadata_tbl and artist_terms_tbl have been pre-defined
track_metadata_tbl
artist_terms_tbl
# Left join artist terms to track metadata by artist_id
joined <- left_join(track_metadata_tbl, artist_terms_tbl, by = "artist_id")
joined <- semi_join(track_metadata_tbl, artist_terms_tbl, by = "artist_id")
anti_join

# How many rows and columns are in the joined table?
dim(joined)

########################
ft_          feature transformation functions   ( cut, )
ml_          machine learning transformations
sdf_         spark dataframe api         (sampling, partitioning )
#######################
The sparklyr way of converting a continuous variable into logical uses ft_binarizer(). The previous diabetes example can be rewritten as the following. Note that the threshold value should be a number, not a string refering to a column in the dataset.
diabetes_data %>%
  ft_binarizer("plasma_glucose_concentration", "has_diabetes", threshold = threshold_mmol_per_l)
In keeping with the Spark philosophy of using DoubleType everywhere, the output from ft_binarizer() isnt actually logical; it is numeric. This is the correct approach for letting you continue to work in Spark and perform other transformations, but if you want to process your data in R, you have to remember to explicitly convert the data to logical. The following is a common code pattern.
a_tibble %>%
  ft_binarizer("x", "is_x_big", threshold = threshold) %>%
  collect() %>%
  mutate(is_x_big = as.logical(is_x_big))

hotttnesss <- track_metadata_tbl %>%
  # Select artist_hotttnesss
  select(artist_hotttnesss) %>%
  # Binarize to is_hottt_or_nottt
  ft_binarizer("artist_hotttnesss","is_hottt_or_nottt", threshold = 0.5) %>%
  # Collect the result
  collect() %>%
  # Convert is_hottt_or_nottt to logical
  mutate(is_hottt_or_nottt = as.logical(is_hottt_or_nottt))
# Draw a barplot of is_hottt_or_nottt
ggplot(hotttnesss, aes(is_hottt_or_nottt)) +
  geom_bar()
####################################################################
The sparklyr equivalent of this is to use ft_bucketizer(). The code takes a similar format to ft_binarizer(), but this time you must pass a vector of cut points to the splits argument. Here is the same example rewritten in sparklyr style.
smoking_data %>%
  ft_bucketizer("cigarettes_per_day", "smoking_status", splits = c(0, 1, 10, 20, Inf))
There are several important things to note. You may have spotted that the breaks argument from cut() is the same as the splits argument from ft_bucketizer(). There is a slight difference in how values on the boundary are handled. In cut(), by default, the upper (right-hand) boundary is included in each bucket, but not the left. ft_bucketizer() includes the lower (left-hand) boundary in each bucket, but not the right. This means that it is equivalent to calling cut() with the argument right = FALSE.
One exception is that ft_bucketizer() includes values on both boundaries for the upper-most bucket. So ft_bucketizer() is also equivalent to setting include.lowest = TRUE when using cut().
The final thing to note is that whereas cut() returns a factor, ft_bucketizer() returns a numeric vector, with values in the first bucket returned as zero, values in the second bucket returned as one, values in the third bucket returned as two, and so on. If you want to work on the results in R, you need to explicitly convert to a factor. This is a common code pattern:
  
  a_tibble %>%
  ft_bucketizer("x", "x_buckets", splits = splits) %>%
  collect() %>%
  mutate(x_buckets = factor(x_buckets, labels = labels)
         
         # track_metadata_tbl, decades, decade_labels have been pre-defined
         track_metadata_tbl
         decades
         decade_labels
         
         hotttnesss_over_time <- track_metadata_tbl %>%
           # Select artist_hotttnesss and year
           select(artist_hotttnesss, year) %>%
           # Convert year to numeric
           mutate(year = as.numeric(year)) %>%
           # Bucketize year to decade using decades vector
           ft_bucketizer("year","decade",splits = decades) %>%
           # Collect the result
           collect() %>%
           # Convert decade to factor using decade_labels
           mutate(decade = factor(decade, labels = decade_labels))
         
         # Draw a boxplot of artist_hotttnesss by decade
         ggplot(hotttnesss_over_time, aes(decade, artist_hotttnesss)) +
           geom_boxplot()  
         
         
         ###########################################
         
         
         # track_metadata_tbl, duration_labels have been pre-defined
         track_metadata_tbl
         duration_labels
         
         familiarity_by_duration <- track_metadata_tbl %>%
           # Select duration and artist_familiarity
           select(duration, artist_familiarity) %>%
           # Bucketize duration
           ft_quantile_discretizer("duration","duration_bin", n.buckets = 5) %>%
           # Collect the result
           collect() %>%
           # Convert duration bin to factor
           mutate(duration_bin = factor(duration_bin, labels = duration_labels))
         
         # Draw a boxplot of artist_familiarity by duration_bin
         ggplot(familiarity_by_duration, aes(duration_bin, artist_familiarity)) +
           geom_boxplot() 
         
         #############################################################
         
         
         More than words: tokenization (1)
         Common uses of text-mining include analyzing shopping reviews to ascertain purchasers feeling about the product, or analyzing financial news to predict the sentiment regarding stock prices. In order to analyze text data, common pre-processing steps are to convert the text to lower-case (see tolower()), and to split sentences into individual words.
         ft_tokenizer() performs both these steps. Its usage takes the same pattern as the other transformations that you have seen, with no other arguments.
         shop_reviews %>%
           ft_tokenizer("review_text", "review_words")
         Since the output can contain a different number of words in each row, output.col is a list column, where every element is a list of strings. To analyze text data, it is usually preferable to have one word per row in the data. The list-of-list-of-strings format can be transformed to a single character vector using unnest() from the tidyr package. There is currently no method for unnesting data on Spark, so for now, you have to collect it to R before transforming it. The code pattern to achieve this is as follows.
         library(tidyr)
         text_data %>%
           ft_tokenizer("sentences", "word") %>%
           collect() %>%
           mutate(word = lapply(word, as.character)) %>%
           unnest(word)
         # track_metadata_tbl has been pre-defined
         track_metadata_tbl
         title_text <- track_metadata_tbl %>%
           # Select artist_name, title
           select(artist_name, title) %>%
           # Tokenize title to words
           ft_tokenizer("title", "word") %>%
           # Collect the result
           collect() %>%
           # Flatten the word column 
           mutate(word = lapply(word, as.character)) %>% 
           # Unnest the list column
           unnest(word)
         #################################
         More than words: tokenization (3)
         ft_tokenizer() uses a simple technique to generate words by splitting text data on spaces. For more advanced usage, you can use regular expressions to split the text data. This is done via the ft_regex_tokenizer() function, which has the same usage as ft_tokenizer(), but with an extra pattern argument for the splitter.
         a_tibble %>%
           ft_regex_tokenizer("x", "y", pattern = regex_pattern)
         The return value from ft_regex_tokenizer(), like ft_tokenizer(), is a list of lists of character vectors.
         The dataset contains a field named artist_mbid that contains an ID for the artist on MusicBrainz, a music metadata encyclopedia website. The IDs take the form of hexadecimal numbers split by hyphens
         
         track_metadata_tbl %>%
           # Select artist_mbid column
           select(artist_mbid) %>%
           # Split it by hyphens
           ft_regex_tokenizer("artist_mbid", "artist_mbid_chunks", pattern = "-")
         ##############################
         # Compare timings of arrange() and sdf_sort()
         microbenchmark(
           arranged = track_metadata_tbl %>%
             # Arrange by year, then artist_name, then release, then title
             arrange(year, artist_name, release, title) %>%
             # Collect the result
             collect(),
           sorted = track_metadata_tbl %>%
             # Sort by year, then artist_name, then release, then title
             sdf_sort(c("year","artist_name","release","title")) %>%
             # Collect the result
             collect(),
           times = 5
         )
         ########################################
         looking at the data types
         # track_metadata_tbl has been pre-defined
         track_metadata_tbl
         # Get the schema
         (schema <- sdf_schema(track_metadata_tbl))
         # ad way to Transform the schema into something more readable
         schema %>%
           lapply(function(x) do.call(data_frame, x)) %>%
           bind_rows()
         R type               Spark type
         logical              boolean
         numeric              doubletype
         integer              integertype
         character            stringtype
         list                 arraytype
         ##################################
         Shrinking the data by sampling
         When you are working with a big dataset, you typically dont really need to work with all of it all the time. Particularly at the start of your project, while you are experimenting wildly with what you want to do, you can often iterate more quickly by working on a smaller subset of the data. sdf_sample() provides a convenient way to do this. It takes a tibble, and the fraction of rows to return. In this case, you want to sample without replacement. To get a random sample of one tenth of your dataset, you would use the following code.
         a_tibble %>%
           sdf_sample(fraction = 0.1, replacement = FALSE)
         Since the results of the sampling are random, and you will likely want to reuse the shrunken dataset, it is common to use compute() to store the results as another Spark data frame. 
         a_tibble %>%
           sdf_sample(<some args>) %>%
           compute("sample_dataset")
         To make the results reproducible, you can also set a random number seed via the seed argument. Doing this means that you get the same random dataset every time you run your code. It doesnt matter which number you use for the seed; just choose your favorite positive integer.
         ######################################
         Training/testing partitions
         Most of the time, when you run a predictive model, you need to fit the model on one subset of your data (the "training" set), then test the model predictions against the rest of your data (the "testing" set).
         sdf_partition() provides a way of partitioning your data frame into training and testing sets. Its usage is as follows.
         a_tibble %>%
           sdf_partition(training = 0.7, testing = 0.3)
         There are two things to note about the usage. Firstly, if the partition values don't add up to one, they will be scaled so that they do. So if you passed training = 0.35 and testing = 0.15, you'd get double what you asked for. Secondly, you can use any set names that you like, and partition the data into more than two sets. So the following is also valid.
         a_tibble %>%
           sdf_partition(a = 0.1, b = 0.2, c = 0.3, d = 0.4)
         The return value is a list of tibbles. you can access each one using the usual list indexing operators.
         partitioned$a
         partitioned[["b"]]
         partitioned <- track_metadata_tbl %>%
           # Partition into training and testing sets
           sdf_partition(training = 0.7, testing = 0.3)
         # Get the dimensions of the training set
         dim(partitioned$training)
         # Get the dimensions of the testing set
         dim(partitioned$testing)
         #################################
         Supported machine learning functions include linear regression and its variants, tree-based models (ml_decision_tree(), and a few others. You can see the list of all the machine learning functions using ls().
                                                                                                             
  ls("package:sparklyr", pattern = "^ml")
  
  #########################################################
  Working with parquet files
  CSV files are great for saving the contents of rectangular data objects (like R data.frames and Spark DataFrames) to disk. The problem is that they are really slow to read and write, making them unusable for large datasets. Parquet files provide a higher performance alternative. As well as being used for Spark data, parquet files can be used with other tools in the Hadoop ecosystem, like Shark, Impala, Hive, and Pig.
  Technically speaking, parquet file is a misnomer. When you store data in parquet format, you actually get a whole directory worth of files. The data is split across multiple .parquet files, allowing it to be easily stored on multiple machines, and there are some metadata files too, describing the contents of each column.
  sparklyr can import parquet files using spark_read_parquet(). This function takes a Spark connection, a string naming the Spark DataFrame that should be created, and a path to the parquet directory. Note that this function will import the data directly into Spark, which is typically faster than importing the data into R, then using copy_to() to copy the data from R to Spark.
  spark_read_parquet(sc, "a_dataset", "path/to/parquet/dir")
  
  
  # parquet_dir has been pre-defined
  parquet_dir
  # List the files in the parquet dir
  filenames <- dir(parquet_dir, full.names = TRUE)
  # Show the filenames and their sizes
  data_frame(
     filename = basename(filenames),
     size_bytes = file.size(filenames)
   )
   # Import the data into Spark
   timbre_tbl <- spark_read_parquet(spark_conn, "timbre", parquet_dir)
   
   ######################################
   
   Partitioning data with a group effect
   Before you can run any models, you need to partition your data into training and testing sets. There's a complication with this dataset, which means you can't just call sdf_partition(). The complication is that each track by a single artist ought to appear in the same set; your model will appear more accurate than it really is if tracks by an artist are used to train the model then appear in the testing set.
   The trick to dealing with this is to partition only the artist IDs, then inner join those partitioned IDs to the original dataset. Note that artist_id is more reliable than artist_name for partitioning, since some artists use variations on their name between tracks. For example, Duke Ellington sometimes has an artist name of "Duke Ellington", but other times has an artist name of "Duke Ellington & His Orchestra", or one of several spelling variants.
   training_testing_artist_ids <- track_data_tbl %>%
     # Select the artist ID
     select(artist_id) %>%
     # Get distinct rows
     distinct() %>%
     # Partition into training/testing sets
     sdf_partition(training = 0.7, testing = 0.3)
   track_data_to_model_tbl <- track_data_tbl %>%
     # Inner join to training partition
     inner_join(training_testing_artist_ids$training, by = "artist_id")
   track_data_to_predict_tbl <- track_data_tbl %>%
     # Inner join to testing partition
     inner_join(training_testing_artist_ids$testing, by = "artist_id")
   #################################################
   An ML Gradient boosted tree model (all numeric features)
   
   feature_colnames <- track_data_to_model_tbl %>%
     # Get the column names
     colnames() %>%
     # Limit to the timbre columns
     str_subset(fixed("timbre"))
   gradient_boosted_trees_model <- track_data_to_model_tbl %>%
     # Run the gradient boosted trees model
     ml_gradient_boosted_trees(features = feature_colnames, response = "year")
   #################################
   Predictions
   # training, testing sets & model are pre-defined
   track_data_to_model_tbl
   track_data_to_predict_tbl
   gradient_boosted_trees_model
   responses <- track_data_to_predict_tbl %>%
     # Select the year column
     select(year) %>%
     # Collect the results
     collect() %>%
     # Add in the predictions
     mutate(
       predicted_year = predict(
         gradient_boosted_trees_model,
         track_data_to_predict_tbl
       )
     )
   
   ###############################
   # plots to assess quality of model fit
   One slightly tricky thing here is that sparklyr doesnt yet support the residuals() function in all its machine learning models. Consequently, you have to calculate the residuals yourself (predicted responses minus actual responses).
   # Draw a scatterplot of predicted vs. actual
   ggplot(responses, aes(actual, predicted)) +
     # Add the points
     geom_point(alpha = 0.1) +
     # Add a line at actual = predicted
     geom_abline(intercept = 0, slope = 1)
   residuals <- responses %>%
     # Transmute response data to residuals
     transmute(residual = responses$predicted-responses$actual)
   # Draw a density plot of residuals
   ggplot(residuals, aes(residual)) +
     # Add a density curve
     geom_density() +
     # Add a vertical line through zero
     geom_vline(xintercept = 0)
   
   ##################################
   Random Forest: modeling
   Like gradient boosted trees, random forests are another form of ensemble model. That is, they use lots of simpler models (decision trees, again) and combine them to make a single better model. Rather than running the same model iteratively, random forests run lots of separate models in parallel, each on a randomly chosen subset of the data, with a randomly chosen subset of features. Then the final decision tree makes predictions by aggregating the results from the individual models.
   sparklyrs random forest function is called ml_random_forest(). Its usage is exactly the same as ml_gradient_boosted_trees() (see the first exercise of this chapter for a reminder on syntax).
   # Get the timbre columns
   feature_colnames <- track_data_to_model_tbl %>%
     colnames() %>%
     str_subset(fixed("timbre"))
   # Run the random forest model
   random_forest_model <- track_data_to_model_tbl %>%
     ml_random_forest(features = feature_colnames, response = "year")
   ################################################
   # Create a response vs. actual dataset
   responses <- track_data_to_predict_tbl %>%
     select(year) %>%
     collect() %>%
     mutate(predicted_year = predict(random_forest_model, track_data_to_predict_tbl))
   
   ################################
   # Create a residual sum of squares dataset
   both_responses %>%
     mutate(residual = both_responses$predicted - both_responses$actual) %>%
     group_by(model) %>%
     summarise(rmse = sqrt(mean(residual * residual)))   
   ######################################