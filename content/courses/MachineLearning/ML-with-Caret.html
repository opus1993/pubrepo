---
author: "Jim Gruman"
output: html_document
date: "12/29/2019"
draft: false
linktitle: ML with Caret
menu:
  example:
    parent: Machine Learning in R
    weight: 16
title: Machine Learning with Caret
type: docs
weight: 16
---



<p>The caret package (short for <strong>C</strong>lassification <strong>A</strong>nd <strong>RE</strong>gression <strong>T</strong>raining) provides a unified interface and contains functions to streamline the model training process for complex regression and classification problems. It utilizes a number of other R packages but tries not to load them all at package start-up. Caret has built-in parallel processing for increased computational efficiency.</p>
<p>As of this writing, version 6.0-84 was available at CRAN, currently maintained by Max Kuhn. The main help pages for the package are at <a href="https://topepo.github.io/caret/">Caret at Github</a>. Max includes extended examples and a large amount of information in the package vignettes.</p>
<pre class="r"><code>library(caret)</code></pre>
<pre><code>## Warning: package &#39;caret&#39; was built under R version 3.6.2</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre class="r"><code>library(mlbench)
library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>library(parallel)
library(doParallel)</code></pre>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## Loading required package: iterators</code></pre>
<pre class="r"><code>cores&lt;-detectCores()
doParallel::registerDoParallel(cores = cores)
library(pROC)</code></pre>
<pre><code>## Warning: package &#39;pROC&#39; was built under R version 3.6.2</code></pre>
<pre><code>## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.</code></pre>
<pre><code>## 
## Attaching package: &#39;pROC&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, smooth, var</code></pre>
<p>Caret has several functions that attempt to streamline the model building and evaluation process, as well as feature selection and other techniques.</p>
<p>One of the primary tools in the package is the train function which are be used to</p>
<blockquote>
<p>evaluate, using resampling, the effect of model tuning parameters on performance</p>
</blockquote>
<blockquote>
<p>choose the ``optimal’’ model across these parameters</p>
</blockquote>
<blockquote>
<p>estimate model performance</p>
</blockquote>
<p>Caret includes options for customizing almost every step of this process (e.g. resampling technique, choosing the optimal parameters etc).</p>
<p>As a quick demonstration, a linear model can be built on the widely known mtcars data to model mpg by hp as follows:</p>
<pre class="r"><code>data(&quot;mtcars&quot;)
set.seed(42)

model&lt;-train(
    mpg ~ hp, mtcars,
    method = &quot;lm&quot;,
)

model$method</code></pre>
<pre><code>## [1] &quot;lm&quot;</code></pre>
<pre class="r"><code>model$finalModel$coefficients</code></pre>
<pre><code>## (Intercept)          hp 
## 30.09886054 -0.06822828</code></pre>
<pre class="r"><code>model$coefnames</code></pre>
<pre><code>## [1] &quot;hp&quot;</code></pre>
<pre class="r"><code>model$results</code></pre>
<pre><code>##   intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD     MAESD
## 1      TRUE 4.064945 0.6633927 3.214819 0.922679 0.09029965 0.7759477</code></pre>
<pre class="r"><code>newdata&lt;-data.frame(hp = seq(50,300,10))

newdata$mpg&lt;-predict(model, newdata)

ggplot(mtcars, aes(hp, mpg))+
  geom_point()+
  geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) +
  theme_minimal() +
  theme(legend.position = &#39;none&#39;)</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/courses/MachineLearning/ML-with-Caret_files/figure-html/lm-1.png" width="672" /></p>
<p>For a slightly more complex demonstration, we look at Housing data for 506 census tracts of Boston from the 1970 census.</p>
<pre class="r"><code>data(&quot;BostonHousing&quot;)

regVar &lt;- c(&quot;age&quot;, &quot;lstat&quot;, &quot;tax&quot;)

featurePlot(x = BostonHousing[, regVar], 
            y = BostonHousing$medv, 
            plot = &quot;scatter&quot;, 
            layout = c(3, 1))</code></pre>
<p><img src="/courses/MachineLearning/ML-with-Caret_files/figure-html/medv%20dataviz-1.png" width="672" /></p>
<p>In this case, we will model median home value medv in a regression by 12 numeric and one 2-level categorical feature. In this case, we will apply 5-fold cross validation. Simple bootstrap resampling is the default in trainControl</p>
<pre class="r"><code># Fit lm model using 5-fold CV: model
model &lt;- train(
  medv~., 
  BostonHousing,
  method = &quot;lm&quot;,
  trControl = trainControl(
    method = &quot;cv&quot;, 
    number = 5,
    verboseIter = TRUE
  )
)</code></pre>
<pre><code>## Aggregating results
## Fitting final model on full training set</code></pre>
<pre class="r"><code># Print model to console
model$method</code></pre>
<pre><code>## [1] &quot;lm&quot;</code></pre>
<pre class="r"><code>model$finalModel$coefficients</code></pre>
<pre><code>##   (Intercept)          crim            zn         indus         chas1 
##  3.645949e+01 -1.080114e-01  4.642046e-02  2.055863e-02  2.686734e+00 
##           nox            rm           age           dis           rad 
## -1.776661e+01  3.809865e+00  6.922246e-04 -1.475567e+00  3.060495e-01 
##           tax       ptratio             b         lstat 
## -1.233459e-02 -9.527472e-01  9.311683e-03 -5.247584e-01</code></pre>
<pre class="r"><code>model$results</code></pre>
<pre><code>##   intercept     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD
## 1      TRUE 4.781609 0.7316689 3.354985 0.4922888  0.0318399 0.1808421</code></pre>
<p>Another method, “repeatedcv”, is used to specify repeated K-fold cross-validation (and the argument <strong>repeats</strong> controls the number of repetitions). <strong>K</strong> is controlled by the number argument and defaults to 10. The new syntax for an lm model using 5 x 5-fold CV: model is then:</p>
<pre class="r"><code>model &lt;- train(
  medv ~ ., 
  BostonHousing,
  method = &quot;lm&quot;,
  trControl = trainControl(
    method = &quot;repeatedcv&quot;, 
    number = 5,
    repeats = 5, 
    verboseIter = TRUE
  )
)</code></pre>
<pre><code>## Aggregating results
## Fitting final model on full training set</code></pre>
<pre class="r"><code># Print model to console
model$method</code></pre>
<pre><code>## [1] &quot;lm&quot;</code></pre>
<pre class="r"><code>model$finalModel$coefficients</code></pre>
<pre><code>##   (Intercept)          crim            zn         indus         chas1 
##  3.645949e+01 -1.080114e-01  4.642046e-02  2.055863e-02  2.686734e+00 
##           nox            rm           age           dis           rad 
## -1.776661e+01  3.809865e+00  6.922246e-04 -1.475567e+00  3.060495e-01 
##           tax       ptratio             b         lstat 
## -1.233459e-02 -9.527472e-01  9.311683e-03 -5.247584e-01</code></pre>
<pre class="r"><code>model$results</code></pre>
<pre><code>##   intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD     MAESD
## 1      TRUE 4.820318 0.7285316 3.379137 0.566678 0.04952456 0.2604407</code></pre>
<hr />
<div id="handling-missing-data" class="section level1">
<h1>Handling missing data</h1>
<p>Most models require numbers and cannot handle missing data. A common approach is to remove rows entirely with missing data, which can create biases and generate over-confident models. A much better strategy is to replace the missing values with the median, if and only if the data are missing at random.</p>
<p>For linear regression models, it is a best practice to preProcess with medianImpute, centering, scaling, and then fit the glm model. Another option is to preProcess with knnImpute.</p>
<p>In the real world, some variables do non contain much information. For example, variables that are mostly constant should be removed because one fold could end up with that column. Nearly constant columns should also be removed.</p>
<p>For example:
preProcess = c(“zv”, “medianImpute”, “center”, “scale”, “pca”)</p>
<p>A Preprocessing cheat sheet:</p>
<ol style="list-style-type: decimal">
<li><p>start with median imputation</p></li>
<li><p>consider knn imputation if the data missing is not at random</p></li>
<li><p>for linear models, center and scale</p></li>
<li><p>try PCA and spatial sign</p></li>
</ol>
<blockquote>
<p>example preProcess = c(“nzv”, “center”, “scale”, “pca”)</p>
</blockquote>
<pre class="r"><code>data(&quot;BloodBrain&quot;)

bloodbrain_x&lt;-bbbDescr
bloodbrain_y&lt;-logBBB

# Identify near zero variance predictors: remove_cols
remove_cols &lt;- nearZeroVar(bloodbrain_x, names = TRUE, 
                           freqCut = 2, uniqueCut = 20)

# Get all column names from bloodbrain_x: all_cols
all_cols&lt;-names(bloodbrain_x)

# Remove from data: bloodbrain_x_small
bloodbrain_x_small &lt;- bloodbrain_x[ , setdiff(all_cols, remove_cols)]</code></pre>
<p>OR — use train with preProcess and “nzv” in the character string</p>
<pre class="r"><code># Fit model on reduced data: model
model &lt;- train(
  x = bloodbrain_x_small, 
  y = bloodbrain_y, 
  method = &quot;glm&quot;
)

# Print model to console
model </code></pre>
<pre><code>## Generalized Linear Model 
## 
## 208 samples
## 112 predictors
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 208, 208, 208, 208, 208, 208, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   742.0165  0.1071357  181.8144</code></pre>
<div id="principal-components-analysis-as-a-pre-processing-step-in-regression-models" class="section level2">
<h2>Principal components analysis as a pre-processing step in regression models</h2>
<ol style="list-style-type: decimal">
<li><p>Combines low-variance and correlated variables</p></li>
<li><p>Aggregates into a single set of high-variance, perpendicular predictors</p></li>
<li><p>Prevents collinearity</p></li>
</ol>
<pre class="r"><code>set.seed(42)

model &lt;- train(
  bbbDescr,
  logBBB,
  method = &quot;glm&quot;,
  trControl = trainControl(
    method = &quot;cv&quot;, number = 10, verbose = TRUE,
  ),
  preProcess = c(&quot;zv&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;)
)</code></pre>
<pre><code>## Aggregating results
## Fitting final model on full training set</code></pre>
<pre class="r"><code>min(model$results$RMSE)</code></pre>
<pre><code>## [1] 0.5560624</code></pre>
<hr />
</div>
</div>
<div id="the-sonar-dataset" class="section level1">
<h1>The Sonar Dataset</h1>
<div id="partial-least-squares-and-regularized-discriminant-analysis" class="section level2">
<h2>Partial Least Squares and Regularized Discriminant Analysis</h2>
<p>The mlbench <strong>Sonar</strong> data set was used initially in a study of the classification of sonar signals to train a network to discriminate between signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock. Each of 208 observations are patterns in a set with 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.</p>
<p>The label associated with each record contains the letter “R” if the object is a rock and “M” if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.</p>
<p>First, we split the data into two groups: a training set and a test set with the <strong>createDataPartition</strong> function. By default, <strong>createDataPartition</strong> does a stratified random split of the data. Here, a partial least squares discriminant analysis (PLSDA) model will be tuned over the number of PLS components that should be retained.</p>
<p>We would like to customize it in a few ways:
- expand the set of PLS models that the function evaluates. By default, the function will tune over three values of each tuning parameter.
- the type of resampling used. The simple bootstrap is used by default. We will have the function use three repeats of 10-fold cross-validation.
- the methods for measuring performance. If unspecified, overall accuracy and the Kappa statistic are computed. For regression models, root mean squared error and R2 are computed. Here, the function will be altered to estimate the area under the ROC curve, the sensitivity and specificity</p>
<p>To change the candidate values of the tuning parameter, either of the <strong>tuneLength</strong> or <strong>tuneGrid</strong> arguments can be used. The train function can generate a candidate set of parameter values and the tuneLength argument controls how many are evaluated. In the case of PLS, the function uses a sequence of integers from 1 to tuneLength. If we want to evaluate all integers between 1 and 15, setting tuneLength = 15 would achieve this. The <strong>tuneGrid</strong> argument is used when specific values are desired. A data frame is used where each row is a tuning parameter setting and each column is a tuning parameter. An example is used below to illustrate this.</p>
<p>The function will pick the tuning parameters associated with the best results. Since we are using custom performance measures, the criterion that should be optimized must also be specified. In the call to train, we can use <strong>metric</strong> = “ROC” to do this.</p>
<pre class="r"><code># Create trainControl object: myControl
data(&quot;Sonar&quot;)
set.seed(42)

inTrain &lt;- createDataPartition(
  y = Sonar$Class,
  ## the outcome data are needed
  p = .75,
  ## The percentage of data in the
  ## training set
  list = FALSE
)

training &lt;- Sonar[ inTrain,]
testing  &lt;- Sonar[-inTrain,]

myControl &lt;- trainControl(
  method = &quot;repeatedcv&quot;,
  repeats = 3,
  summaryFunction = twoClassSummary,
  classProbs = TRUE, # IMPORTANT!   must have to calculate AUC or logloss
#  verboseIter = TRUE
)

# Train pls with custom trainControl: model
plsModel&lt;-train(
  Class~., 
  data = training,
  method = &quot;pls&quot;,
  trControl = myControl,
  tuneLength = 15,
  preProc = c(&quot;center&quot;,&quot;scale&quot;),
  metric = &quot;ROC&quot;)

# Print model to console
plsModel</code></pre>
<pre><code>## Partial Least Squares 
## 
## 157 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## Pre-processing: centered (60), scaled (60) 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 141, 142, 141, 142, 141, 141, ... 
## Resampling results across tuning parameters:
## 
##   ncomp  ROC        Sens       Spec     
##    1     0.8216353  0.6939815  0.7071429
##    2     0.8677993  0.7884259  0.7892857
##    3     0.9093833  0.7902778  0.8238095
##    4     0.8945933  0.7736111  0.8029762
##    5     0.8750909  0.7555556  0.7934524
##    6     0.8638476  0.7740741  0.7976190
##    7     0.8667163  0.7898148  0.7934524
##    8     0.8670552  0.8000000  0.8154762
##    9     0.8740162  0.8004630  0.8196429
##   10     0.8764716  0.8000000  0.8208333
##   11     0.8747106  0.7921296  0.8113095
##   12     0.8714947  0.7759259  0.8130952
##   13     0.8659722  0.7712963  0.7934524
##   14     0.8622354  0.7833333  0.7928571
##   15     0.8665427  0.8041667  0.7976190
## 
## ROC was used to select the optimal model using the largest value.
## The final value used for the model was ncomp = 3.</code></pre>
<pre class="r"><code>ggplot(plsModel)</code></pre>
<p><img src="/courses/MachineLearning/ML-with-Caret_files/figure-html/sonar-1.png" width="672" /></p>
<pre class="r"><code># Let’s save the model. If you want to deploy it, you can push .rda file with your code to production.
# save(plsModel, file = &quot;./plsModel.rda&quot;)

# Once you successfully save it, close the current R session. Then, you can load it back in the new session. It’s ready for use.
# load(&quot;plsModel.rda&quot;)</code></pre>
<p>In this output the grid of results are the average resampled estimates of performance. The note at the bottom tells the user that 4 PLS components were found to be optimal. Based on this value, a final PLS model is fit to the whole data set using this specification and this is the model that is used to predict future samples.</p>
<p>The package has several functions for visualizing the results. One method for doing this is the ggplot function for train objects. The command ggplot(plsFit) produces the results seen in Figure and shows the relationship between the resampled performance values and the number of PLS components.</p>
<pre class="r"><code>plsClasses &lt;- predict(plsModel, newdata = testing)
str(plsClasses)</code></pre>
<pre><code>##  Factor w/ 2 levels &quot;M&quot;,&quot;R&quot;: 1 2 2 1 1 2 1 2 1 2 ...</code></pre>
<pre class="r"><code>plsProbs &lt;- predict(plsModel, newdata = testing, type = &quot;prob&quot;)
head(plsProbs)</code></pre>
<pre><code>##            M         R
## 3  0.7024082 0.2975918
## 4  0.4722453 0.5277547
## 7  0.4640816 0.5359184
## 8  0.6450411 0.3549589
## 9  0.6122580 0.3877420
## 19 0.2623379 0.7376621</code></pre>
<p><strong>Caret</strong> contains a function to compute the confusion matrix and associated statistics for the model fit:</p>
<pre class="r"><code>confusionMatrix(data = plsClasses, testing$Class)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  M  R
##          M 20  6
##          R  7 18
##                                           
##                Accuracy : 0.7451          
##                  95% CI : (0.6037, 0.8567)
##     No Information Rate : 0.5294          
##     P-Value [Acc &gt; NIR] : 0.001311        
##                                           
##                   Kappa : 0.4896          
##                                           
##  Mcnemar&#39;s Test P-Value : 1.000000        
##                                           
##             Sensitivity : 0.7407          
##             Specificity : 0.7500          
##          Pos Pred Value : 0.7692          
##          Neg Pred Value : 0.7200          
##              Prevalence : 0.5294          
##          Detection Rate : 0.3922          
##    Detection Prevalence : 0.5098          
##       Balanced Accuracy : 0.7454          
##                                           
##        &#39;Positive&#39; Class : M               
## </code></pre>
<p>To fit an another model to the data, train can be invoked with minimal changes. Lists of the wide variety of models available can be found at (<a href="https://topepo.github.io/caret/available-models.html" class="uri">https://topepo.github.io/caret/available-models.html</a>) or (<a href="https://topepo.github.io/caret/train-models-by-tag.html" class="uri">https://topepo.github.io/caret/train-models-by-tag.html</a>). For example, to fit a regularized discriminant model to these data, the following syntax is used:</p>
<pre class="r"><code>## To illustrate, a custom grid is used
rdaGrid = data.frame(gamma = (0:4)/4, lambda = 3/4)
set.seed(42)
rdaModel&lt;- train(
  Class ~ .,
  data = training,
  method = &quot;rda&quot;,
  tuneGrid = rdaGrid,
  trControl = myControl,
  metric = &quot;ROC&quot;
)
rdaModel</code></pre>
<pre><code>## Regularized Discriminant Analysis 
## 
## 157 samples
##  60 predictor
##   2 classes: &#39;M&#39;, &#39;R&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 142, 142, 141, 142, 140, 142, ... 
## Resampling results across tuning parameters:
## 
##   gamma  ROC        Sens       Spec     
##   0.00   0.8879299  0.8296296  0.8345238
##   0.25   0.9224289  0.8837963  0.8589286
##   0.50   0.9177331  0.8675926  0.8029762
##   0.75   0.8986938  0.8064815  0.7845238
##   1.00   0.7436508  0.6652778  0.6250000
## 
## Tuning parameter &#39;lambda&#39; was held constant at a value of 0.75
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were gamma = 0.25 and lambda = 0.75.</code></pre>
<pre class="r"><code>rdaClasses &lt;- predict(rdaModel, newdata = testing)
confusionMatrix(rdaClasses, testing$Class)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  M  R
##          M 23  7
##          R  4 17
##                                           
##                Accuracy : 0.7843          
##                  95% CI : (0.6468, 0.8871)
##     No Information Rate : 0.5294          
##     P-Value [Acc &gt; NIR] : 0.0001502       
##                                           
##                   Kappa : 0.5641          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.5464936       
##                                           
##             Sensitivity : 0.8519          
##             Specificity : 0.7083          
##          Pos Pred Value : 0.7667          
##          Neg Pred Value : 0.8095          
##              Prevalence : 0.5294          
##          Detection Rate : 0.4510          
##    Detection Prevalence : 0.5882          
##       Balanced Accuracy : 0.7801          
##                                           
##        &#39;Positive&#39; Class : M               
## </code></pre>
<p>How do these models compare in terms of their resampling results? The <strong>resamples</strong> function can be used to collect, summarize and contrast the resampling results. Since the random number seeds were initialized to the same value prior to calling train, the same folds were used for each model. To assemble them:</p>
<pre class="r"><code>resamps &lt;- resamples(list(pls = plsModel, rda = rdaModel))
summary(resamps)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = resamps)
## 
## Models: pls, rda 
## Number of resamples: 30 
## 
## ROC 
##          Min.   1st Qu.    Median      Mean  3rd Qu. Max. NA&#39;s
## pls 0.7142857 0.8497024 0.9419643 0.9093833 0.968626    1    0
## rda 0.5937500 0.8784722 0.9409722 0.9224289 1.000000    1    0
## 
## Sens 
##      Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA&#39;s
## pls 0.500 0.6666667 0.7777778 0.7902778 0.8888889    1    0
## rda 0.625 0.8020833 0.8888889 0.8837963 1.0000000    1    0
## 
## Spec 
##          Min.   1st Qu.    Median      Mean 3rd Qu. Max. NA&#39;s
## pls 0.5714286 0.7142857 0.8035714 0.8238095       1    1    0
## rda 0.3750000 0.7767857 0.8571429 0.8589286       1    1    0</code></pre>
<pre class="r"><code>xyplot(resamps, what = &quot;BlandAltman&quot;)</code></pre>
<p><img src="/courses/MachineLearning/ML-with-Caret_files/figure-html/resamples%20Sonar-1.png" width="672" /></p>
<p>The results look similar. Since, for each resample, there are paired results a paired t-test can be used to assess whether there is a difference in the average resampled area under the ROC curve. The diff.resamples function can be used to compute this:</p>
<pre class="r"><code>diffs &lt;- diff(resamps)
summary(diffs)</code></pre>
<pre><code>## 
## Call:
## summary.diff.resamples(object = diffs)
## 
## p-value adjustment: bonferroni 
## Upper diagonal: estimates of the difference
## Lower diagonal: p-value for H0: difference = 0
## 
## ROC 
##     pls    rda     
## pls        -0.01305
## rda 0.5535         
## 
## Sens 
##     pls     rda     
## pls         -0.09352
## rda 0.03227         
## 
## Spec 
##     pls    rda     
## pls        -0.03512
## rda 0.2497</code></pre>
<p>Based on this analysis, the difference between the models is -0.013 ROC units (the RDA model is slightly higher) and the two-sided p-value for this difference is 0.000.</p>
<hr />
</div>
</div>
<div id="sgi-simulated-dataset-of-telecom-churn" class="section level1">
<h1>SGI Simulated Dataset of Telecom Churn</h1>
<p>The C50 dataset comes with pre-defined churnTrain and churnTest partitions. Note that the observations are un-balanced, with responses that churn = “yes” at 14.4%</p>
<div id="generalized-boosted-models" class="section level2">
<h2>Generalized Boosted Models</h2>
<pre class="r"><code>library(caret)
library(C50)
data(churn)

str(churnTrain)</code></pre>
<pre><code>## &#39;data.frame&#39;:    3333 obs. of  20 variables:
##  $ state                        : Factor w/ 51 levels &quot;AK&quot;,&quot;AL&quot;,&quot;AR&quot;,..: 17 36 32 36 37 2 20 25 19 50 ...
##  $ account_length               : int  128 107 137 84 75 118 121 147 117 141 ...
##  $ area_code                    : Factor w/ 3 levels &quot;area_code_408&quot;,..: 2 2 2 1 2 3 3 2 1 2 ...
##  $ international_plan           : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 2 2 2 1 2 1 2 ...
##  $ voice_mail_plan              : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 1 1 1 2 1 1 2 ...
##  $ number_vmail_messages        : int  25 26 0 0 0 0 24 0 0 37 ...
##  $ total_day_minutes            : num  265 162 243 299 167 ...
##  $ total_day_calls              : int  110 123 114 71 113 98 88 79 97 84 ...
##  $ total_day_charge             : num  45.1 27.5 41.4 50.9 28.3 ...
##  $ total_eve_minutes            : num  197.4 195.5 121.2 61.9 148.3 ...
##  $ total_eve_calls              : int  99 103 110 88 122 101 108 94 80 111 ...
##  $ total_eve_charge             : num  16.78 16.62 10.3 5.26 12.61 ...
##  $ total_night_minutes          : num  245 254 163 197 187 ...
##  $ total_night_calls            : int  91 103 104 89 121 118 118 96 90 97 ...
##  $ total_night_charge           : num  11.01 11.45 7.32 8.86 8.41 ...
##  $ total_intl_minutes           : num  10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...
##  $ total_intl_calls             : int  3 3 5 7 3 6 7 6 4 5 ...
##  $ total_intl_charge            : num  2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...
##  $ number_customer_service_calls: int  1 1 0 2 3 0 3 0 1 0 ...
##  $ churn                        : Factor w/ 2 levels &quot;yes&quot;,&quot;no&quot;: 2 2 2 2 2 2 2 2 2 2 ...</code></pre>
<pre class="r"><code>table(churnTrain$churn) / nrow(churnTrain)</code></pre>
<pre><code>## 
##       yes        no 
## 0.1449145 0.8550855</code></pre>
<p>The caret package will seed to predict the first factor of the churn feature - “yes” - which happens to be coded as 1 in this dataset.</p>
<pre class="r"><code>predictors&lt;- names(churnTrain)[names(churnTrain) != &quot;churn&quot;]</code></pre>
<p><em>preProcess</em> calculates values that can be used to apply to any data set. They include centering, scaling, spatial sign imputation, PCA or ICA, Box-Cox transformations, and others. YeoJohnson is like Box-Cox, but does not require positive values always.</p>
<p>The prediction function can be called on the preProcess object and a data set to observe the preProcess transformation. The sequence of preProcess transformations is handled properly by caret internally.</p>
<pre class="r"><code>numerics&lt;- c(&quot;account_length&quot;,&quot;total_day_calls&quot;, &quot;total_night_calls&quot;)
procValues&lt;- preProcess(churnTrain[,numerics],
                        method = c(&quot;center&quot;, &quot;scale&quot;, &quot;YeoJohnson&quot;))
trainScaled&lt;- predict(procValues, churnTrain[,numerics])
testScaled&lt;- predict(procValues, churnTest[,numerics])

procValues</code></pre>
<pre><code>## Created from 3333 samples and 3 variables
## 
## Pre-processing:
##   - centered (3)
##   - ignored (0)
##   - scaled (3)
##   - Yeo-Johnson transformation (3)
## 
## Lambda estimates for Yeo-Johnson transformation:
## 0.89, 1.17, 0.93</code></pre>
<p>Modeling often requires specific package knowledge to build the response variable and arrive at optimal tuning. In this demonstration with the Generalized Boosted Regression Model <strong>gbm</strong>, the caret package greatly simplifies the setup and interface. In the basic call to <strong>train</strong>, we specify the predictor set, the outcome data, and the modeling technique. Note that the <strong>train</strong> is able to use the original factor input for classification. Again, for binary outcomes, the function models the probability of the first factor level. A numeric object would indicate that we are doing regression.</p>
<p>One problem is that <strong>gbm</strong> spits out a lot of information during model fitting. Let’s omit theh functions logging by using the option verbose = FALSE.</p>
<p>Also, the default <strong>gbm</strong> re-sampling is bootstrap. We can better control for and compare algorithms by creating a <strong>trainControl</strong> object to run 5 repeats of 10-fold cross validation.</p>
<p>The default <strong>gbm</strong> performance metrics for classification are Cohen’s kappa. Suppose we wanted to estimate sensitivity, specificty, and AUC of ROC. We need to tell <strong>train</strong> to produce class probabilities, estimate these statistics, and ran models by ROC AUC. In trainControl, indicating classProbs = TRUE and summaryFunction = twoClassSummary will ensure that the output model has these details.</p>
<p>By default, <strong>train</strong> uses a minimal search grid: 3 values for each tuning parameter. We might also want to expand the scope of possible <strong>gbm</strong> models to test. Let’s look at tree depths from 1:7, boosting iterations from 100 to 1,000, and two different learning rates. See ?train for the tunable parameters by model type.</p>
<pre class="r"><code>ctrl&lt;- trainControl(method = &quot;repeatedcv&quot;,
                    repeats = 5,
                    classProbs = TRUE,
                    summaryFunction = twoClassSummary)

grid&lt;- expand.grid(interaction.depth = seq(1,7, by = 2),
                   n.trees = seq(100, 1000, by = 50),
                   shrinkage = c(0.01, 0.1),
                   n.minobsinnode = 5)

gbmTune &lt;- train(x=churnTrain[,predictors],
                 y= churnTrain$churn,
                 method = &quot;gbm&quot;,
                 metric = &quot;ROC&quot;,
                 tuneGrid = grid,
                 verbose = FALSE,
                 trControl = ctrl,
                 preProcess = )

# an alternate formula interface is train(churn~., data = churnTrain, method=&quot;gbm&quot;)</code></pre>
<p>Let’s have a look at the predictions and performance measures.</p>
<pre class="r"><code>gbmPred&lt;- predict(gbmTune, churnTest)

str(gbmPred)</code></pre>
<pre><code>##  Factor w/ 2 levels &quot;yes&quot;,&quot;no&quot;: 2 2 2 2 2 2 2 2 2 2 ...</code></pre>
<pre class="r"><code>gbmProbs&lt;- predict(gbmTune, churnTest, type = &quot;prob&quot;)

confusionMatrix(gbmPred, churnTest$churn)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  yes   no
##        yes  145   27
##        no    79 1416
##                                           
##                Accuracy : 0.9364          
##                  95% CI : (0.9236, 0.9476)
##     No Information Rate : 0.8656          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.6969          
##                                           
##  Mcnemar&#39;s Test P-Value : 7.287e-07       
##                                           
##             Sensitivity : 0.64732         
##             Specificity : 0.98129         
##          Pos Pred Value : 0.84302         
##          Neg Pred Value : 0.94716         
##              Prevalence : 0.13437         
##          Detection Rate : 0.08698         
##    Detection Prevalence : 0.10318         
##       Balanced Accuracy : 0.81431         
##                                           
##        &#39;Positive&#39; Class : yes             
## </code></pre>
<pre class="r"><code>ggplot(gbmTune) + theme(legend.position = &quot;top&quot;)</code></pre>
<p><img src="/courses/MachineLearning/ML-with-Caret_files/figure-html/sgi%20churn%20gbm%20model-1.png" width="672" /></p>
<p>The train function runs the model one last time on all training data for the winning set of hyperparameters.</p>
<pre class="r"><code>rocCurve&lt;- roc(response = churnTest$churn,
               predictor = gbmProbs[, &quot;yes&quot;],
               levels = rev(levels(churnTest$churn)))</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<pre class="r"><code>rocCurve</code></pre>
<pre><code>## 
## Call:
## roc.default(response = churnTest$churn, predictor = gbmProbs[,     &quot;yes&quot;], levels = rev(levels(churnTest$churn)))
## 
## Data: gbmProbs[, &quot;yes&quot;] in 1443 controls (churnTest$churn no) &lt; 224 cases (churnTest$churn yes).
## Area under the curve: 0.9166</code></pre>
<pre class="r"><code>plot(rocCurve,
     print.thres = c(.5,.2),
     print.thres.pch = 16,
     print.thres.cex = 1.2)</code></pre>
<p><img src="/courses/MachineLearning/ML-with-Caret_files/figure-html/churn%20gbm%20model%20roc-1.png" width="672" /></p>
<p>Next step: model a Support Vector Machine and a Flexible discriminant model</p>
<pre class="r"><code>svmTune&lt;-train(churn ~ ., data = churnTrain,
               method = &quot;svmRadial&quot;,
               preProc = c(&quot;center&quot;,&quot;scale&quot;),
               tuneLength = 10,
               trControl = ctrl,
               metric = &quot;ROC&quot;)

fdaTune&lt;-train(churn ~ ., data = churnTrain,
               method = &quot;fda&quot;,
               tuneLength = 10,
               trControl = ctrl,
               metric = &quot;ROC&quot;)</code></pre>
<pre><code>## Loading required package: earth</code></pre>
<pre><code>## Warning: package &#39;earth&#39; was built under R version 3.6.2</code></pre>
<pre><code>## Loading required package: Formula</code></pre>
<pre><code>## Loading required package: plotmo</code></pre>
<pre><code>## Warning: package &#39;plotmo&#39; was built under R version 3.6.2</code></pre>
<pre><code>## Loading required package: plotrix</code></pre>
<pre><code>## Loading required package: TeachingDemos</code></pre>
<pre><code>## Warning: package &#39;TeachingDemos&#39; was built under R version 3.6.2</code></pre>
<pre class="r"><code>svmPred&lt;- predict(svmTune, churnTest)

svmProbs&lt;- predict(svmTune, churnTest, type = &quot;prob&quot;)

confusionMatrix(svmPred, churnTest$churn)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  yes   no
##        yes   92   30
##        no   132 1413
##                                           
##                Accuracy : 0.9028          
##                  95% CI : (0.8876, 0.9166)
##     No Information Rate : 0.8656          
##     P-Value [Acc &gt; NIR] : 2.087e-06       
##                                           
##                   Kappa : 0.4828          
##                                           
##  Mcnemar&#39;s Test P-Value : 2.100e-15       
##                                           
##             Sensitivity : 0.41071         
##             Specificity : 0.97921         
##          Pos Pred Value : 0.75410         
##          Neg Pred Value : 0.91456         
##              Prevalence : 0.13437         
##          Detection Rate : 0.05519         
##    Detection Prevalence : 0.07319         
##       Balanced Accuracy : 0.69496         
##                                           
##        &#39;Positive&#39; Class : yes             
## </code></pre>
<pre class="r"><code>ggplot(svmTune) + theme(legend.position = &quot;top&quot;)</code></pre>
<p><img src="/courses/MachineLearning/ML-with-Caret_files/figure-html/churn%20svm%20and%20fda%20model%20roc-1.png" width="672" /></p>
<pre class="r"><code>rocCurve&lt;- roc(response = churnTest$churn,
               predictor = svmProbs[, &quot;yes&quot;],
               levels = rev(levels(churnTest$churn)))</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<pre class="r"><code>plot(rocCurve,
     print.thres = c(.5,.2),
     print.thres.pch = 16,
     print.thres.cex = 1.2)</code></pre>
<p><img src="/courses/MachineLearning/ML-with-Caret_files/figure-html/churn%20svm%20and%20fda%20model%20roc-2.png" width="672" /></p>
<pre class="r"><code>fdaPred&lt;- predict(fdaTune, churnTest)

fdaProbs&lt;- predict(fdaTune, churnTest, type = &quot;prob&quot;)

confusionMatrix(fdaPred, churnTest$churn)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  yes   no
##        yes  106   75
##        no   118 1368
##                                           
##                Accuracy : 0.8842          
##                  95% CI : (0.8679, 0.8992)
##     No Information Rate : 0.8656          
##     P-Value [Acc &gt; NIR] : 0.013006        
##                                           
##                   Kappa : 0.4584          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.002501        
##                                           
##             Sensitivity : 0.47321         
##             Specificity : 0.94802         
##          Pos Pred Value : 0.58564         
##          Neg Pred Value : 0.92059         
##              Prevalence : 0.13437         
##          Detection Rate : 0.06359         
##    Detection Prevalence : 0.10858         
##       Balanced Accuracy : 0.71062         
##                                           
##        &#39;Positive&#39; Class : yes             
## </code></pre>
<pre class="r"><code>ggplot(fdaTune) + theme(legend.position = &quot;top&quot;)</code></pre>
<p><img src="/courses/MachineLearning/ML-with-Caret_files/figure-html/churn%20svm%20and%20fda%20model%20roc-3.png" width="672" /></p>
<pre class="r"><code>rocCurve&lt;- roc(response = churnTest$churn,
               predictor = fdaProbs[, &quot;yes&quot;],
               levels = rev(levels(churnTest$churn)))</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<pre class="r"><code>plot(rocCurve,
     print.thres = c(.5,.2),
     print.thres.pch = 16,
     print.thres.cex = 1.2)</code></pre>
<p><img src="/courses/MachineLearning/ML-with-Caret_files/figure-html/churn%20svm%20and%20fda%20model%20roc-4.png" width="672" /></p>
</div>
<div id="glmnet" class="section level2">
<h2>GLMnet</h2>
<p>Recall the glmnet, which is almost always the first model to try on new datasets. It fits quickly and provides linear regression components while ignoring noisy coefficients.</p>
<pre class="r"><code>grid&lt;- expand.grid(alpha = 0:1,
                   lambda = 0:10 / 10)

glmnetTune &lt;- train(churn ~ ., data = churnTrain,
  metric = &quot;ROC&quot;,
  method = &quot;glmnet&quot;,
  tuneGrid = grid,
  trControl = ctrl)

plot(glmnetTune)</code></pre>
<p><img src="/courses/MachineLearning/ML-with-Caret_files/figure-html/churn%20glmnet-1.png" width="672" /></p>
</div>
<div id="random-forest" class="section level2">
<h2>Random Forest</h2>
<p>A random forest may be a good practice to try as second model, as it is easier to tune, but it is slower</p>
<pre class="r"><code>grid&lt;- expand.grid(mtry = seq(25,45, by = 5),
                   splitrule = &quot;extratrees&quot;,
                   min.node.size = c(1, 5, 20))

rfTune&lt;- train(churn ~ ., data = churnTrain,
  metric = &quot;ROC&quot;,
  method = &quot;ranger&quot;,
  trControl = ctrl,
  tuneGrid = grid
)
rfTune</code></pre>
<pre><code>## Random Forest 
## 
## 3333 samples
##   19 predictor
##    2 classes: &#39;yes&#39;, &#39;no&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 3000, 3000, 2999, 3000, 3000, 3000, ... 
## Resampling results across tuning parameters:
## 
##   mtry  min.node.size  ROC        Sens       Spec     
##   25     1             0.9182534  0.6861224  0.9920702
##   25     5             0.9177765  0.6881973  0.9920702
##   25    20             0.9169999  0.6596939  0.9924912
##   30     1             0.9178059  0.7167347  0.9912281
##   30     5             0.9171996  0.7122364  0.9906667
##   30    20             0.9163033  0.6931633  0.9909474
##   35     1             0.9166804  0.7307823  0.9904561
##   35     5             0.9170929  0.7266582  0.9904561
##   35    20             0.9158549  0.7154932  0.9898947
##   40     1             0.9158828  0.7374150  0.9898947
##   40     5             0.9160785  0.7361735  0.9906667
##   40    20             0.9161064  0.7241752  0.9897544
##   45     1             0.9152543  0.7423639  0.9899649
##   45     5             0.9155461  0.7436139  0.9901754
##   45    20             0.9149522  0.7303827  0.9893333
## 
## Tuning parameter &#39;splitrule&#39; was held constant at a value of extratrees
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were mtry = 25, splitrule = extratrees
##  and min.node.size = 1.</code></pre>
<pre class="r"><code>plot(rfTune)</code></pre>
<p><img src="/courses/MachineLearning/ML-with-Caret_files/figure-html/churn%20rf-1.png" width="672" /></p>
<p>Often when comparing models, the <strong>resamples</strong> function is your friend.</p>
<pre class="r"><code>model_list&lt;- list(
  gbm = gbmTune,
  svm = svmTune,
  fda = fdaTune,
  glmnet = glmnetTune,
  rf = rfTune
)

resamps &lt;- resamples(model_list)
resamps</code></pre>
<pre><code>## 
## Call:
## resamples.default(x = model_list)
## 
## Models: gbm, svm, fda, glmnet, rf 
## Number of resamples: 50 
## Performance metrics: ROC, Sens, Spec 
## Time estimates for: everything, final model fit</code></pre>
<pre class="r"><code>summary(resamps)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = resamps)
## 
## Models: gbm, svm, fda, glmnet, rf 
## Number of resamples: 50 
## 
## ROC 
##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## gbm    0.7985380 0.8804642 0.9107851 0.9051387 0.9361477 0.9621345    0
## svm    0.8107456 0.8536699 0.8747076 0.8743018 0.8967083 0.9377730    0
## fda    0.7910491 0.8471939 0.8724781 0.8714801 0.8971838 0.9414966    0
## glmnet 0.7196637 0.7930328 0.8098371 0.8089440 0.8348516 0.8665951    0
## rf     0.8645102 0.9032447 0.9204854 0.9182534 0.9414931 0.9605629    0
## 
## Sens 
##             Min.    1st Qu.     Median       Mean    3rd Qu.      Max. NA&#39;s
## gbm    0.5416667 0.61224490 0.65986395 0.65340986 0.68750000 0.7755102    0
## svm    0.2448980 0.35416667 0.40199830 0.40664966 0.45833333 0.5625000    0
## fda    0.2708333 0.46938776 0.48979592 0.49397109 0.52816752 0.6938776    0
## glmnet 0.0000000 0.04081633 0.04166667 0.05634354 0.08290816 0.1458333    0
## rf     0.5625000 0.63265306 0.69068878 0.68612245 0.72916667 0.8367347    0
## 
## Spec 
##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## gbm    0.9578947 0.9754386 0.9807018 0.9807018 0.9885965 0.9964912    0
## svm    0.9543860 0.9649123 0.9754386 0.9743860 0.9824561 1.0000000    0
## fda    0.9192982 0.9333333 0.9473684 0.9453333 0.9543860 0.9789474    0
## glmnet 0.9894737 0.9964912 0.9964912 0.9973333 1.0000000 1.0000000    0
## rf     0.9789474 0.9894737 0.9929825 0.9920702 0.9964912 1.0000000    0</code></pre>
<pre class="r"><code># densityplot(resamps, metric = &quot;ROC&quot;) 

bwplot(resamps, metric = &quot;ROC&quot;) </code></pre>
<p><img src="/courses/MachineLearning/ML-with-Caret_files/figure-html/model%20comparisons-1.png" width="672" /></p>
<pre class="r"><code># xyplot(resamps, metric = &quot;ROC&quot;) </code></pre>
</div>
</div>
