---
author: "Jim Gruman"
output: html_document
date: "2019-12-10T00:00:00+01:00"
draft: false
linktitle: Tidyverse ML
menu:
  example:
    parent: Machine Learning in R
    weight: 15
title: Machine Learning in R
type: docs
weight: 15
---

## Machine Learning in the Tidyverse

The general approach for functional programming many models:

| Step1:  Make a list column, using | Step2:  Work with list columns | Step3:  Simplify the list columns |
|:----------------------------------|:-------------------------------|:----------------------------------|
| initial_split()                   | map()                          | unnest()                          |
| vfold_cv()                        | training()                     | map_dbl()                         |
| crossing()                        | testing()                      |                                   |
|                                   | lm()                           |                                   |
|                                   | ranger()                       |                                   |
|                                   | mae()                          |                                   |


```{r}
library(tidyverse)
library(tidyr)
library(broom)
library(purrr)
library(dslabs)
library(rsample)
library(ModelMetrics)
library(ranger)
library(viridis)
```

Load the gapminder dataset from the <span style="color:blue">dslabs</span> package

```{r}
data(gapminder)
###########   later, use continent and region to impute averages
gapminder <- gapminder %>%
  filter(!is.na(gdp), !is.na(infant_mortality)) %>%
  select(-continent, -region)
```

Nest the dataframe into a tibble

```{r}
gap_nested<- gapminder %>%
     group_by(country)%>%
     nest()
```

Create linear models for each country

```{r}
gap_models<-gap_nested %>%
    mutate(model = map(.x=data, .f = ~lm(formula = life_expectancy~year, data = .x)))
```

Illustrate the linear coefficients for each country

```{r}
gap_models %>%
  mutate(coef = map(model, ~tidy(.x))) %>%
  unnest(coef) %>%
  filter(term == "year")%>%
  ggplot()+geom_histogram(aes(estimate))+
  labs(title = "Linear Coefficients of Life Expectancy by Year",
       subtitle = "Histogram for all Gapminder Countries",
       caption = "Plot: Jim Gruman. Data: dslabs package")+
  theme_minimal()
  
```

Illustrate the R squared model fit for each country

The best models

```{r}
gap_models %>%
  mutate(coef =  map(model, ~glance(.x))) %>%
  unnest(coef) %>%
  ungroup()%>%
  top_n(n= 6, wt = r.squared)
```


The worst models

```{r}
gap_models %>%
  mutate(coef =  map(model, ~glance(.x))) %>%
  unnest(coef) %>%
  ungroup()%>%
  top_n(n= 6, wt = -r.squared)
```

Compare the predicted with the original value, in Italy

```{r}
gap_models %>% filter(country == "Italy")%>%
  mutate(augmented = map(model, ~augment(.x))) %>%
  unnest(augmented) %>%
  ggplot(aes(x=year, y=life_expectancy))+
  geom_point()+
  geom_line(aes(y=.fitted), color = "red")+
  labs(title = "Gapminder Life Expectancy Trend",
       subtitle = "Italy actuals and Fitted Linear Model",
       caption = "Plot: Jim Gruman. Data: dslabs package")+
  theme_minimal()
```

This is the predicted with the original value, in Botswana

```{r}
gap_models %>% filter(country == "Botswana")%>%
  mutate(augmented = map(model, ~augment(.x))) %>%
  unnest(augmented) %>%
  ggplot(aes(x=year, y=life_expectancy))+
  geom_point()+
  geom_line(aes(y=.fitted), color = "red")+
  labs(title = "Gapminder Life Expectancy Trend",
       subtitle = "Botswana actuals and Fitted Linear Model on Year only",
       caption = "Plot: Jim Gruman. Data: dslabs package")+
  theme_minimal()
```

Explore more complex regression models. In this case, with three features:

```{r}
gap_models<-gap_nested %>%
  mutate(model = map(.x=data, .f = ~lm(formula = life_expectancy~year+fertility+population, data = .x, na.action=na.exclude)))
```

The worst models, updated with the three feature linear model:

```{r}
gap_models %>%
  mutate(coef =  map(model, ~glance(.x))) %>%
  unnest(coef) %>%
  ungroup()%>%
  top_n(n= 6, wt = -r.squared)
```

Plot of the predicted fitted values with the original value, in Botswana, on the three feature model:

```{r}
gap_models %>% filter(country == "Botswana")%>%
  mutate(augmented = map(model, ~augment(.x))) %>%
  unnest(augmented) %>%
  ggplot(aes(x=year, y=life_expectancy))+
  geom_point()+
  geom_line(aes(y=.fitted), color = "red")+
  labs(title = "Gapminder Life Expectancy Trend",
       subtitle = "Botswana actuals and Fitted Linear Model on three features",
       caption = "Plot: Jim Gruman. Data: dslabs package")+
  theme_minimal()
```

## How will my model perform on new data?   

Did I select best performing model?
  
Always make a test/train split

Prepare the initial split object

```{r}
gap_split <- initial_split(gapminder, prop = .75)
```

Extract the training dataframe

```{r}
training_data <- training(gap_split)
```

Extract the testing dataframe

```{r}
testing_data <- testing(gap_split)
```

Calculate the dimensions of both training_data and testing_data to confirm data set sizes

```{r}
dim(training_data)
dim(testing_data)
```

Prepare the dataframe containing the cross validation partitions

```{r}
cv_split <- vfold_cv(training_data, v = 5)

cv_data <- cv_split %>% 
  mutate(
    # Extract the train dataframe for each split
    train = map(splits, ~training(.x)), 
    # Extract the validate dataframe for each split
    validate = map(splits, ~testing(.x))
  )
```

Build linear models against all available features

```{r}
cv_models_lm <- cv_data %>% 
  mutate(model = map(train, ~lm(formula = life_expectancy~., data = .x)))
```

Extract the actual life expectancy, for comparison with the fitted models

```{r}
cv_prep_lm <- cv_models_lm %>% 
  mutate(
    # Extract the recorded life expectancy for the records in the validate dataframes
    validate_actual = map(validate, ~.x$life_expectancy),
    # Predict life expectancy for each validate set using its corresponding model
    validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y))
  )
```

Calculate the mean absolute error for each validate fold       

```{r}
cv_eval_lm <- cv_prep_lm %>% 
  mutate(validate_mae = map2_dbl(validate_actual, validate_predicted, ~mae(actual = .x, predicted = .y)))
```

Print the validate_mae column

```{r}
cv_eval_lm$validate_mae
```

Calculate the mean of validate_mae column

```{r}
mean(cv_eval_lm$validate_mae)
```

Build a random forest model for each fold

```{r}
cv_models_rf <- cv_data %>% 
  mutate(model = map(train, ~ranger(formula = life_expectancy~., data = .x, num.trees = 100, seed = 42)))
```

Generate predictions using the random forest model

```{R}
cv_prep_rf <- cv_models_rf %>% 
  mutate(validate_predicted = map2(.x = model , .y = validate, ~predict(.x, .y)$predictions),
         validate_actual = map(validate, ~.x$life_expectancy))
```

Calculate validate MAE for each fold

```{r}
cv_eval_rf <- cv_prep_rf %>% 
  mutate(validate_mae = map2_dbl(validate_actual, validate_predicted, ~mae(actual = .x, predicted = .y)))
```

Print the validate_mae column

```{r}
cv_eval_rf$validate_mae
```

Calculate the mean of validate_mae column

```{r}
mean(cv_eval_rf$validate_mae)
```

Prepare for tuning your cross validation folds by varying mtry

```{r}
cv_tune<- tidyr::expand_grid(cv_data, mtry = 2:5)

# if this fails, try tidyr::expand_grid in place of crossing()
```

Build a model for each fold & mtry combination

```{r}
cv_model_tunerf <- cv_tune %>% 
  mutate(model = map2(.x = train, .y = mtry, ~ranger(formula = life_expectancy~., 
                                                     data = .x, mtry = .y, 
                                                     num.trees = 100, seed = 42)))
```

Generate validate predictions for each model

```{r}
cv_prep_tunerf <- cv_model_tunerf %>% 
  mutate(validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y)$predictions),
         validate_actual = map(validate, ~.x$life_expectancy))
```

Calculate validate MAE for each fold and mtry combination

```{r}
cv_eval_tunerf <- cv_prep_tunerf %>% 
  mutate(validate_mae = map2_dbl(.x = validate_actual, .y = validate_predicted, ~mae(actual = .x, predicted = .y)))
```

Calculate the mean validate_mae for each mtry used  

```{r}
cv_eval_tunerf %>% 
  group_by(mtry) %>% 
  summarise(mean_mae = mean(validate_mae))
```

The best random forest model has the lowest MAE:  mtry of 2 for 0.954


Build the model using all training data and the best performing parameter

```{r}
best_model <- ranger(formula = life_expectancy~., data = training_data,
                     mtry = 2, num.trees = 100, seed = 42)
```

Prepare the test_actual vector

```{r}
test_actual <- testing_data$life_expectancy
```

Predict life_expectancy for the testing_data

```{r}
test_predicted <- predict(best_model, testing_data)$predictions
```

Calculate the test MAE

```{r}
mae(test_actual, test_predicted)
```

###################################################

# Binary classification models

These are very common models   -  in this case, using simple logistic regression

Changing to now use the attrition dataset. These data are from the IBM Watson Analytics Lab. The website describes the data with “Uncover the factors that lead to employee attrition and explore important questions such as ‘show me a breakdown of distance from home by job role and attrition’ or ‘compare average monthly income by education and attrition’. This is a fictional data set created by IBM data scientists.”

```{r}
data(attrition)
```

Prepare the initial split object

```{r}
data_split <- initial_split(attrition, prop = .75)
```

Extract the training dataframe

```{r}
training_data <- training(data_split)
```

Extract the testing dataframe

```{r}
testing_data <- testing(data_split)
```

Prepare the dataframe containing the cross validation partitions

```{r}
cv_split <- vfold_cv(training_data, v = 5)

cv_data <- cv_split %>% 
  mutate(
    # Extract the train dataframe for each split
    train = map(splits, ~training(.x)),
    # Extract the validate dataframe for each split
    validate = map(splits, ~testing(.x))
  )
```

Build a model using the train data for each fold of the cross validation

```{r}
cv_models_lr <- cv_data %>% 
  mutate(model = map(train, ~glm(formula = Attrition~., 
                                 data = .x, family = "binomial")))
```


Extract the first model and validate 

```{r}
model <- cv_models_lr$model[[1]]
validate <- cv_models_lr$validate[[1]]
```

Prepare binary vector of actual Attrition values in validate
```{r}
validate_actual <- validate$Attrition == "Yes"
```

Predict the probabilities for the observations in validate

```{r}
validate_prob <- predict(model, validate, type = "response")
```

Prepare binary vector of predicted Attrition values for validate

```{r}
validate_predicted <- validate_prob > 0.5
```

Compare the actual & predicted performance visually using a table

```{r}
table(validate_actual, validate_predicted)
```

Calculate the accuracy    

```{r}
1-ce(validate_actual, validate_predicted)
```

Calculate the precision

```{r}
precision(validate_actual, validate_predicted)
```

Calculate the recall

```{r}
recall(validate_actual, validate_predicted)
```

Prepare for cross-validated performance

```{r}
cv_prep_lr <- cv_models_lr %>% 
  mutate(
    # Prepare binary vector of actual Attrition values in validate
    validate_actual = map(validate, ~.x$Attrition == "Yes"),
    # Prepare binary vector of predicted Attrition values for validate
    validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response") > 0.5)
  )
```

Calculate the validate recall for each cross validation fold
```{r}
cv_perf_recall <- cv_prep_lr %>% 
  mutate(validate_recall = map2_dbl(validate_actual, validate_predicted, 
                                    ~recall(actual = .x, predicted = .y)))
```

Print the validate_recall column

```{r}
cv_perf_recall$validate_recall
```

Calculate the average of the validate_recall column

```{r}
mean(cv_perf_recall$validate_recall)
```

##########################################

#With a random forest for classification

Prepare for tuning your cross validation folds by varying mtry

```{r}
cv_tune <- cv_data %>%
  crossing(mtry = 2:16) 
```

Build a cross validation model for each fold & mtry combination

```{r}
cv_models_rf <- cv_tune %>% 
  mutate(model = map2(train, mtry, ~ranger(formula = Attrition~., 
                                           data = .x, mtry = .y,
                                        num.trees = 100, seed = 42)))
```

```{r}
cv_prep_rf <- cv_models_rf %>% 
  mutate(
    # Prepare binary vector of actual Attrition values in validate
    validate_actual = map(validate, ~.x$Attrition == "Yes"),
    # Prepare binary vector of predicted Attrition values for validate
    validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y, type = "response")$predictions == "Yes")
  )
```

Calculate the validate recall for each cross validation fold

```{r}
cv_perf_recall <- cv_prep_rf %>% 
  mutate(recall = map2_dbl(.x = validate_actual, .y = validate_predicted, ~recall(actual = .x, predicted = .y)))
```

Calculate the mean recall for each mtry used  

```{r}
cv_perf_recall %>% 
  group_by(mtry) %>% 
  summarise(mean_recall = mean(recall))
```

none of the random forest models here outperforms the recall of the logistic regression models

Build out the final production model

Build the logistic regression model using all training data
```{r}
best_model <- glm(formula = Attrition~., 
                  data = training_data, family = "binomial")
```

Prepare binary vector of actual Attrition values for testing_data

```{r}
test_actual <- testing_data$Attrition == "Yes"
```

Prepare binary vector of predicted Attrition values for testing_data

```{r}
test_predicted <- predict(best_model, testing_data, type = "response") > 0.5
```

Compare the actual & predicted performance visually using a table

```{r}
table(test_actual, test_predicted)
```

Calculate the test accuracy

```{r}
1-ce(test_actual, test_predicted)
```

Calculate the test precision

```{r}
precision(test_actual, test_predicted)
```

Calculate the test recall

```{r}
recall(test_actual, test_predicted)
```