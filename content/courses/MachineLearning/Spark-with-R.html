---
author: "Jim Gruman"
output: html_document
date: "12/31/2019"
draft: false
linktitle: Spark with R
menu:
  example:
    parent: Machine Learning in R
    weight: 17
title: Spark with R
type: docs
weight: 17
---



<p>In a world where information is growing exponentially, tools like Apache Spark provide support to solve many of the relvant problems we face today. From companies looking for ways to improve based on data-driven decisions, to research organizations solving problems in health-care, finance, education, and energy, Spark enables analyzing much more information faster and more reliably than ever before.</p>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Javier Luraschi, Kevin Kuo, and Edgar Ruis have published a brand new book in 2020 on learning how to use Apache Spark with R. The book intends to take someone unfamiliar with Spark or R and help you become proficient by teaching you a set of tools, skills and practices applicable to large-scale data science.</p>
<p><a href="https://therinspark.com/"><img src="mastering-spark-with-r.png" alt="Mastering Spark with R" /></a></p>
<p><a href="https://github.com/r-spark/the-r-in-spark/">Mastering Spark with R Github</a></p>
<p><a href="https://github.com/r-spark/the-r-in-spark/releases">Mastering Spark with R Code Examples</a></p>
<p>Buy the book here:</p>
<p><a href="https://www.amazon.com/gp/product/149204637X/">Mastering Spark with R</a></p>
<p>Another excellent online resource for learning Apache Spark with R is</p>
<p><a href="https://spark.rstudio.com/"><img src="img/sparklyr-v2.png" alt="sparklyr: R interface for Apache Spark" /></a></p>
<p>Be aware that the Rstudio content is not intended to be read from start to finish and assumes that you, the reader, have some knowledge of Apache Spark, R, and cluster computing.</p>
<p>Richie Cotton of DataCamp also offers an overview course:</p>
<p>[<img src="shield_image_course_3309_20180726-12-1ek0ctx.png" alt="Introduction to Spark in R using sparklyr" />]
(<a href="https://www.datacamp.com/courses/introduction-to-spark-in-r-using-sparklyr" class="uri">https://www.datacamp.com/courses/introduction-to-spark-in-r-using-sparklyr</a>)</p>
<p>There are many additional resources that can help you to troubleshoot particular issues while getting started and, in general, introduce you to the broader Spark and R communities to help you get specific answers, discuss topics, and connect with many users actively using Spark with R. For general sparklyr questions, you can post in the <a href="http://bit.ly/2PfNqzN">RStudio Community</a> tagged as sparklyr. For general Spark questions, <a href="http://bit.ly/2TEfU4L">Stack Overflow</a> is a great resource; there are also many topics specifically about sparklyr. If you believe something needs to be fixed, open a <a href="http://bit.ly/30b5NGT">GitHub</a> issue or send a pull request.</p>
<hr />
<p>To develop a definition of Apache Spark, we break it down as follows:</p>
<p><strong>Unified</strong></p>
<blockquote>
<p>Spark supports many libraries, cluster technologies, and storage systems.</p>
</blockquote>
<p><strong>Analytics</strong></p>
<blockquote>
<p>Analytics is the discovery and interpretation of data to produce and communicate information.</p>
</blockquote>
<p><strong>Engine</strong></p>
<blockquote>
<p>Spark is expected to be efficient and generic.</p>
</blockquote>
<p><strong>Large-Scale</strong></p>
<blockquote>
<p>You can interpret large-scale as cluster-scale, a set of connected computers working together.</p>
</blockquote>
<hr />
<div id="audience" class="section level2">
<h2>Audience</h2>
<p>When thinking of who should use sparklyr, the following roles come to mind:</p>
<p><strong>New Users</strong></p>
<p>For new users, it is our belief that sparklyr provides the easiest way to get started with Spark. Our hope is that the early chapters of this book will get you up and running with ease and set you up for long-term success.</p>
<p><strong>Data Scientists</strong></p>
<p>For data scientists who already use and love R, sparklyr integrates with many other R practices and packages like dplyr, magrittr, broom, DBI, tibble, rlang, and many others, which will make you feel at home while working with Spark. For those new to R and Spark, the combination of high-level workflows available in sparklyr and low-level extensibility mechanisms make it a productive environment to match the needs and skills of every data scientist.</p>
<p><strong>Expert Users</strong></p>
<p>For those users who are already immersed in Spark and can write code natively in Scala, consider making your Spark libraries available as an R package to the R community, a diverse and skilled community that can put your contributions to good use while moving open science forward.</p>
<p>sparklyr is the R package that brings together these communities, expectations, future directions, packages, and package extensions.</p>
<hr />
<p>Tip: When using Windows, avoid directories with spaces in their path. If running getwd() from R returns a path with spaces, consider switching to a path with no spaces using setwd(“path”) or by creating an RStudio project in a path with no spaces.</p>
<p>Additionally, because Spark is built in the Scala programming language, which is run by the Java Virtual Machine (JVM), you also need to install Java 8 on your system. It is likely that your system already has Java installed, but you should still check the version and update or downgrade as described in Installing Java. You can use the following R command to check which version is installed on your system:</p>
<pre class="r"><code>getwd()

system(&quot;java -version&quot;)</code></pre>
<p>[1] “C:/Users/jimgr/Documents/R/pubrepo/content/courses/MachineLearning”
java version “1.8.0_231”
Java(TM) SE Runtime Environment (build 1.8.0_231-b11)
Java HotSpot(TM) Client VM (build 25.231-b11, mixed mode, sharing)
[1] 0</p>
</div>
<div id="installation" class="section level2">
<h2>Installation</h2>
<p>As with many other R packages, you can install sparklyr from CRAN as follows:</p>
<pre class="r"><code>install.packages(&quot;sparklyr&quot;)

packageVersion(&quot;sparklyr&quot;)</code></pre>
<p>Start by loading sparklyr:</p>
<pre class="r"><code>library(sparklyr)</code></pre>
<p>You can easily install Spark by running spark_install(). This downloads, installs, and configures the latest version of Spark locally on your computer.</p>
<pre class="r"><code>spark_install()

spark_version()

spark_available_versions()</code></pre>
<p>Note: The default installation paths are ~/spark for macOS and Linux, and %LOCALAPPDATA%/spark for Windows. To customize the installation path, you can run options(spark.install.dir = “installation-path”) before spark_install() and spark_connect()</p>
</div>
<div id="connecting" class="section level2">
<h2>Connecting</h2>
<pre class="r"><code>sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.4&quot;)</code></pre>
</div>
<div id="using-spark" class="section level2">
<h2>Using Spark</h2>
<p>Now that you are connected, we can run a few simple commands. For instance, let’s start by copying the mtcars dataset into Apache Spark by using copy_to():</p>
<pre class="r"><code>cars &lt;- copy_to(sc, mtcars)</code></pre>
<p>The data was copied into Spark, but we can access it from R using the cars reference. To print its contents, we can simply type <em>cars</em>:</p>
<pre class="r"><code>cars</code></pre>
<pre><code>## # Source: spark&lt;mtcars&gt; [?? x 11]
##      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4
##  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4
##  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1
##  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1
##  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2
##  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1
##  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4
##  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2
##  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2
## 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4
## # ... with more rows</code></pre>
</div>
<div id="web-interface" class="section level2">
<h2>Web Interface</h2>
<p>Most of the Spark commands are executed from the R console; however, monitoring and analyzing execution is done through Spark’s web interface. This interface is a web application provided by Spark that you can access by running:</p>
<pre class="r"><code>spark_web(sc)</code></pre>
</div>
<div id="analysis" class="section level2">
<h2>Analysis</h2>
<p>When using Spark from R to analyze data, you can use SQL (Structured Query Language) or dplyr (a grammar of data manipulation). You can use SQL through the DBI package; for instance, to count how many records are available in our cars dataset, we can run the following:</p>
<pre class="r"><code>library(DBI)
dbGetQuery(sc, &quot;SELECT count(*) FROM mtcars&quot;)</code></pre>
<pre><code>##   count(1)
## 1       32</code></pre>
<p>When using dplyr, you write less code, and it’s often much easier to write than SQL. This is precisely why we won’t make use of SQL in this book; however, if you are proficient in SQL, this is a viable option for you. For instance, counting records in dplyr is more compact and easier to understand:</p>
<pre class="r"><code>library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>count(cars)</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 1]
##       n
##   &lt;dbl&gt;
## 1    32</code></pre>
<p>In general, we usually start by analyzing data in Spark with dplyr, followed by sampling rows and selecting a subset of the available columns. The last step is to collect data from Spark to perform further data processing in R, like data visualization. Let’s perform a very simple data analysis example by selecting, sampling, and plotting the cars dataset in Spark:</p>
<pre class="r"><code>select(cars, hp, mpg) %&gt;%
  sample_n(100) %&gt;%
  collect() %&gt;%
  plot()</code></pre>
<p><img src="/courses/MachineLearning/Spark-with-R_files/figure-html/analysis%20with%20dplyr2-1.png" width="672" /></p>
</div>
<div id="modeling" class="section level2">
<h2>Modeling</h2>
<pre class="r"><code>model &lt;- ml_linear_regression(cars, mpg ~ hp)
model</code></pre>
<pre><code>## Formula: mpg ~ hp
## 
## Coefficients:
## (Intercept)          hp 
## 30.09886054 -0.06822828</code></pre>
<p>Now we can use this model to predict values that are not in the original dataset. For instance, we can add entries for cars with horsepower beyond 250 and also visualize the predicted values</p>
<pre class="r"><code>model %&gt;%
  ml_predict(copy_to(sc, data.frame(hp = 250 + 10 * 1:10))) %&gt;%
  transmute(hp = hp, mpg = prediction) %&gt;%
  full_join(select(cars, hp, mpg)) %&gt;%
  collect() %&gt;%
  plot()</code></pre>
<pre><code>## Joining, by = c(&quot;hp&quot;, &quot;mpg&quot;)</code></pre>
<p><img src="/courses/MachineLearning/Spark-with-R_files/figure-html/modeling2-1.png" width="672" /></p>
</div>
<div id="extensions" class="section level2">
<h2>Extensions</h2>
<p>In the same way that R is known for its vibrant community of package authors, at a smaller scale, many extensions for Spark and R have been written and are available. For instance, the sparkly.nested extension is an R package that extends sparklyr to help you manage values that contain nested information. A common use case involves JSON files that contain nested lists that require preprocessing before you can do meaningful data analysis. To use this extension, we first need to install it as follows:</p>
<pre class="r"><code>install.packages(&quot;sparklyr.nested&quot;)</code></pre>
<p>Then, we can use the sparklyr.nested extension to group all of the horsepower data points over the number of cylinders:</p>
<pre class="r"><code>library(sparklyr.nested)</code></pre>
<pre><code>## Warning: package &#39;sparklyr.nested&#39; was built under R version 3.6.2</code></pre>
<pre class="r"><code>sparklyr.nested::sdf_nest(cars, hp) %&gt;%
  group_by(cyl) %&gt;%
  summarise(data = collect_list(data))</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 2]
##     cyl data       
##   &lt;dbl&gt; &lt;list&gt;     
## 1     4 &lt;list [11]&gt;
## 2     8 &lt;list [14]&gt;
## 3     6 &lt;list [7]&gt;</code></pre>
</div>
<div id="streaming" class="section level2">
<h2>Streaming</h2>
<p>While processing large static datasets is the most typical use case for Spark, processing dynamic datasets in real time is also possible and, for some applications, a requirement. You can think of a streaming dataset as a static data source with new data arriving continuously, like stock market quotes. Streaming data is usually read from Kafka (an open source stream-processing software platform) or from distributed storage that receives new data continuously.</p>
<p>To try out streaming, let’s first create an input/ folder with some data that we will use as the input for this stream:</p>
<pre class="r"><code>dir.create(&quot;input&quot;)</code></pre>
<pre><code>## Warning in dir.create(&quot;input&quot;): &#39;input&#39; already exists</code></pre>
<pre class="r"><code>write.csv(mtcars, &quot;input/cars_1.csv&quot;, row.names = F)</code></pre>
<p>Then, we define a stream that processes incoming data from the input/ folder, performs a custom transformation in R, and pushes the output into an output/ folder:</p>
<pre class="r"><code>stream &lt;- stream_read_csv(sc, &quot;input/&quot;) %&gt;%
    select(mpg, cyl, disp) %&gt;%
    stream_write_csv(&quot;output/&quot;)</code></pre>
<p>As soon as the stream of real-time data starts, the input/ folder is processed and turned into a set of new files under the output/ folder containing the new transformed files. Since the input contained only one file, the output folder will also contain a single file resulting from applying the custom spark_apply() transformation.</p>
<pre class="r"><code>dir(&quot;output&quot;, pattern = &quot;.csv&quot;)</code></pre>
<pre><code>## [1] &quot;part-00000-c8f671aa-13ec-4174-94d2-45226dc3cd75-c000.csv&quot;
## [2] &quot;part-00000-c9675c62-ad31-413d-9838-24a87996c176-c000.csv&quot;</code></pre>
<p>Up to this point, this resembles static data processing; however, we can keep adding files to the input/ location, and Spark will parallelize and process data automatically. Let’s add one more file and validate that it’s automatically processed:</p>
<pre class="r"><code># Write more data into the stream source
write.csv(mtcars, &quot;input/cars_2.csv&quot;, row.names = F)</code></pre>
<p>Wait a few seconds and validate that the data is processed by the Spark stream:</p>
<pre class="r"><code># Check the contents of the stream destination
dir(&quot;output&quot;, pattern = &quot;.csv&quot;)</code></pre>
<pre><code>## [1] &quot;part-00000-c8f671aa-13ec-4174-94d2-45226dc3cd75-c000.csv&quot;
## [2] &quot;part-00000-c9675c62-ad31-413d-9838-24a87996c176-c000.csv&quot;</code></pre>
<p>You should then stop the stream:</p>
<pre class="r"><code>stream_stop(stream)</code></pre>
<p>You can use dplyr, SQL, Spark models, or distributed R to analyze streams in real time. In Chapter 12 we properly introduce you to all the interesting transformations you can perform to analyze real-time data.</p>
</div>
<div id="logs" class="section level2">
<h2>Logs</h2>
<p>Logging is definitely less interesting than real-time data processing; however, it’s a tool you should be or become familiar with. A log is just a text file to which Spark appends information relevant to the execution of tasks in the cluster. For local clusters, we can retrieve all the recent logs by running the following:</p>
<pre class="r"><code>spark_log(sc, filter = &quot;sparklyr&quot;)</code></pre>
<pre><code>## 20/01/02 13:41:34 INFO SparkContext: Submitted application: sparklyr
## 20/01/02 13:41:35 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:59006/jars/sparklyr-2.4-2.11.jar with timestamp 1577994095155
## 20/01/02 13:42:44 INFO SparkContext: Submitted application: sparklyr
## 20/01/02 13:42:44 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:59174/jars/sparklyr-2.4-2.11.jar with timestamp 1577994164649
## 20/01/02 13:42:47 INFO Executor: Fetching spark://127.0.0.1:59174/jars/sparklyr-2.4-2.11.jar with timestamp 1577994164649
## 20/01/02 13:42:47 INFO Utils: Fetching spark://127.0.0.1:59174/jars/sparklyr-2.4-2.11.jar to C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-2aa5fe5c-4391-41be-a12f-e977299e5f9e\userFiles-419835d9-e1d3-4161-b4a3-a4cfbe3b3d4d\fetchFileTemp5236050792410757813.tmp
## 20/01/02 13:42:47 INFO Executor: Adding file:/C:/Users/jimgr/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-2aa5fe5c-4391-41be-a12f-e977299e5f9e/userFiles-419835d9-e1d3-4161-b4a3-a4cfbe3b3d4d/sparklyr-2.4-2.11.jar to class loader
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-2aa5fe5c-4391-41be-a12f-e977299e5f9e\userFiles-419835d9-e1d3-4161-b4a3-a4cfbe3b3d4d\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-2aa5fe5c-4391-41be-a12f-e977299e5f9e\userFiles-419835d9-e1d3-4161-b4a3-a4cfbe3b3d4d\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-2aa5fe5c-4391-41be-a12f-e977299e5f9e\userFiles-419835d9-e1d3-4161-b4a3-a4cfbe3b3d4d\sparklyr-2.4-2.11.jar
## 20/01/02 13:43:06 INFO SparkContext: Submitted application: sparklyr
## 20/01/02 13:43:06 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:59300/jars/sparklyr-2.4-2.11.jar with timestamp 1577994186521
## 20/01/02 13:47:01 INFO SparkContext: Submitted application: sparklyr
## 20/01/02 13:47:01 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:59595/jars/sparklyr-2.4-2.11.jar with timestamp 1577994421726
## 20/01/02 13:47:04 INFO Executor: Fetching spark://127.0.0.1:59595/jars/sparklyr-2.4-2.11.jar with timestamp 1577994421726
## 20/01/02 13:47:04 INFO Utils: Fetching spark://127.0.0.1:59595/jars/sparklyr-2.4-2.11.jar to C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-84b3499f-35da-4011-8bdd-0a3f5167b341\userFiles-b3642d86-fcd5-4851-8f55-9ee9dcf14c85\fetchFileTemp4839239836642646819.tmp
## 20/01/02 13:47:05 INFO Executor: Adding file:/C:/Users/jimgr/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-84b3499f-35da-4011-8bdd-0a3f5167b341/userFiles-b3642d86-fcd5-4851-8f55-9ee9dcf14c85/sparklyr-2.4-2.11.jar to class loader
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-84b3499f-35da-4011-8bdd-0a3f5167b341\userFiles-b3642d86-fcd5-4851-8f55-9ee9dcf14c85\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-84b3499f-35da-4011-8bdd-0a3f5167b341\userFiles-b3642d86-fcd5-4851-8f55-9ee9dcf14c85\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-84b3499f-35da-4011-8bdd-0a3f5167b341\userFiles-b3642d86-fcd5-4851-8f55-9ee9dcf14c85\sparklyr-2.4-2.11.jar
## 20/01/02 13:47:19 INFO SparkContext: Submitted application: sparklyr
## 20/01/02 13:47:20 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:59713/jars/sparklyr-2.4-2.11.jar with timestamp 1577994440260
## 20/01/02 13:47:33 INFO SparkContext: Submitted application: sparklyr
## 20/01/02 13:47:33 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:59849/jars/sparklyr-2.4-2.11.jar with timestamp 1577994453718
## 20/01/02 13:47:36 INFO Executor: Fetching spark://127.0.0.1:59849/jars/sparklyr-2.4-2.11.jar with timestamp 1577994453718
## 20/01/02 13:47:36 INFO Utils: Fetching spark://127.0.0.1:59849/jars/sparklyr-2.4-2.11.jar to C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-9307471a-4d50-4668-bb21-b62fda21e67f\userFiles-2472c11c-808e-4b90-be78-d868b973ab65\fetchFileTemp4933013849357174334.tmp
## 20/01/02 13:47:37 INFO Executor: Adding file:/C:/Users/jimgr/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-9307471a-4d50-4668-bb21-b62fda21e67f/userFiles-2472c11c-808e-4b90-be78-d868b973ab65/sparklyr-2.4-2.11.jar to class loader
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-9307471a-4d50-4668-bb21-b62fda21e67f\userFiles-2472c11c-808e-4b90-be78-d868b973ab65\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-9307471a-4d50-4668-bb21-b62fda21e67f\userFiles-2472c11c-808e-4b90-be78-d868b973ab65\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-9307471a-4d50-4668-bb21-b62fda21e67f\userFiles-2472c11c-808e-4b90-be78-d868b973ab65\sparklyr-2.4-2.11.jar
## 20/01/02 13:47:51 INFO SparkContext: Submitted application: sparklyr
## 20/01/02 13:47:51 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:59983/jars/sparklyr-2.4-2.11.jar with timestamp 1577994471843
## 20/01/02 13:48:24 INFO SparkContext: Submitted application: sparklyr
## 20/01/02 13:48:24 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:60143/jars/sparklyr-2.4-2.11.jar with timestamp 1577994504463
## 20/01/02 13:48:27 INFO Executor: Fetching spark://127.0.0.1:60143/jars/sparklyr-2.4-2.11.jar with timestamp 1577994504463
## 20/01/02 13:48:27 INFO Utils: Fetching spark://127.0.0.1:60143/jars/sparklyr-2.4-2.11.jar to C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-e1c2413a-e349-4d9a-8481-4c7c7c957f51\userFiles-45014c8b-fbb0-4a44-b548-186a4d5586d7\fetchFileTemp1774485821301050512.tmp
## 20/01/02 13:48:27 INFO Executor: Adding file:/C:/Users/jimgr/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-e1c2413a-e349-4d9a-8481-4c7c7c957f51/userFiles-45014c8b-fbb0-4a44-b548-186a4d5586d7/sparklyr-2.4-2.11.jar to class loader
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-e1c2413a-e349-4d9a-8481-4c7c7c957f51\userFiles-45014c8b-fbb0-4a44-b548-186a4d5586d7\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-e1c2413a-e349-4d9a-8481-4c7c7c957f51\userFiles-45014c8b-fbb0-4a44-b548-186a4d5586d7\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-e1c2413a-e349-4d9a-8481-4c7c7c957f51\userFiles-45014c8b-fbb0-4a44-b548-186a4d5586d7\sparklyr-2.4-2.11.jar
## 20/01/02 13:48:43 INFO SparkContext: Submitted application: sparklyr
## 20/01/02 13:48:43 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:60312/jars/sparklyr-2.4-2.11.jar with timestamp 1577994523932
## 20/01/02 13:51:47 INFO SparkContext: Submitted application: sparklyr
## 20/01/02 13:51:47 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:60560/jars/sparklyr-2.4-2.11.jar with timestamp 1577994707995
## 20/01/02 13:51:50 INFO Executor: Fetching spark://127.0.0.1:60560/jars/sparklyr-2.4-2.11.jar with timestamp 1577994707995
## 20/01/02 13:51:50 INFO Utils: Fetching spark://127.0.0.1:60560/jars/sparklyr-2.4-2.11.jar to C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-4c25f5fb-99ce-493c-be13-ba6df9c72970\userFiles-b3311c6d-03f7-4354-b63d-955c56258b37\fetchFileTemp3023125751688504982.tmp
## 20/01/02 13:51:51 INFO Executor: Adding file:/C:/Users/jimgr/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-4c25f5fb-99ce-493c-be13-ba6df9c72970/userFiles-b3311c6d-03f7-4354-b63d-955c56258b37/sparklyr-2.4-2.11.jar to class loader
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-4c25f5fb-99ce-493c-be13-ba6df9c72970\userFiles-b3311c6d-03f7-4354-b63d-955c56258b37\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-4c25f5fb-99ce-493c-be13-ba6df9c72970\userFiles-b3311c6d-03f7-4354-b63d-955c56258b37\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-4c25f5fb-99ce-493c-be13-ba6df9c72970\userFiles-b3311c6d-03f7-4354-b63d-955c56258b37\sparklyr-2.4-2.11.jar
## 20/01/02 13:52:05 INFO SparkContext: Submitted application: sparklyr
## 20/01/02 13:52:06 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:60684/jars/sparklyr-2.4-2.11.jar with timestamp 1577994726164
## 20/01/02 13:52:12 INFO Executor: Fetching spark://127.0.0.1:60684/jars/sparklyr-2.4-2.11.jar with timestamp 1577994726164
## 20/01/02 13:52:12 INFO Utils: Fetching spark://127.0.0.1:60684/jars/sparklyr-2.4-2.11.jar to C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-13ae77ec-58c5-4e8d-9b43-2b06e0941494\userFiles-3fc8b98a-e387-4128-94b8-416032c80909\fetchFileTemp2563300702458924396.tmp
## 20/01/02 13:52:12 INFO Executor: Adding file:/C:/Users/jimgr/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-13ae77ec-58c5-4e8d-9b43-2b06e0941494/userFiles-3fc8b98a-e387-4128-94b8-416032c80909/sparklyr-2.4-2.11.jar to class loader
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-13ae77ec-58c5-4e8d-9b43-2b06e0941494\userFiles-3fc8b98a-e387-4128-94b8-416032c80909\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-13ae77ec-58c5-4e8d-9b43-2b06e0941494\userFiles-3fc8b98a-e387-4128-94b8-416032c80909\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-13ae77ec-58c5-4e8d-9b43-2b06e0941494\userFiles-3fc8b98a-e387-4128-94b8-416032c80909\sparklyr-2.4-2.11.jar
## 20/01/02 13:54:12 INFO SparkContext: Submitted application: sparklyr
## 20/01/02 13:54:13 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:61043/jars/sparklyr-2.4-2.11.jar with timestamp 1577994853113
## 20/01/02 13:54:38 INFO Executor: Fetching spark://127.0.0.1:61043/jars/sparklyr-2.4-2.11.jar with timestamp 1577994853113
## 20/01/02 13:54:38 INFO Utils: Fetching spark://127.0.0.1:61043/jars/sparklyr-2.4-2.11.jar to C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-aa575115-2847-4d68-abe5-15f48f61bf99\userFiles-729a3869-ff7b-4fc5-a1b1-3a1646a6ec22\fetchFileTemp342951406623107309.tmp
## 20/01/02 13:54:38 INFO Executor: Adding file:/C:/Users/jimgr/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-aa575115-2847-4d68-abe5-15f48f61bf99/userFiles-729a3869-ff7b-4fc5-a1b1-3a1646a6ec22/sparklyr-2.4-2.11.jar to class loader
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-aa575115-2847-4d68-abe5-15f48f61bf99\userFiles-729a3869-ff7b-4fc5-a1b1-3a1646a6ec22\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-aa575115-2847-4d68-abe5-15f48f61bf99\userFiles-729a3869-ff7b-4fc5-a1b1-3a1646a6ec22\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-aa575115-2847-4d68-abe5-15f48f61bf99\userFiles-729a3869-ff7b-4fc5-a1b1-3a1646a6ec22\sparklyr-2.4-2.11.jar
## 20/01/02 16:04:11 INFO SparkContext: Submitted application: sparklyr
## 20/01/02 16:04:11 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:53349/jars/sparklyr-2.4-2.11.jar with timestamp 1578002651491
## 20/01/02 16:04:15 INFO Executor: Fetching spark://127.0.0.1:53349/jars/sparklyr-2.4-2.11.jar with timestamp 1578002651491
## 20/01/02 16:04:15 INFO Utils: Fetching spark://127.0.0.1:53349/jars/sparklyr-2.4-2.11.jar to C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-b1d5947d-1e8d-4113-9b81-da75cba55021\userFiles-9ad894da-e841-4a2b-82fd-517d6ebbb5af\fetchFileTemp1257753856413346821.tmp
## 20/01/02 16:04:15 INFO Executor: Adding file:/C:/Users/jimgr/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-b1d5947d-1e8d-4113-9b81-da75cba55021/userFiles-9ad894da-e841-4a2b-82fd-517d6ebbb5af/sparklyr-2.4-2.11.jar to class loader
## 20/01/03 10:53:42 INFO SparkContext: Submitted application: sparklyr
## 20/01/03 10:53:43 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:56059/jars/sparklyr-2.4-2.11.jar with timestamp 1578070423382
## 20/01/03 10:53:46 INFO Executor: Fetching spark://127.0.0.1:56059/jars/sparklyr-2.4-2.11.jar with timestamp 1578070423382
## 20/01/03 10:53:46 INFO Utils: Fetching spark://127.0.0.1:56059/jars/sparklyr-2.4-2.11.jar to C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-52aec169-ffb9-41f1-adf8-73dca9486f0b\userFiles-b350b5ef-e684-40da-af05-6e25ee146a24\fetchFileTemp4580869859070593341.tmp
## 20/01/03 10:53:46 INFO Executor: Adding file:/C:/Users/jimgr/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-52aec169-ffb9-41f1-adf8-73dca9486f0b/userFiles-b350b5ef-e684-40da-af05-6e25ee146a24/sparklyr-2.4-2.11.jar to class loader
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-52aec169-ffb9-41f1-adf8-73dca9486f0b\userFiles-b350b5ef-e684-40da-af05-6e25ee146a24\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-52aec169-ffb9-41f1-adf8-73dca9486f0b\userFiles-b350b5ef-e684-40da-af05-6e25ee146a24\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-52aec169-ffb9-41f1-adf8-73dca9486f0b\userFiles-b350b5ef-e684-40da-af05-6e25ee146a24\sparklyr-2.4-2.11.jar
## 20/01/03 10:54:01 INFO SparkContext: Submitted application: sparklyr
## 20/01/03 10:54:02 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:56180/jars/sparklyr-2.4-2.11.jar with timestamp 1578070442043
## 20/01/03 10:54:13 INFO Executor: Fetching spark://127.0.0.1:56180/jars/sparklyr-2.4-2.11.jar with timestamp 1578070442043
## 20/01/03 10:54:13 INFO Utils: Fetching spark://127.0.0.1:56180/jars/sparklyr-2.4-2.11.jar to C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-60812e89-12f2-4207-8f8c-186542cd1752\userFiles-40444d6e-7a04-4cff-8f79-a7efc4f1b63e\fetchFileTemp8921957727771415792.tmp
## 20/01/03 10:54:13 INFO Executor: Adding file:/C:/Users/jimgr/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-60812e89-12f2-4207-8f8c-186542cd1752/userFiles-40444d6e-7a04-4cff-8f79-a7efc4f1b63e/sparklyr-2.4-2.11.jar to class loader
## 20/01/03 10:54:34 INFO SparkContext: Submitted application: sparklyr
## 20/01/03 10:54:35 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:56327/jars/sparklyr-2.4-2.11.jar with timestamp 1578070475095
## 20/01/03 10:54:38 INFO Executor: Fetching spark://127.0.0.1:56327/jars/sparklyr-2.4-2.11.jar with timestamp 1578070475095
## 20/01/03 10:54:38 INFO Utils: Fetching spark://127.0.0.1:56327/jars/sparklyr-2.4-2.11.jar to C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-eee85580-e143-48c1-a092-26382f2301e7\userFiles-1d44024b-c499-4ed9-be90-253863af667c\fetchFileTemp558323683217817457.tmp
## 20/01/03 10:54:38 INFO Executor: Adding file:/C:/Users/jimgr/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-eee85580-e143-48c1-a092-26382f2301e7/userFiles-1d44024b-c499-4ed9-be90-253863af667c/sparklyr-2.4-2.11.jar to class loader
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-eee85580-e143-48c1-a092-26382f2301e7\userFiles-1d44024b-c499-4ed9-be90-253863af667c\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-eee85580-e143-48c1-a092-26382f2301e7\userFiles-1d44024b-c499-4ed9-be90-253863af667c\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-eee85580-e143-48c1-a092-26382f2301e7\userFiles-1d44024b-c499-4ed9-be90-253863af667c\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-60812e89-12f2-4207-8f8c-186542cd1752\userFiles-40444d6e-7a04-4cff-8f79-a7efc4f1b63e\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-60812e89-12f2-4207-8f8c-186542cd1752\userFiles-40444d6e-7a04-4cff-8f79-a7efc4f1b63e\sparklyr-2.4-2.11.jar
## java.io.IOException: Failed to delete: C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-60812e89-12f2-4207-8f8c-186542cd1752\userFiles-40444d6e-7a04-4cff-8f79-a7efc4f1b63e\sparklyr-2.4-2.11.jar
## 20/01/03 16:36:49 INFO SparkContext: Submitted application: sparklyr
## 20/01/03 16:36:49 INFO SparkContext: Added JAR file:/C:/Users/jimgr/Documents/R/win-library/3.6/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:52583/jars/sparklyr-2.4-2.11.jar with timestamp 1578091009582
## 20/01/03 16:36:52 INFO Executor: Fetching spark://127.0.0.1:52583/jars/sparklyr-2.4-2.11.jar with timestamp 1578091009582
## 20/01/03 16:36:52 INFO Utils: Fetching spark://127.0.0.1:52583/jars/sparklyr-2.4-2.11.jar to C:\Users\jimgr\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-a65c65f6-bbb6-4e09-8d0e-fac3cab7ea6c\userFiles-affeca5b-8193-4b27-b323-459e276e21d2\fetchFileTemp8946085174672082714.tmp
## 20/01/03 16:36:52 INFO Executor: Adding file:/C:/Users/jimgr/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-a65c65f6-bbb6-4e09-8d0e-fac3cab7ea6c/userFiles-affeca5b-8193-4b27-b323-459e276e21d2/sparklyr-2.4-2.11.jar to class loader</code></pre>
<p>Most of the time, you won’t need to worry about Spark logs, except in cases for which you need to troubleshoot a failed computation; in those cases, logs are an invaluable resource to be aware of. Now you know.</p>
</div>
<div id="disconnecting" class="section level2">
<h2>Disconnecting</h2>
<p>For local clusters (really, any cluster), after you are done processing data, you should disconnect by running the following:</p>
<pre class="r"><code>spark_disconnect_all()</code></pre>
<pre><code>## [1] 1</code></pre>
<hr />
</div>
</div>
<div id="analysis-1" class="section level1">
<h1>Analysis</h1>
<p>This chapter introduces concepts to perform data analysis in Spark from R. Spoiler alert: these are the same tools used with plain R! This is not a mere coincidence; rather, we want data scientists to live in a world where technology is hidden from them, where you can use the R packages you know and love, and they “just work” in Spark!</p>
<p>In a data analysis project, the main goal is to understand what the data is trying to “tell us”, hoping that it provides an answer to a specific question. Most data analysis projects follow the same set of steps, from importing data, to wrangling&gt;visualizing&gt;modeling, to communicating results.</p>
<p>When working with not-large-scale datasets—as in datasets that fit in memory—we can perform all those steps from R, without using Spark. However, when data does not fit in memory or computation is simply too slow, we can slightly modify this approach by incorporating Spark. But how?</p>
<p>For data analysis, the ideal approach is to let Spark do what it’s good at. Spark is a(((“parallel execution”))) parallel computation engine that works at a large scale and provides a SQL engine and modeling libraries. You can use these to perform most of the same operations R performs. Such operations include data selection, transformation, and modeling.</p>
<p>Data <em>import</em>, <em>wrangling</em>, and <em>modeling</em> can be performed inside Spark. The idea is to use R to tell Spark what data operations to run, and then only bring the results into R. As illustrated here, the ideal method <em>pushes compute</em> to the Spark cluster, and then <em>collects results</em> into R.</p>
<p>The <strong>sparklyr</strong> package aids in using the “push compute, collect results” principle. Most of its functions are wrappers on top of Spark API calls. This allows us to take advantage of Spark’s analysis components, instead of R’s. For example, when you need to fit a linear regression model, instead of using R’s familiar <code>lm()</code> function, you would use Spark’s <code>ml_linear_regression()</code> function. This R function then calls Spark to create this model.</p>
<p>For more common data manipulation tasks, sparklyr provides a backend for dplyr. This means you can use dplyr verbs with which you’re already familiar in R, and then sparklyr and dplyr will translate those actions into Spark SQL statements, which are generally more compact and easier to read than SQL statements. So, if you are already familiar with R and dplyr, there is nothing new to learn. This might feel a bit anticlimactic—indeed, it is—but it’s also great since you can focus that energy on learning other skills required to do large-scale computing.</p>
<p>To start, load the sparklyr and dplyr packages and then open a new local connection.</p>
<pre class="r"><code>library(sparklyr)
library(dplyr)

sc&lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.4&quot;)</code></pre>
<p>When using Spark with R, you need to approach importing data differently. Usually, importing means that R will read files and load them into memory; when you are using Spark, the data is imported into Spark, not R. In Figure 3.5, notice how the data source is connected to Spark instead of being connected to R.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-access"></span>
<img src="/courses/MachineLearning/Spark-with-R_files/figure-html/analysis-access-1.png" alt="Import Data to Spark not R" width="100%" height="220pt" />
<p class="caption">
Figure 1: Import Data to Spark not R
</p>
</div>
<p><strong>Note:</strong> When you doing analysis over large-scale datasets, the vast majority of the necessary data will be already available in your Spark cluster (which is usually made available to users via Hive tables, or by accessing the file system directly), the Data chapter will cover this extensively.</p>
<p>Rather than importing all data into Spark, you can also request Spark to access the data source without importing it – this is a decision you should make based on speed and performance. Importing all of the data into the Spark session will incur a up-front cost, once; since Spark needs to wait for the data to be loaded before analyzing it. If the data is not imported, you usually incur a cost with every Spark operation since Spark needs to retrieve a subset from the cluster’s storage, which is usually disk drives that happen to be much slower than reading from Spark’s memory. More will be covered in the Tuning chapter.</p>
<p>Let’s prime the session with some data by importing <code>mtcars</code> into Spark using <code>copy_to()</code>; you can also import data from distributed files in many different file formats, which you’ll learn in the Data chapter.</p>
<pre class="r"><code>cars &lt;- copy_to(sc, mtcars)</code></pre>
<p><strong>Note:</strong> In an enterprise setting, <code>copy_to()</code> should only be used to transfer small tables from R, large data transfers should be performed with specialized data transfer tools.</p>
<p>The data is now accessible to Spark and transformations can now be applied with ease; the next section will cover how to wrangle data by running transformations inside Spark, using <code>dplyr</code>.</p>
<p>Data wrangling uses transformations to understand the data, it is often referred to as the process of transforming data from one “raw” data form into another format with the intent of making it more appropriate for data analysis.</p>
<p>In the R environment, <em>cars</em> can be treated as if it is a local data frame, so <code>dplyr</code> verbs can be used. For instance, we can find out the mean of all columns as with <code>summarise_all()</code>:</p>
<pre class="r"><code>summarize_all(cars, mean)</code></pre>
<p>While this code is exactly the same as the code you would run when using <code>dplyr</code> without Spark, a lot is happening under the hood! The data is NOT being imported into R; instead,<code>dplyr</code> converts this task into SQL statements that are then sent to Spark. The <code>show_query()</code> command makes it possible to peer into the SQL statement that <code>sparklyr</code> and <code>dplyr</code> created and sent to Spark. We can also use this time to introduce the pipe (<code>%&gt;%</code>) operator, a custom operator from the <code>magrittr</code> package that takes pipes a computation into the first argument of the next function, making your data analysis much easier to read.</p>
<pre class="r"><code>summarize_all(cars, mean) %&gt;%
  show_query()</code></pre>
<p>As it is evident, <code>dplyr</code> is much more concise than SQL; but rest assured, you will not have to see nor understand SQL when using <code>dplyr</code>. Your focus can remain on obtaining insights from the data, as opposed to figuring out how to express a given set of transformation in SQL. Here is another example that groups the cars dataset by “transmission” type.</p>
<pre class="r"><code>cars %&gt;%
  mutate(transmission = ifelse(am == 0, &quot;automatic&quot;, &quot;manual&quot;)) %&gt;%
  group_by(transmission) %&gt;%
  summarise_all(mean)</code></pre>
<pre><code>## Warning: Missing values are always removed in SQL.
## Use `mean(x, na.rm = TRUE)` to silence this warning
## This warning is displayed only once per session.</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 12]
##   transmission   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
##   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 manual        24.4  5.08  144.  127.  4.05  2.41  17.4 0.538     1  4.38  2.92
## 2 automatic     17.1  6.95  290.  160.  3.29  3.77  18.2 0.368     0  3.21  2.74</code></pre>
<div id="built-in-functions" class="section level3">
<h3>Built-in Functions</h3>
<p>Spark SQL is based on Hive’s SQL conventions and functions and it is possible to call all these functions using <code>dplyr</code> as well. This means that we can use any Spark SQL functions to accomplish operations that may not be available via <code>dplyr</code>. The functions can be accessed by calling them as if they were R functions. Instead of failing, <code>dplyr</code> passes functions it does not recognize “as-is” to the query engine. This gives us a lot of flexibility on the function we can use!</p>
<p>For instance, the <em>percentile</em> function returns the exact percentile of a column in a group. The function expects a column name, and either a single percentile value, or an array of multiple percentile values. We can use this Spark SQL function from <code>dplyr</code> as follows:</p>
<pre class="r"><code>summarise(cars, mpg_percentile = percentile(mpg, 0.25))</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 1]
##   mpg_percentile
##            &lt;dbl&gt;
## 1           15.4</code></pre>
<p>There is no <code>percentile()</code> function in R, so <code>dplyr</code> passes the that portion of the code, “as-is”, to the resulting SQL query.</p>
<pre class="r"><code>summarise(cars, mpg_percentile = percentile(mpg, 0.25)) %&gt;%
  show_query()</code></pre>
<pre><code>## &lt;SQL&gt;
## SELECT percentile(`mpg`, 0.25) AS `mpg_percentile`
## FROM `mtcars`</code></pre>
</div>
<div id="correlations" class="section level3">
<h3>Correlations</h3>
<p>A very common exploration technique is to calculate and visualize correlations, which we often calculate to find out what kind of statistical relationship exists between paired sets of variables. Spark provides functions to calculate correlations across the entire dataset and returns the results to R as a DataFrame object:</p>
<pre class="r"><code>ml_corr(cars)</code></pre>
<pre><code>## # A tibble: 11 x 11
##       mpg    cyl   disp     hp    drat     wt    qsec     vs      am   gear
##     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
##  1  1     -0.852 -0.848 -0.776  0.681  -0.868  0.419   0.664  0.600   0.480
##  2 -0.852  1      0.902  0.832 -0.700   0.782 -0.591  -0.811 -0.523  -0.493
##  3 -0.848  0.902  1      0.791 -0.710   0.888 -0.434  -0.710 -0.591  -0.556
##  4 -0.776  0.832  0.791  1     -0.449   0.659 -0.708  -0.723 -0.243  -0.126
##  5  0.681 -0.700 -0.710 -0.449  1      -0.712  0.0912  0.440  0.713   0.700
##  6 -0.868  0.782  0.888  0.659 -0.712   1     -0.175  -0.555 -0.692  -0.583
##  7  0.419 -0.591 -0.434 -0.708  0.0912 -0.175  1       0.745 -0.230  -0.213
##  8  0.664 -0.811 -0.710 -0.723  0.440  -0.555  0.745   1      0.168   0.206
##  9  0.600 -0.523 -0.591 -0.243  0.713  -0.692 -0.230   0.168  1       0.794
## 10  0.480 -0.493 -0.556 -0.126  0.700  -0.583 -0.213   0.206  0.794   1    
## 11 -0.551  0.527  0.395  0.750 -0.0908  0.428 -0.656  -0.570  0.0575  0.274
## # ... with 1 more variable: carb &lt;dbl&gt;</code></pre>
<p>The corrr R package specializes in correlations. It contains friendly functions to prepare and visualize the results. Included inside the package is a backend for Spark, so when a Spark object is used in corrr, the actual computation also happens in Spark. In the background, the correlate() function runs sparklyr::ml_corr(), so there is no need to collect any data into R prior to running the command:</p>
<pre class="r"><code>corrr::correlate(cars, use = &quot;pairwise.complete.obs&quot;, method = &quot;pearson&quot;)</code></pre>
<pre><code>## 
## Correlation method: &#39;pearson&#39;
## Missing treated using: &#39;pairwise.complete.obs&#39;</code></pre>
<pre><code>## # A tibble: 11 x 12
##    rowname    mpg    cyl   disp     hp    drat     wt    qsec     vs      am
##    &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
##  1 mpg     NA     -0.852 -0.848 -0.776  0.681  -0.868  0.419   0.664  0.600 
##  2 cyl     -0.852 NA      0.902  0.832 -0.700   0.782 -0.591  -0.811 -0.523 
##  3 disp    -0.848  0.902 NA      0.791 -0.710   0.888 -0.434  -0.710 -0.591 
##  4 hp      -0.776  0.832  0.791 NA     -0.449   0.659 -0.708  -0.723 -0.243 
##  5 drat     0.681 -0.700 -0.710 -0.449 NA      -0.712  0.0912  0.440  0.713 
##  6 wt      -0.868  0.782  0.888  0.659 -0.712  NA     -0.175  -0.555 -0.692 
##  7 qsec     0.419 -0.591 -0.434 -0.708  0.0912 -0.175 NA       0.745 -0.230 
##  8 vs       0.664 -0.811 -0.710 -0.723  0.440  -0.555  0.745  NA      0.168 
##  9 am       0.600 -0.523 -0.591 -0.243  0.713  -0.692 -0.230   0.168 NA     
## 10 gear     0.480 -0.493 -0.556 -0.126  0.700  -0.583 -0.213   0.206  0.794 
## 11 carb    -0.551  0.527  0.395  0.750 -0.0908  0.428 -0.656  -0.570  0.0575
## # ... with 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>We can pipe the results to other corrr functions. For example, the shave() function turns all of the duplicated results into NAs. Again, while this feels like standard R code using existing R packages, Spark is being used under the hood to perform the correlation.</p>
<p>Additionally, as shown in Figure 3.6, the results can be easily visualized using the rplot() function, as shown here:</p>
<pre class="r"><code>corrr::correlate(cars, use = &quot;pairwise.complete.obs&quot;, method = &quot;pearson&quot;) %&gt;%
  corrr::shave() %&gt;%
  corrr::rplot()</code></pre>
<pre><code>## 
## Correlation method: &#39;pearson&#39;
## Missing treated using: &#39;pairwise.complete.obs&#39;</code></pre>
<pre><code>## Don&#39;t know how to automatically pick scale for object of type noquote. Defaulting to continuous.</code></pre>
<p><img src="/courses/MachineLearning/Spark-with-R_files/figure-html/analysis-corrr-show-1.png" width="672" /></p>
</div>
<div id="visualize" class="section level3">
<h3>Visualize</h3>
<p>R is great at data visualizations. Its capabilities for creating plots is extended by the many R packages that focus on this analysis step. Unfortunately, the vast majority of R functions that create plots depend on the data already being in local memory within R, so they fail when using a remote table inside Spark.</p>
<p>It is possible to create visualizations in R from data source from Spark. To understand how to do this, let’s first break down how computer programs build plots: It takes the raw data and performs some sort of transformation. The transformed data is then mapped to a set of coordinates. Finally, the mapped values are drawn in a plot. Figure <a href="#fig:analysis-plot">2</a> summarizes each of the steps.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-plot"></span>
<img src="/courses/MachineLearning/Spark-with-R_files/figure-html/analysis-plot-1.png" alt="Stages of an R plot" width="100%" height="100pt" />
<p class="caption">
Figure 2: Stages of an R plot
</p>
</div>
<p>In essence, the approach for visualizing is the same as in wrangling, push the computation to Spark, and then collect the results in R for plotting. The heavy lifting of preparing the data, such as in aggregating the data by groups or bins, can be done inside Spark, and then collect the much smaller data set into R. Inside R, the plot becomes a more basic operation. For example, to plot a histogram, the bins are calculated in Spark, and then in R, use a simple column plot, as opposed to a histogram plot, because there is no need for R to re-calculate the bins.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-spark-plot"></span>
<img src="/courses/MachineLearning/Spark-with-R_files/figure-html/analysis-spark-plot-1.png" alt="Plotting with Spark and R" width="100%" height="200pt" />
<p class="caption">
Figure 3: Plotting with Spark and R
</p>
</div>
<p>Using this conceptual model, let’s apply this when using <code>ggplot2</code>.</p>
</div>
<div id="using-ggplot2" class="section level3">
<h3>Using ggplot2</h3>
<p>To create a bar plot using <code>ggplot2</code>, we simply call a function:</p>
<pre class="r"><code>ggplot(aes(as.factor(cyl), mpg), data = mtcars) + geom_col()</code></pre>
<div class="figure" style="text-align: center"><span id="fig:analysis-ggplot2-simple"></span>
<img src="/courses/MachineLearning/Spark-with-R_files/figure-html/analysis-ggplot2-simple-1.png" alt="Plotting inside R" width="500pt" height="400pt" />
<p class="caption">
Figure 4: Plotting inside R
</p>
</div>
<p>In this case, the <code>mtcars</code> raw data was <em>automatically</em> transformed into three discrete aggregated numbers, then each result was mapped into an <code>x</code> and <code>y</code> plane, and then the plot was drawn. As R users, all of the stages of building the plot are conveniently abstracted for us.</p>
<p>In Spark, there are a couple of key steps when codifying the “push compute, collect results” approach. First, ensure that the transformation operations happen inside Spark. In the example below, <code>group_by()</code> and <code>summarise()</code> will run as inside Spark. The second is to bring the results back into R after the data has been transformed. Make sure to transform and then collect, in that order; if <code>collect()</code> is run first, then R will try to ingest the entire data set from Spark. Depending on the size of the data, collecting all of the data will slow down or may even bring down your system.</p>
<pre class="r"><code>car_group &lt;- cars %&gt;%
  group_by(cyl) %&gt;%
  summarise(mpg = sum(mpg, na.rm = TRUE)) %&gt;%
  collect() %&gt;%
  print()</code></pre>
<pre><code>## # A tibble: 3 x 2
##     cyl   mpg
##   &lt;dbl&gt; &lt;dbl&gt;
## 1     4  293.
## 2     8  211.
## 3     6  138.</code></pre>
<p>In this example, now that the data has been pre-aggregated and collected into R, only three records are passed to the plotting function.</p>
<pre class="r"><code>ggplot(aes(as.factor(cyl), mpg), data = car_group) + 
  geom_col(fill = &quot;#999999&quot;) + coord_flip()</code></pre>
<p><img src="/courses/MachineLearning/Spark-with-R_files/figure-html/analysis-ggplot2-groups-1.png" width="672" /></p>
</div>
<div id="using-dbplot" class="section level3">
<h3>Using dbplot</h3>
<p>The <code>dbplot</code> package provides helper functions for plotting with remote data. The R code <code>dbplot</code> uses to transform the data is written so that it can be translated into Spark. It then uses those results to create a graph using the <code>ggplot2</code> package where data transformation and plotting are both triggered by a single function.</p>
<p>The <code>dbplot_histogram()</code> function makes Spark calculate the bins and the count per bin and outputs a <code>ggplot</code> object which can be further refined by adding more steps to the plot object. <code>dbplot_histogram()</code> also accepts a <code>binwidth</code> argument to control the range used to compute the bins</p>
<pre class="r"><code>library(dbplot)

cars %&gt;%
dbplot_histogram(mpg, binwidth = 3) +
  coord_flip()+
labs(title = &quot;MPG Distribution&quot;,
     subtitle = &quot;Histogram over miles per gallon&quot;)</code></pre>
<p><img src="/courses/MachineLearning/Spark-with-R_files/figure-html/analysis-dbplot-simple-1.png" width="672" /></p>
<p>Histograms provide a great way to analyze a single variable. To analyze two variables, a scatter or raster plot is commonly used.</p>
<p>Scatter plots are used to compare the relationship between two continuous variables. For example, a scatter plot will display the relationship between the weight of a car and its gas consumption. The plot will show that the higher the weight, the higher the gas consumption because the dots clump together into almost a line that goes from the top left towards the bottom right. See Figure <a href="#fig:analysis-point"><strong>??</strong></a> for an example of the plot.</p>
<p><img src="/courses/MachineLearning/Spark-with-R_files/figure-html/analysis-scatter-rendeer-1.png" width="672" /></p>
<p>However, for scatter plots, no amount of “pushing the computation” to Spark will help with this problem because the data has to be plotted in individual dots.</p>
<p>The best alternative is to find a plot type that represents the x/y relationship and concentration in a way that it is easy to perceive and to “physically” plot. The <em>raster</em> plot may be the best answer. It returns a grid of x/y positions and the results of a given aggregation usually represented by the color of the square.</p>
<p>You can use <code>dbplot_raster()</code> to create a scatter-like plot in Spark, while only retrieving a small subset of the remote dataset:</p>
<p><strong>Tip:</strong> You can also use <code>dbplot</code> to retrieve the raw data and visualize by other means; to retrieve the aggregates but not the plots use: <code>db_compute_bins()</code>, <code>db_compute_count()</code>, <code>db_compute_raster()</code> and <code>db_compute_boxplot()</code>.</p>
</div>
<div id="model" class="section level2">
<h2>Model</h2>
<p>First, an analysis project goes through as many transformations and models to find the answer. The ideal data analysis language enables you to quickly adjust over each wrangle-visualize-model iteration. Fortunately, this is the case when using Spark and R.</p>
<p>To illustrate how easy it is to iterate over wrangling and modeling in Spark, consider the following example. We will start by performing a linear regression against all features and predict MPG:</p>
<pre class="r"><code>cars %&gt;% 
  ml_linear_regression(mpg ~ .) %&gt;%
  summary()</code></pre>
<p>It is also very easy to iterate with other kinds of models. The following one replaces the linear model with a generalized linear model:</p>
<pre class="r"><code>cars %&gt;% 
  ml_generalized_linear_regression(mpg ~ hp + cyl) %&gt;%
  summary()</code></pre>
<div id="caching" class="section level3">
<h3>Caching</h3>
<p>The examples in this chapter are built using a very small data set. In real-life scenarios, large amounts of data are used for models. If the data needs to be transformed first, the volume of the data could exact a heavy toll on the Spark session. Before fitting the models, it is a good idea to save the results of all the transformations in a new table inside Spark memory.</p>
<p>The <code>compute()</code> command can take the end of a <code>dplyr</code> piped command set and save the results to Spark memory.</p>
<pre class="r"><code>cached_cars &lt;- cars %&gt;% 
  mutate(cyl = paste0(&quot;cyl_&quot;, cyl)) %&gt;%
  compute(&quot;cached_cars&quot;)</code></pre>
<pre class="r"><code>cached_cars %&gt;%
  ml_linear_regression(mpg ~ .) %&gt;%
  summary()</code></pre>
<pre><code>## Deviance Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.47339 -1.37936 -0.06554  1.05105  4.39057 
## 
## Coefficients:
## (Intercept) cyl_cyl_8.0 cyl_cyl_4.0        disp          hp        drat 
## 16.15953652  3.29774653  1.66030673  0.01391241 -0.04612835  0.02635025 
##          wt        qsec          vs          am        gear        carb 
## -3.80624757  0.64695710  1.74738689  2.61726546  0.76402917  0.50935118 
## 
## R-Squared: 0.8816
## Root Mean Squared Error: 2.041</code></pre>
<p>As more insights are gained from the data, more questions may be raised. That is why we expect to iterate through data <em>wrangle</em>, <em>visualize</em>, and <em>model</em> multiple times. Each iteration should provide incremental insights of what the data is “telling us”. There will be a point where we reach a satisfactory level of understanding. It is at this point that we will be ready to share the results of the analysis, this is the topic of the next section.</p>
</div>
</div>
<div id="communicate" class="section level2">
<h2>Communicate</h2>
<p>It is important to clearly communicate the analysis results – as important as the analysis work itself! The public, colleagues or stakeholders need to understand what you found out and how.</p>
<p>To communicate effectively we need to use artifacts, such as reports and presentations; these are common output formats that we can create in R, using R Markdown.</p>
<p>R Markdown documents allow weave narrative text and code together. The amount of output formats provides a very compelling reason to learn and use R Markdown. There are many available output formats like HTML, PDF, PowerPoint, Word, web slides, Websites, books and so on.</p>
<p>Since an R Markdown document is self-contained and meant to be reproducible, before rendering documents we should first disconnect from Spark to free resources:</p>
<pre class="r"><code>spark_disconnect(sc)</code></pre>
<hr />
</div>
<div id="modeling-1" class="section level2">
<h2>Modeling</h2>
<p>The examples here will utilize the OkCupid dataset <span class="citation">[@kim2015okcupid]</span>, available at <a href="https://github.com/r-spark/okcupid">github.com/r-spark/okcupid</a>. The dataset consists of user profile data from an online dating site, and contains a diverse set of features, including biographical characteristics such as gender and profession, and free text fields related to personal interests. There are about 60,000 profiles in the dataset, which fits comfortably into memory on a modern laptop and wouldn’t be considered “big data”, so you can easily follow along running Spark local mode.</p>
<p>Download this dataset as follows:</p>
<pre class="r"><code>download.file(
  &quot;https://github.com/r-spark/okcupid/raw/master/profiles.csv.zip&quot;,
  &quot;okcupid.zip&quot;)

unzip(&quot;okcupid.zip&quot;, exdir = &quot;data&quot;)
unlink(&quot;okcupid.zip&quot;)</code></pre>
<p>Consider the following problem:</p>
<blockquote>
<p>Predict whether someone is actively working, i.e. not retired, a student, or unemployed.</p>
</blockquote>
<p>Next up, we will explore this dataset.</p>
</div>
<div id="exploratory-data-analysis" class="section level2">
<h2>Exploratory Data Analysis</h2>
<p>Exploratory data analysis (EDA), in the context of predictive modeling, is the exercise of looking at excerpts and summaries of the data. The specific goals of the EDA stage is informed by the business problem, but here are some common objectives:</p>
<ul>
<li>Check for data quality — confirm meaning and prevalence of missing values and reconcile statistics against existing controls,</li>
<li>Understand univariate relationships between variables, and</li>
<li>Perform an initial assessment on what variables to include and what transformations need to be done on them.</li>
</ul>
<p>We’ll first connect to Spark, load libraries and read in the data.</p>
<pre class="r"><code>library(sparklyr)
library(ggplot2)
library(dbplot)
library(dplyr)

sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.4&quot;)

okc &lt;- spark_read_csv(
  sc, 
  &quot;d:/the-r-in-spark-1.0.0/profiles.csv&quot;,
#  &quot;data/profiles.csv&quot;, 
  escape = &quot;\&quot;&quot;, 
  memory = FALSE,
  options = list(multiline = TRUE)
) %&gt;%
  mutate(
    height = as.numeric(height),
    income = ifelse(income == &quot;-1&quot;, NA, as.numeric(income))
  ) %&gt;%
  mutate(sex = ifelse(is.na(sex), &quot;missing&quot;, sex)) %&gt;%
  mutate(drinks = ifelse(is.na(drinks), &quot;missing&quot;, drinks)) %&gt;%
  mutate(drugs = ifelse(is.na(drugs), &quot;missing&quot;, drugs)) %&gt;%
  mutate(job = ifelse(is.na(job), &quot;missing&quot;, job))</code></pre>
<p>We specify <code>escape = "\""</code> and <code>options = list(multiline = TRUE)</code> here to accommodate for embedded quote characters and newlines in the essay fields. We also convert the <code>height</code> and <code>income</code> columns to numeric types, and recode missing values in the string columns. Note that it may very well take a few tries of specifying different parameters to get the initial data ingest correct, and sometimes you may have to revisit this step after you learn more about the data during modeling.</p>
<p>We can now take a quick look at our data with <code>glimpse()</code>:</p>
<pre class="r"><code>glimpse(okc)</code></pre>
<pre><code>## Observations: ??
## Variables: 31
## Database: spark_connection
## $ age         &lt;int&gt; 22, 35, 38, 23, 29, 29, 32, 31, 24, 37, 35, 28, 24, 30,...
## $ body_type   &lt;chr&gt; &quot;a little extra&quot;, &quot;average&quot;, &quot;thin&quot;, &quot;thin&quot;, &quot;athletic&quot;...
## $ diet        &lt;chr&gt; &quot;strictly anything&quot;, &quot;mostly other&quot;, &quot;anything&quot;, &quot;veget...
## $ drinks      &lt;chr&gt; &quot;socially&quot;, &quot;often&quot;, &quot;socially&quot;, &quot;socially&quot;, &quot;socially&quot;...
## $ drugs       &lt;chr&gt; &quot;never&quot;, &quot;sometimes&quot;, &quot;missing&quot;, &quot;missing&quot;, &quot;never&quot;, &quot;m...
## $ education   &lt;chr&gt; &quot;working on college/university&quot;, &quot;working on space camp...
## $ essay0      &lt;chr&gt; &quot;about me:&lt;br /&gt;\n&lt;br /&gt;\ni would love to think that i ...
## $ essay1      &lt;chr&gt; &quot;currently working as an international agent for a frei...
## $ essay2      &lt;chr&gt; &quot;making people laugh.&lt;br /&gt;\nranting about a good salti...
## $ essay3      &lt;chr&gt; &quot;the way i look. i am a six foot half asian, half cauca...
## $ essay4      &lt;chr&gt; &quot;books:&lt;br /&gt;\nabsurdistan, the republic, of mice and m...
## $ essay5      &lt;chr&gt; &quot;food.&lt;br /&gt;\nwater.&lt;br /&gt;\ncell phone.&lt;br /&gt;\nshelter....
## $ essay6      &lt;chr&gt; &quot;duality and humorous things&quot;, NA, NA, &quot;cats and german...
## $ essay7      &lt;chr&gt; &quot;trying to find someone to hang out with. i am down for...
## $ essay8      &lt;chr&gt; &quot;i am new to california and looking for someone to wisp...
## $ essay9      &lt;chr&gt; &quot;you want to be swept off your feet!&lt;br /&gt;\nyou are tir...
## $ ethnicity   &lt;chr&gt; &quot;asian, white&quot;, &quot;white&quot;, NA, &quot;white&quot;, &quot;asian, black, ot...
## $ height      &lt;dbl&gt; 75, 70, 68, 71, 66, 67, 65, 65, 67, 65, 70, 72, 72, 66,...
## $ income      &lt;dbl&gt; NaN, 80000, NaN, 20000, NaN, NaN, NaN, NaN, NaN, NaN, N...
## $ job         &lt;chr&gt; &quot;transportation&quot;, &quot;hospitality / travel&quot;, &quot;missing&quot;, &quot;s...
## $ last_online &lt;chr&gt; &quot;2012-06-28-20-30&quot;, &quot;2012-06-29-21-41&quot;, &quot;2012-06-27-09-...
## $ location    &lt;chr&gt; &quot;south san francisco, california&quot;, &quot;oakland, california...
## $ offspring   &lt;chr&gt; &quot;doesn&amp;rsquo;t have kids, but might want them&quot;, &quot;doesn&amp;...
## $ orientation &lt;chr&gt; &quot;straight&quot;, &quot;straight&quot;, &quot;straight&quot;, &quot;straight&quot;, &quot;straig...
## $ pets        &lt;chr&gt; &quot;likes dogs and likes cats&quot;, &quot;likes dogs and likes cats...
## $ religion    &lt;chr&gt; &quot;agnosticism and very serious about it&quot;, &quot;agnosticism b...
## $ sex         &lt;chr&gt; &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, ...
## $ sign        &lt;chr&gt; &quot;gemini&quot;, &quot;cancer&quot;, &quot;pisces but it doesn&amp;rsquo;t matter...
## $ smokes      &lt;chr&gt; &quot;sometimes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, NA, &quot;no&quot;, &quot;w...
## $ speaks      &lt;chr&gt; &quot;english&quot;, &quot;english (fluently), spanish (poorly), frenc...
## $ status      &lt;chr&gt; &quot;single&quot;, &quot;single&quot;, &quot;available&quot;, &quot;single&quot;, &quot;single&quot;, &quot;s...</code></pre>
<p>Now we will add our response variable as a column in the dataset and look at its distribution</p>
<pre class="r"><code>okc &lt;- okc %&gt;%
  mutate(
    not_working = ifelse(job %in% c(&quot;student&quot;, &quot;unemployed&quot;, &quot;retired&quot;), 1 , 0)
  )

okc %&gt;% 
  group_by(not_working) %&gt;% 
  tally()</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 2]
##   not_working     n
##         &lt;dbl&gt; &lt;dbl&gt;
## 1           0 54541
## 2           1  5405</code></pre>
<p>Before we proceed further, let us perform an initial split of our data into a training set and a testing set and put away the latter. In practice, this is a crucial step because we would like to have a holdout set that we set aside at the end of the modeling process to evaluate model performance. If we were to include the entire dataset during EDA, information from the testing set could “leak” into the visualizations and summary statistics, and bias our model building process even though the data is not used directly in a learning algorithm. This would undermine the credibility of our performance metrics. Splitting the data can be done easily by using the <code>sdf_partition()</code> function:</p>
<pre class="r"><code>data_splits &lt;- sdf_random_split(okc, training = 0.8, testing = 0.2, seed = 42)
okc_train &lt;- data_splits$training
okc_test &lt;- data_splits$testing</code></pre>
<p>We can quickly look at the distribution of our response variable:</p>
<pre class="r"><code>okc_train %&gt;%
  group_by(not_working) %&gt;%
  tally() %&gt;%
  mutate(frac = n / sum(n))</code></pre>
<pre><code>## Warning: Missing values are always removed in SQL.
## Use `SUM(x, na.rm = TRUE)` to silence this warning
## This warning is displayed only once per session.</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 3]
##   not_working     n   frac
##         &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1           0 43789 0.910 
## 2           1  4313 0.0897</code></pre>
<p>Using the <code>sdf_describe()</code> function, we can obtain numerical summaries of specific columns:</p>
<pre class="r"><code>sdf_describe(okc_train, cols = c(&quot;age&quot;, &quot;income&quot;))</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 3]
##   summary age                income            
##   &lt;chr&gt;   &lt;chr&gt;              &lt;chr&gt;             
## 1 count   48102              9230              
## 2 mean    32.336534863415245 105942.57854821235
## 3 stddev  9.43908920033797   203550.81474192906
## 4 min     18                 20000.0           
## 5 max     110                1000000.0</code></pre>
<p>Like we saw in the Analysis chapter, we can also utilize the dbplot package to plot distributions of these variables. For example, a histogram of the distribution of the <code>age</code> variable:</p>
<pre class="r"><code>dbplot_histogram(okc_train, age) +
  geom_hline(yintercept = 0, size = 1, colour = &quot;#333333&quot;) +
  labs(title = &quot;Distribution of Age&quot;, subtitle = &quot;Age histogram in OKCupid dataset&quot;)</code></pre>
<p><img src="/courses/MachineLearning/Spark-with-R_files/figure-html/modeling-eda-hist-1.png" width="672" /></p>
<p>A common EDA exercise is to look at the relationships between the response and the individual predictors. Often, you may have prior business knowledge on what these relationships should be, so this can serve as a data quality check. Also, unexpected trends can inform variable interactions you might want to include in the model. As an example, we can explore the <code>religion</code> variable:</p>
<pre class="r"><code>prop_data &lt;- okc_train %&gt;%
  mutate(religion = regexp_extract(religion, &quot;^\\\\w+&quot;, 0)) %&gt;% 
  group_by(religion, not_working) %&gt;%
  tally() %&gt;%
  group_by(religion) %&gt;%
  summarize(
    count = sum(n),
    prop = sum(not_working * n) / sum(n)
  ) %&gt;%
  mutate(se = sqrt(prop * (1 - prop) / count)) %&gt;%
  collect()</code></pre>
<pre><code>## Warning: Missing values are always removed in SQL.
## Use `SUM(x, na.rm = TRUE)` to silence this warning
## This warning is displayed only once per session.</code></pre>
<pre class="r"><code>prop_data</code></pre>
<pre><code>## # A tibble: 10 x 4
##    religion     count   prop      se
##    &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
##  1 christianity  4688 0.117  0.00469
##  2 atheism       5615 0.117  0.00424
##  3 judaism       2460 0.0793 0.00548
##  4 other         6247 0.0839 0.00346
##  5 hinduism       361 0.0942 0.0154 
##  6 &lt;NA&gt;         16232 0.0711 0.002  
##  7 agnosticism   7031 0.0996 0.00361
##  8 catholicism   3815 0.0894 0.00458
##  9 islam          114 0.184  0.0363 
## 10 buddhism      1539 0.0897 0.00728</code></pre>
<p>Note that <code>prop_data</code> is a small data frame that has been collected into memory in our R session, we can take advantage of ggplot2 to create an informative visualization.</p>
<pre class="r"><code>prop_data %&gt;%
  ggplot(aes(x = religion, y = prop)) + geom_point(size = 2) +
  geom_errorbar(aes(ymin = prop - 1.96 * se, ymax = prop + 1.96 * se),
                width = .1) +
  geom_hline(yintercept = sum(prop_data$prop * prop_data$count) /
                              sum(prop_data$count))</code></pre>
<p><img src="/courses/MachineLearning/Spark-with-R_files/figure-html/modeling-eda-prop-code-1.png" width="672" /></p>
<p>Next, we take a look at the relationship between a couple of predictors: alcohol use and drug use. We would expect there to be some correlation between them. You can compute a contingency table via <code>sdf_crosstab()</code>:</p>
<pre class="r"><code>contingency_tbl &lt;- okc_train %&gt;% 
  sdf_crosstab(&quot;drinks&quot;, &quot;drugs&quot;) %&gt;%
  collect()

contingency_tbl</code></pre>
<pre><code>## # A tibble: 7 x 5
##   drinks_drugs missing never often sometimes
##   &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;
## 1 very often        54   142    43       146
## 2 socially        8245 21023   125      4132
## 3 not at all       155  2351    16       105
## 4 desperately       74    86    23        73
## 5 often           1067  1723    64      1285
## 6 missing         1074  1233    10        66
## 7 rarely           612  3690    38       447</code></pre>
<p>We can visualize this contingency table using a mosaic plot:</p>
<pre class="r"><code>library(ggmosaic)
library(forcats)
library(tidyr)

contingency_tbl %&gt;%
  rename(drinks = drinks_drugs) %&gt;%
  gather(&quot;drugs&quot;, &quot;count&quot;, missing:sometimes) %&gt;%
  mutate(
    drinks = as_factor(drinks) %&gt;% 
      fct_relevel(&quot;missing&quot;, &quot;not at all&quot;, &quot;rarely&quot;, &quot;socially&quot;, 
                  &quot;very often&quot;, &quot;desperately&quot;),
    drugs = as_factor(drugs) %&gt;%
      fct_relevel(&quot;missing&quot;, &quot;never&quot;, &quot;sometimes&quot;, &quot;often&quot;)
  ) %&gt;%
  ggplot() +
  geom_mosaic(aes(x = product(drinks, drugs), fill = drinks, 
                  weight = count))</code></pre>
<p><img src="/courses/MachineLearning/Spark-with-R_files/figure-html/modeling-eda-contingency-code-1.png" width="672" /></p>
<p>To further explore the relationship between these two variables, we can perform correspondence analysis using the FactoMineR package. This technique enables us to summarize the relationship between the high dimensional factor levels by mapping each level to a point on the plane. We first obtain the mapping using <code>FactoMineR::CA()</code> as follows:</p>
<pre class="r"><code>dd_obj &lt;- contingency_tbl %&gt;% 
  tibble::column_to_rownames(var = &quot;drinks_drugs&quot;) %&gt;%
  FactoMineR::CA(graph = FALSE)</code></pre>
<p>We can then plot the results using ggplot:</p>
<pre class="r"><code>dd_drugs &lt;-
  dd_obj$row$coord %&gt;%
  as.data.frame() %&gt;%
  mutate(
    label = gsub(&quot;_&quot;, &quot; &quot;, rownames(dd_obj$row$coord)),
    Variable = &quot;Drugs&quot;
  )

dd_drinks &lt;-
  dd_obj$col$coord %&gt;%
  as.data.frame() %&gt;%
  mutate(
    label = gsub(&quot;_&quot;, &quot; &quot;, rownames(dd_obj$col$coord)),
    Variable = &quot;Alcohol&quot;
  )
  
ca_coord &lt;- rbind(dd_drugs, dd_drinks)
  
ggplot(ca_coord, aes(x = `Dim 1`, y = `Dim 2`, 
                     col = Variable)) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  geom_text(aes(label = label)) +
  coord_equal()</code></pre>
<p><img src="/courses/MachineLearning/Spark-with-R_files/figure-html/modeling-eda-factominer-code-1.png" width="672" /></p>
</div>
<div id="feature-engineering" class="section level2">
<h2>Feature Engineering</h2>
<p>The feature engineering exercise comprises transforming the data to increase the performance of the model. This can include things like centering and scaling numerical values and performing string manipulation to extract meaningful variables. It also often includes variable selection — the process of selecting which predictors are used in the model.</p>
<p>In the histogram, we see that the <code>age</code> variable has a range from 18 to over 60. Some algorithms, especially neural networks, train faster if we normalize our inputs so that they are of the same magnitude. Let’s now normalize the <code>age</code> variable by removing the mean and scaling to unit variance, beginning by calculating its mean and standard deviation:</p>
<pre class="r"><code>scale_values &lt;- okc_train %&gt;%
  summarize(
    mean_age = mean(age),
    sd_age = sd(age)
  ) %&gt;%
  collect()

scale_values</code></pre>
<pre><code>## # A tibble: 1 x 2
##   mean_age sd_age
##      &lt;dbl&gt;  &lt;dbl&gt;
## 1     32.3   9.44</code></pre>
<p>We can then use these to transform the dataset:</p>
<pre class="r"><code>okc_train &lt;- okc_train %&gt;%
  mutate(scaled_age = (age - !!scale_values$mean_age) /
           !!scale_values$sd_age)</code></pre>
<pre class="r"><code>dbplot_histogram(okc_train, scaled_age)+
  geom_hline(yintercept = 0, size = 1, colour = &quot;#333333&quot;) +
  labs(title = &quot;Distribution of Scaled Age&quot;, subtitle = &quot;Scaled age histogram in OKCupid dataset&quot;) </code></pre>
<p><img src="/courses/MachineLearning/Spark-with-R_files/figure-html/modeling-eda-scale-code-1.png" width="672" /></p>
<p>Since some of the profile features are multiple-select, in other words, a person can choose to associate with multiple options for a variable, we need to process them before we can build meaningful models. If we take a look at the ethnicity column, for example, we see that there are many different combinations:</p>
<pre class="r"><code>okc_train %&gt;%
  group_by(ethnicity) %&gt;%
  tally()</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 2]
##    ethnicity                                           n
##    &lt;chr&gt;                                           &lt;dbl&gt;
##  1 hispanic / latin, white                          1044
##  2 middle eastern, hispanic / latin                   31
##  3 asian, indian, pacific islander                     5
##  4 native american                                    56
##  5 asian, black, native american, hispanic / latin     1
##  6 asian, indian, white                                5
##  7 native american, other                             11
##  8 asian, black, pacific islander, white, other        1
##  9 asian, native american                              2
## 10 black, native american, pacific islander, white     1
## # ... with more rows</code></pre>
<p>One way to proceed would be to treat each combination of races as a separate level, but that would lead to a very large number of levels which becomes problematic in many algorithms. To better encode this information, we can create dummy variables for each race, as follows:</p>
<pre class="r"><code>ethnicities &lt;- c(&quot;asian&quot;, &quot;middle eastern&quot;, &quot;black&quot;, &quot;native american&quot;, &quot;indian&quot;, 
                 &quot;pacific islander&quot;, &quot;hispanic / latin&quot;, &quot;white&quot;, &quot;other&quot;)
ethnicity_vars &lt;- ethnicities %&gt;% 
  purrr::map(~ expr(ifelse(like(ethnicity, !!.x), 1, 0))) %&gt;%
  purrr::set_names(paste0(&quot;ethnicity_&quot;, gsub(&quot;\\s|/&quot;, &quot;&quot;, ethnicities)))
okc_train &lt;- mutate(okc_train, !!!ethnicity_vars)
okc_train %&gt;% 
  select(starts_with(&quot;ethnicity_&quot;)) %&gt;%
  glimpse()</code></pre>
<pre><code>## Observations: ??
## Variables: 9
## Database: spark_connection
## $ ethnicity_asian           &lt;dbl&gt; 0, 0, NaN, NaN, 0, NaN, 1, 0, 0, 1, 0, 0,...
## $ ethnicity_middleeastern   &lt;dbl&gt; 0, 0, NaN, NaN, 0, NaN, 0, 0, 0, 0, 0, 0,...
## $ ethnicity_black           &lt;dbl&gt; 0, 0, NaN, NaN, 0, NaN, 0, 0, 0, 0, 0, 0,...
## $ ethnicity_nativeamerican  &lt;dbl&gt; 0, 0, NaN, NaN, 0, NaN, 0, 0, 0, 0, 0, 0,...
## $ ethnicity_indian          &lt;dbl&gt; 0, 1, NaN, NaN, 0, NaN, 0, 0, 0, 0, 0, 0,...
## $ ethnicity_pacificislander &lt;dbl&gt; 0, 0, NaN, NaN, 0, NaN, 0, 0, 0, 0, 0, 0,...
## $ ethnicity_hispaniclatin   &lt;dbl&gt; 1, 0, NaN, NaN, 0, NaN, 0, 0, 0, 0, 1, 0,...
## $ ethnicity_white           &lt;dbl&gt; 0, 0, NaN, NaN, 0, NaN, 0, 1, 0, 0, 0, 1,...
## $ ethnicity_other           &lt;dbl&gt; 0, 0, NaN, NaN, 0, NaN, 0, 0, 0, 0, 0, 0,...</code></pre>
<p>For the free text fields, a straightforward way to extract features is counting the total number of characters. We will store the train dataset in Spark’s memory with <code>compute()</code> to speed up computation.</p>
<pre class="r"><code>okc_train &lt;- okc_train %&gt;%
  mutate(
    essay_length = char_length(paste(!!!syms(paste0(&quot;essay&quot;, 0:9))))
  ) %&gt;% compute()</code></pre>
<pre class="r"><code>dbplot_histogram(okc_train, essay_length, bins = 100)</code></pre>
<p><img src="/courses/MachineLearning/Spark-with-R_files/figure-html/modeling-eda-essay-render-1.png" width="672" /></p>
</div>
</div>
<div id="sparklyr-has-some-functions-such-as-spark_read_csv-that-will-read-a-csv-file-into-spark" class="section level1">
<h1>sparklyr has some functions such as spark_read_csv() that will read a CSV file into Spark</h1>
</div>
<div id="load-dplyr" class="section level1">
<h1>Load dplyr</h1>
<p>library(dplyr)
# Explore track_metadata structure
str(track_metadata)
# Connect to your Spark cluster
spark_conn &lt;- spark_connect(master = “local”)
# Copy track_metadata to Spark
track_metadata_tbl &lt;- copy_to(spark_conn, track_metadata, overwrite = TRUE)
# List the data frames available in Spark
src_tbls(spark_conn)
# Disconnect from Spark
spark_disconnect(sc = spark_conn)
###################################
# Link to the track_metadata table in Spark
track_metadata_tbl &lt;- tbl(spark_conn, “track_metadata”)
# See how big the dataset is
dim(track_metadata_tbl)
# See how small the tibble is ( from the r pryr package)
object_size(track_metadata_tbl)
################################
# Print 5 rows, all columns
print(track_metadata_tbl, n=5, width=Inf)
# Examine structure of tibble
str(track_metadata_tbl)
# Examine structure of data
glimpse(track_metadata_tbl)
#############################################
Before you try the exercise, take heed of two warnings. Firstly, don’t mistake dplyr’s filter() function with the stats package’s filter() function. Secondly, sparklyr converts your dplyr code into SQL database code before passing it to Spark. That means that only a limited number of filtering operations are currently supported. For example, you can’t filter character rows using regular expressions with code like
a_tibble %&gt;%
filter(grepl(“a regex”, x))
The help page for translate_sql() describes the functionality that is available. You are OK to use comparison operators like &gt;, !=, and %in%; arithmetic operators like +, ^, and %%; and logical operators like &amp;, | and !. Many mathematical functions such as log(), abs(), round(), and sin() are also supported.
As before, square bracket indexing does not currently work.
######################################
# track_metadata_tbl has been pre-defined
track_metadata_tbl
# Manipulate the track metadata
track_metadata_tbl %&gt;%
# Select columns
select(title, duration) %&gt;%
# Mutate columns
mutate(duration_minutes = duration/60 )
In case you hadnt got the message already that base-R functions dont work with Spark tibbles, you cant use within() or transform() for this purpose.
##############################</p>
</div>
<div id="track_metadata_tbl-has-been-pre-defined" class="section level1">
<h1>track_metadata_tbl has been pre-defined</h1>
<p>track_metadata_tbl
track_metadata_tbl %&gt;%
# Select columns starting with artist
select(starts_with(“artist”))
track_metadata_tbl %&gt;%
# Select columns ending with id
select(ends_with(“id”))</p>
<div id="section" class="section level21">
<p></p>
</div>
</div>
<div id="track_metadata_tbl-has-been-pre-defined-1" class="section level1">
<h1>track_metadata_tbl has been pre-defined</h1>
<p>track_metadata_tbl
track_metadata_tbl %&gt;%
# Select columns containing ti
select(contains(“ti”))
track_metadata_tbl %&gt;%
# Select columns matching ti.?t
select(matches(“ti.?t”))</p>
<div id="section-1" class="section level41">
<p></p>
<p>#for factors, can use levels() function
track_metadata_tbl %&gt;%
# Only return rows with distinct artist_name
distinct(artist_name)</p>
</div>
<div id="section-2" class="section level38">
<p></p>
</div>
</div>
<div id="table-is-not-supported-in-sparklyr" class="section level1">
<h1>table is not supported in sparklyr</h1>
</div>
<div id="track_metadata_tbl-has-been-pre-defined-2" class="section level1">
<h1>track_metadata_tbl has been pre-defined</h1>
<p>track_metadata_tbl
track_metadata_tbl %&gt;%
# Count the artist_name values
count(artist_name, sort = TRUE) %&gt;%
# Restrict to top 20
top_n(20)</p>
<div id="section-3" class="section level26">
<p></p>
</div>
</div>
<div id="copy_to-moves-data-from-r-to-spark.-collect-moves-it-back" class="section level1">
<h1>copy_to() moves data from R to spark. collect() moves it back</h1>
</div>
<div id="track_metadata_tbl-has-been-pre-defined-3" class="section level1">
<h1>track_metadata_tbl has been pre-defined</h1>
<p>track_metadata_tbl
results &lt;- track_metadata_tbl %&gt;%
# Filter where artist familiarity is greater than 0.9
filter(artist_familiarity &gt; 0.9)
# Examine the class of the results
class(results)
# Collect your results
collected &lt;- results %&gt;%
collect()
# Examine the class of the collected results
class(collected)</p>
<div id="section-4" class="section level35">
<p></p>
</div>
</div>
<div id="use-compute-to-compute-the-calculation-but-store-the-results-in-a-temporary-data-frame-on-spark.-compute-takes-two-arguments-a-tibble-and-a-variable-name-for-the-spark-data-frame-that-will-store-the-results." class="section level1">
<h1>use compute() to compute the calculation, but store the results in a temporary data frame on Spark. Compute takes two arguments: a tibble, and a variable name for the Spark data frame that will store the results.</h1>
</div>
<div id="track_metadata_tbl-has-been-pre-defined-4" class="section level1">
<h1>track_metadata_tbl has been pre-defined</h1>
<p>track_metadata_tbl
computed &lt;- track_metadata_tbl %&gt;%
# Filter where artist familiarity is greater than 0.8
filter(artist_familiarity &gt; 0.8) %&gt;%
# Compute the results
compute(“familiar_artists”)
# See the available datasets
src_tbls(spark_conn)
# Examine the class of the computed results
class(computed)</p>
<div id="section-5" class="section level30">
<p></p>
</div>
</div>
<div id="note-that-the-columns-passed-to-group_by-should-typically-be-categorical-variables.-for-example-if-you-wanted-to-calculate-the-average-weight-of-people-relative-to-their-height-it-doesnt-make-sense-to-group-by-height-since-everyones-height-is-unique.-you-could-however-use-cut-to-convert-the-heights-into-different-categories-and-calculate-the-mean-weight-for-each-category." class="section level1">
<h1>Note that the columns passed to group_by() should typically be categorical variables. For example, if you wanted to calculate the average weight of people relative to their height, it doesn’t make sense to group by height, since everyone’s height is unique. You could, however, use cut() to convert the heights into different categories, and calculate the mean weight for each category.</h1>
</div>
<div id="track_metadata_tbl-has-been-pre-defined-5" class="section level1">
<h1>track_metadata_tbl has been pre-defined</h1>
<p>track_metadata_tbl
duration_by_artist &lt;- track_metadata_tbl %&gt;%
# Group by artist
group_by(artist_name) %&gt;%
# Calc mean duration
summarize(mean_duration = mean(duration))
duration_by_artist %&gt;%
# Sort by ascending mean duration
arrange(mean_duration)
duration_by_artist %&gt;%
# Sort by descending mean duration
arrange(desc(mean_duration))</p>
<div id="section-6" class="section level30">
<p></p>
</div>
</div>
<div id="track_metadata_tbl-has-been-pre-defined-6" class="section level1">
<h1>track_metadata_tbl has been pre-defined</h1>
<p>track_metadata_tbl
track_metadata_tbl %&gt;%
# Group by artist
group_by(artist_name) %&gt;%
# Calc time since first release
mutate(time_since_first_release = year - min(year)) %&gt;%
# Arrange by descending time since first release
arrange(desc(time_since_first_release))</p>
<div id="section-7" class="section level29">
<p></p>
<p>As previously mentioned, when you use the dplyr interface, sparklyr converts your code into SQL before passing it to Spark. Most of the time, this is what you want. However, you can also write raw SQL to accomplish the same task. Most of the time, this is a silly idea since the code is harder to write and harder to debug. However, if you want your code to be portable – that is, used outside of R as well – then it may be useful. For example, a fairly common workflow is to use sparklyr to experiment with data processing, then switch to raw SQL in a production environment. By writing raw SQL to begin with, you can just copy and paste your queries when you move to production.
SQL queries are written as strings, and passed to dbGetQuery() from the DBI package. The pattern is as follows.
query &lt;- “SELECT col1, col2 FROM some_data WHERE some_condition”
a_data.frame &lt;- dbGetQuery(spark_conn, query)
Note that unlike the dplyr code you’ve written, dbGetQuery() will always execute the query and return the results to R immediately. If you want to delay returning the data, you can use dbSendQuery() to execute the query, then dbFetch() to return the results. That’s more advanced usage, not covered here. Also note that DBI functions return data.frames rather than tibbles, since DBI is a lower-level package.</p>
</div>
</div>
<div id="write-sql-query" class="section level1">
<h1>Write SQL query</h1>
<p>query &lt;- “SELECT * FROM track_metadata WHERE year &lt; 1935 AND duration &gt; 300”
# Run the query
(results &lt;- dbGetQuery(spark_conn, query))
###################################
# track_metadata_tbl and artist_terms_tbl have been pre-defined
track_metadata_tbl
artist_terms_tbl
# Left join artist terms to track metadata by artist_id
joined &lt;- left_join(track_metadata_tbl, artist_terms_tbl, by = “artist_id”)
joined &lt;- semi_join(track_metadata_tbl, artist_terms_tbl, by = “artist_id”)
anti_join</p>
</div>
<div id="how-many-rows-and-columns-are-in-the-joined-table" class="section level1">
<h1>How many rows and columns are in the joined table?</h1>
<p>dim(joined)</p>
<div id="section-8" class="section level24">
<p></p>
<p>ft_ feature transformation functions ( cut, )
ml_ machine learning transformations
sdf_ spark dataframe api (sampling, partitioning )
#######################
The sparklyr way of converting a continuous variable into logical uses ft_binarizer(). The previous diabetes example can be rewritten as the following. Note that the threshold value should be a number, not a string refering to a column in the dataset.
diabetes_data %&gt;%
ft_binarizer(“plasma_glucose_concentration”, “has_diabetes”, threshold = threshold_mmol_per_l)
In keeping with the Spark philosophy of using DoubleType everywhere, the output from ft_binarizer() isnt actually logical; it is numeric. This is the correct approach for letting you continue to work in Spark and perform other transformations, but if you want to process your data in R, you have to remember to explicitly convert the data to logical. The following is a common code pattern.
a_tibble %&gt;%
ft_binarizer(“x”, “is_x_big”, threshold = threshold) %&gt;%
collect() %&gt;%
mutate(is_x_big = as.logical(is_x_big))</p>
<p>hotttnesss &lt;- track_metadata_tbl %&gt;%
# Select artist_hotttnesss
select(artist_hotttnesss) %&gt;%
# Binarize to is_hottt_or_nottt
ft_binarizer(“artist_hotttnesss”,“is_hottt_or_nottt”, threshold = 0.5) %&gt;%
# Collect the result
collect() %&gt;%
# Convert is_hottt_or_nottt to logical
mutate(is_hottt_or_nottt = as.logical(is_hottt_or_nottt))
# Draw a barplot of is_hottt_or_nottt
ggplot(hotttnesss, aes(is_hottt_or_nottt)) +
geom_bar()
####################################################################
The sparklyr equivalent of this is to use ft_bucketizer(). The code takes a similar format to ft_binarizer(), but this time you must pass a vector of cut points to the splits argument. Here is the same example rewritten in sparklyr style.
smoking_data %&gt;%
ft_bucketizer(“cigarettes_per_day”, “smoking_status”, splits = c(0, 1, 10, 20, Inf))
There are several important things to note. You may have spotted that the breaks argument from cut() is the same as the splits argument from ft_bucketizer(). There is a slight difference in how values on the boundary are handled. In cut(), by default, the upper (right-hand) boundary is included in each bucket, but not the left. ft_bucketizer() includes the lower (left-hand) boundary in each bucket, but not the right. This means that it is equivalent to calling cut() with the argument right = FALSE.
One exception is that ft_bucketizer() includes values on both boundaries for the upper-most bucket. So ft_bucketizer() is also equivalent to setting include.lowest = TRUE when using cut().
The final thing to note is that whereas cut() returns a factor, ft_bucketizer() returns a numeric vector, with values in the first bucket returned as zero, values in the second bucket returned as one, values in the third bucket returned as two, and so on. If you want to work on the results in R, you need to explicitly convert to a factor. This is a common code pattern:</p>
<p>a_tibble %&gt;%
ft_bucketizer(“x”, “x_buckets”, splits = splits) %&gt;%
collect() %&gt;%
mutate(x_buckets = factor(x_buckets, labels = labels)</p>
<pre><code>     # track_metadata_tbl, decades, decade_labels have been pre-defined
     track_metadata_tbl
     decades
     decade_labels
     
     hotttnesss_over_time &lt;- track_metadata_tbl %&gt;%
       # Select artist_hotttnesss and year
       select(artist_hotttnesss, year) %&gt;%
       # Convert year to numeric
       mutate(year = as.numeric(year)) %&gt;%
       # Bucketize year to decade using decades vector
       ft_bucketizer(&quot;year&quot;,&quot;decade&quot;,splits = decades) %&gt;%
       # Collect the result
       collect() %&gt;%
       # Convert decade to factor using decade_labels
       mutate(decade = factor(decade, labels = decade_labels))
     
     # Draw a boxplot of artist_hotttnesss by decade
     ggplot(hotttnesss_over_time, aes(decade, artist_hotttnesss)) +
       geom_boxplot()  
     
     
     ###########################################
     
     
     # track_metadata_tbl, duration_labels have been pre-defined
     track_metadata_tbl
     duration_labels
     
     familiarity_by_duration &lt;- track_metadata_tbl %&gt;%
       # Select duration and artist_familiarity
       select(duration, artist_familiarity) %&gt;%
       # Bucketize duration
       ft_quantile_discretizer(&quot;duration&quot;,&quot;duration_bin&quot;, n.buckets = 5) %&gt;%
       # Collect the result
       collect() %&gt;%
       # Convert duration bin to factor
       mutate(duration_bin = factor(duration_bin, labels = duration_labels))
     
     # Draw a boxplot of artist_familiarity by duration_bin
     ggplot(familiarity_by_duration, aes(duration_bin, artist_familiarity)) +
       geom_boxplot() 
     
     #############################################################
     
     
     More than words: tokenization (1)
     Common uses of text-mining include analyzing shopping reviews to ascertain purchasers feeling about the product, or analyzing financial news to predict the sentiment regarding stock prices. In order to analyze text data, common pre-processing steps are to convert the text to lower-case (see tolower()), and to split sentences into individual words.
     ft_tokenizer() performs both these steps. Its usage takes the same pattern as the other transformations that you have seen, with no other arguments.
     shop_reviews %&gt;%
       ft_tokenizer(&quot;review_text&quot;, &quot;review_words&quot;)
     Since the output can contain a different number of words in each row, output.col is a list column, where every element is a list of strings. To analyze text data, it is usually preferable to have one word per row in the data. The list-of-list-of-strings format can be transformed to a single character vector using unnest() from the tidyr package. There is currently no method for unnesting data on Spark, so for now, you have to collect it to R before transforming it. The code pattern to achieve this is as follows.
     library(tidyr)
     text_data %&gt;%
       ft_tokenizer(&quot;sentences&quot;, &quot;word&quot;) %&gt;%
       collect() %&gt;%
       mutate(word = lapply(word, as.character)) %&gt;%
       unnest(word)
     # track_metadata_tbl has been pre-defined
     track_metadata_tbl
     title_text &lt;- track_metadata_tbl %&gt;%
       # Select artist_name, title
       select(artist_name, title) %&gt;%
       # Tokenize title to words
       ft_tokenizer(&quot;title&quot;, &quot;word&quot;) %&gt;%
       # Collect the result
       collect() %&gt;%
       # Flatten the word column 
       mutate(word = lapply(word, as.character)) %&gt;% 
       # Unnest the list column
       unnest(word)
     #################################
     More than words: tokenization (3)
     ft_tokenizer() uses a simple technique to generate words by splitting text data on spaces. For more advanced usage, you can use regular expressions to split the text data. This is done via the ft_regex_tokenizer() function, which has the same usage as ft_tokenizer(), but with an extra pattern argument for the splitter.
     a_tibble %&gt;%
       ft_regex_tokenizer(&quot;x&quot;, &quot;y&quot;, pattern = regex_pattern)
     The return value from ft_regex_tokenizer(), like ft_tokenizer(), is a list of lists of character vectors.
     The dataset contains a field named artist_mbid that contains an ID for the artist on MusicBrainz, a music metadata encyclopedia website. The IDs take the form of hexadecimal numbers split by hyphens
     
     track_metadata_tbl %&gt;%
       # Select artist_mbid column
       select(artist_mbid) %&gt;%
       # Split it by hyphens
       ft_regex_tokenizer(&quot;artist_mbid&quot;, &quot;artist_mbid_chunks&quot;, pattern = &quot;-&quot;)
     ##############################
     # Compare timings of arrange() and sdf_sort()
     microbenchmark(
       arranged = track_metadata_tbl %&gt;%
         # Arrange by year, then artist_name, then release, then title
         arrange(year, artist_name, release, title) %&gt;%
         # Collect the result
         collect(),
       sorted = track_metadata_tbl %&gt;%
         # Sort by year, then artist_name, then release, then title
         sdf_sort(c(&quot;year&quot;,&quot;artist_name&quot;,&quot;release&quot;,&quot;title&quot;)) %&gt;%
         # Collect the result
         collect(),
       times = 5
     )
     ########################################
     looking at the data types
     # track_metadata_tbl has been pre-defined
     track_metadata_tbl
     # Get the schema
     (schema &lt;- sdf_schema(track_metadata_tbl))
     # ad way to Transform the schema into something more readable
     schema %&gt;%
       lapply(function(x) do.call(data_frame, x)) %&gt;%
       bind_rows()
     R type               Spark type
     logical              boolean
     numeric              doubletype
     integer              integertype
     character            stringtype
     list                 arraytype
     ##################################
     Shrinking the data by sampling
     When you are working with a big dataset, you typically dont really need to work with all of it all the time. Particularly at the start of your project, while you are experimenting wildly with what you want to do, you can often iterate more quickly by working on a smaller subset of the data. sdf_sample() provides a convenient way to do this. It takes a tibble, and the fraction of rows to return. In this case, you want to sample without replacement. To get a random sample of one tenth of your dataset, you would use the following code.
     a_tibble %&gt;%
       sdf_sample(fraction = 0.1, replacement = FALSE)
     Since the results of the sampling are random, and you will likely want to reuse the shrunken dataset, it is common to use compute() to store the results as another Spark data frame. 
     a_tibble %&gt;%
       sdf_sample(&lt;some args&gt;) %&gt;%
       compute(&quot;sample_dataset&quot;)
     To make the results reproducible, you can also set a random number seed via the seed argument. Doing this means that you get the same random dataset every time you run your code. It doesnt matter which number you use for the seed; just choose your favorite positive integer.
     ######################################
     Training/testing partitions
     Most of the time, when you run a predictive model, you need to fit the model on one subset of your data (the &quot;training&quot; set), then test the model predictions against the rest of your data (the &quot;testing&quot; set).
     sdf_partition() provides a way of partitioning your data frame into training and testing sets. Its usage is as follows.
     a_tibble %&gt;%
       sdf_partition(training = 0.7, testing = 0.3)
     There are two things to note about the usage. Firstly, if the partition values don&#39;t add up to one, they will be scaled so that they do. So if you passed training = 0.35 and testing = 0.15, you&#39;d get double what you asked for. Secondly, you can use any set names that you like, and partition the data into more than two sets. So the following is also valid.
     a_tibble %&gt;%
       sdf_partition(a = 0.1, b = 0.2, c = 0.3, d = 0.4)
     The return value is a list of tibbles. you can access each one using the usual list indexing operators.
     partitioned$a
     partitioned[[&quot;b&quot;]]
     partitioned &lt;- track_metadata_tbl %&gt;%
       # Partition into training and testing sets
       sdf_partition(training = 0.7, testing = 0.3)
     # Get the dimensions of the training set
     dim(partitioned$training)
     # Get the dimensions of the testing set
     dim(partitioned$testing)
     #################################
     Supported machine learning functions include linear regression and its variants, tree-based models (ml_decision_tree(), and a few others. You can see the list of all the machine learning functions using ls().
                                                                                                         </code></pre>
<p>ls(“package:sparklyr”, pattern = “^ml”)</p>
<p>#########################################################
Working with parquet files
CSV files are great for saving the contents of rectangular data objects (like R data.frames and Spark DataFrames) to disk. The problem is that they are really slow to read and write, making them unusable for large datasets. Parquet files provide a higher performance alternative. As well as being used for Spark data, parquet files can be used with other tools in the Hadoop ecosystem, like Shark, Impala, Hive, and Pig.
Technically speaking, parquet file is a misnomer. When you store data in parquet format, you actually get a whole directory worth of files. The data is split across multiple .parquet files, allowing it to be easily stored on multiple machines, and there are some metadata files too, describing the contents of each column.
sparklyr can import parquet files using spark_read_parquet(). This function takes a Spark connection, a string naming the Spark DataFrame that should be created, and a path to the parquet directory. Note that this function will import the data directly into Spark, which is typically faster than importing the data into R, then using copy_to() to copy the data from R to Spark.
spark_read_parquet(sc, “a_dataset”, “path/to/parquet/dir”)</p>
<p># parquet_dir has been pre-defined
parquet_dir
# List the files in the parquet dir
filenames &lt;- dir(parquet_dir, full.names = TRUE)
# Show the filenames and their sizes
data_frame(
filename = basename(filenames),
size_bytes = file.size(filenames)
)
# Import the data into Spark
timbre_tbl &lt;- spark_read_parquet(spark_conn, “timbre”, parquet_dir)</p>
<p>######################################</p>
<p>Partitioning data with a group effect
Before you can run any models, you need to partition your data into training and testing sets. There’s a complication with this dataset, which means you can’t just call sdf_partition(). The complication is that each track by a single artist ought to appear in the same set; your model will appear more accurate than it really is if tracks by an artist are used to train the model then appear in the testing set.
The trick to dealing with this is to partition only the artist IDs, then inner join those partitioned IDs to the original dataset. Note that artist_id is more reliable than artist_name for partitioning, since some artists use variations on their name between tracks. For example, Duke Ellington sometimes has an artist name of “Duke Ellington”, but other times has an artist name of “Duke Ellington &amp; His Orchestra”, or one of several spelling variants.
training_testing_artist_ids &lt;- track_data_tbl %&gt;%
# Select the artist ID
select(artist_id) %&gt;%
# Get distinct rows
distinct() %&gt;%
# Partition into training/testing sets
sdf_partition(training = 0.7, testing = 0.3)
track_data_to_model_tbl &lt;- track_data_tbl %&gt;%
# Inner join to training partition
inner_join(training_testing_artist_ids<span class="math inline">\(training, by = &quot;artist_id&quot;)  track_data_to_predict_tbl &lt;- track_data_tbl %&gt;%  # Inner join to testing partition  inner_join(training_testing_artist_ids\)</span>testing, by = “artist_id”)
#################################################
An ML Gradient boosted tree model (all numeric features)</p>
<p>feature_colnames &lt;- track_data_to_model_tbl %&gt;%
# Get the column names
colnames() %&gt;%
# Limit to the timbre columns
str_subset(fixed(“timbre”))
gradient_boosted_trees_model &lt;- track_data_to_model_tbl %&gt;%
# Run the gradient boosted trees model
ml_gradient_boosted_trees(features = feature_colnames, response = “year”)
#################################
Predictions
# training, testing sets &amp; model are pre-defined
track_data_to_model_tbl
track_data_to_predict_tbl
gradient_boosted_trees_model
responses &lt;- track_data_to_predict_tbl %&gt;%
# Select the year column
select(year) %&gt;%
# Collect the results
collect() %&gt;%
# Add in the predictions
mutate(
predicted_year = predict(
gradient_boosted_trees_model,
track_data_to_predict_tbl
)
)</p>
<p>###############################
# plots to assess quality of model fit
One slightly tricky thing here is that sparklyr doesnt yet support the residuals() function in all its machine learning models. Consequently, you have to calculate the residuals yourself (predicted responses minus actual responses).
# Draw a scatterplot of predicted vs. actual
ggplot(responses, aes(actual, predicted)) +
# Add the points
geom_point(alpha = 0.1) +
# Add a line at actual = predicted
geom_abline(intercept = 0, slope = 1)
residuals &lt;- responses %&gt;%
# Transmute response data to residuals
transmute(residual = responses<span class="math inline">\(predicted-responses\)</span>actual)
# Draw a density plot of residuals
ggplot(residuals, aes(residual)) +
# Add a density curve
geom_density() +
# Add a vertical line through zero
geom_vline(xintercept = 0)</p>
<p>##################################
Random Forest: modeling
Like gradient boosted trees, random forests are another form of ensemble model. That is, they use lots of simpler models (decision trees, again) and combine them to make a single better model. Rather than running the same model iteratively, random forests run lots of separate models in parallel, each on a randomly chosen subset of the data, with a randomly chosen subset of features. Then the final decision tree makes predictions by aggregating the results from the individual models.
sparklyrs random forest function is called ml_random_forest(). Its usage is exactly the same as ml_gradient_boosted_trees() (see the first exercise of this chapter for a reminder on syntax).
# Get the timbre columns
feature_colnames &lt;- track_data_to_model_tbl %&gt;%
colnames() %&gt;%
str_subset(fixed(“timbre”))
# Run the random forest model
random_forest_model &lt;- track_data_to_model_tbl %&gt;%
ml_random_forest(features = feature_colnames, response = “year”)
################################################
# Create a response vs. actual dataset
responses &lt;- track_data_to_predict_tbl %&gt;%
select(year) %&gt;%
collect() %&gt;%
mutate(predicted_year = predict(random_forest_model, track_data_to_predict_tbl))</p>
<p>################################
# Create a residual sum of squares dataset
both_responses %&gt;%
mutate(residual = both_responses<span class="math inline">\(predicted - both_responses\)</span>actual) %&gt;%
group_by(model) %&gt;%
summarise(rmse = sqrt(mean(residual * residual)))<br />
######################################</p>
</div>
</div>
